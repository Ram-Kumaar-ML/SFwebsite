[{"title":"Getting Started","type":0,"sectionRef":"#","url":"docs/Alerts_notifications/getting_started","content":"","keywords":""},{"title":"Coming Soon...","type":1,"pageTitle":"Getting Started","url":"docs/Alerts_notifications/getting_started#coming-soon","content":""},{"title":"Alert Management","type":0,"sectionRef":"#","url":"docs/Alerts_notifications/alert_management","content":"","keywords":""},{"title":"Coming Soon...","type":1,"pageTitle":"Alert Management","url":"docs/Alerts_notifications/alert_management#coming-soon","content":""},{"title":"dashboard_management","type":0,"sectionRef":"#","url":"docs/Dashboards/dashboard_management","content":"dashboard_management","keywords":""},{"title":"SLO","type":0,"sectionRef":"#","url":"docs/Alerts_notifications/slo","content":"","keywords":""},{"title":"Coming Soon...","type":1,"pageTitle":"SLO","url":"docs/Alerts_notifications/slo#coming-soon","content":""},{"title":"Getting Started","type":0,"sectionRef":"#","url":"docs/Dashboards/getting_started","content":"","keywords":""},{"title":"Coming Soon...","type":1,"pageTitle":"Getting Started","url":"docs/Dashboards/getting_started#coming-soon","content":""},{"title":"Overview","type":0,"sectionRef":"#","url":"docs/Integrations/java/overview","content":"Overview JAVA monitoring on SnappyFlow is available for the following platforms Instances Kubernetes","keywords":""},{"title":"Monitoring JAVA applications running on Instances","type":0,"sectionRef":"#","url":"docs/Integrations/java/java_instances","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Monitoring JAVA applications running on Instances","url":"docs/Integrations/java/java_instances#overview","content":"JVM on instances is monitored using sfAgent configured with jvm plugin. The plugin monitors JVM metrics, jvm arguments used to start Java process and deadlock metrics. JVM plugin internally uses the following utilities to collect metrics: Jstats for JVM metrics Jolokia will be started by plugin to collect deadlock metrics if monitor Deadlocks parameter is set in configuration file  "},{"title":"Pre-requisites","type":1,"pageTitle":"Monitoring JAVA applications running on Instances","url":"docs/Integrations/java/java_instances#pre-requisites","content":"Jcmd has to be installed in the instance "},{"title":"Configuration","type":1,"pageTitle":"Monitoring JAVA applications running on Instances","url":"docs/Integrations/java/java_instances#configuration","content":"sfAgent section provides steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: <profile_key> generate_name: true tags: Name: <instance name> appName: <application name> projectName: <project name> metrics: plugins: - name: jvm enabled: true interval: 60 config: process: * heapinterval: 3600 monitorDeadlocks: false deadlockMonitoringInterval: 300 Copy "},{"title":"Viewing data and dashboards","type":1,"pageTitle":"Monitoring JAVA applications running on Instances","url":"docs/Integrations/java/java_instances#viewing-data-and-dashboards","content":"Data generated by this plugin can be viewed in browse data page inside the respective application under plugin=jvm and documentType=jvm Dashboard for this data can be instantiated by importing dashboard template JVM to the application dashboard. "},{"title":"Monitoring ActiveMQ Message Broker running on Instances","type":0,"sectionRef":"#","url":"docs/Integrations/activemq","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Monitoring ActiveMQ Message Broker running on Instances","url":"docs/Integrations/activemq#overview","content":"Activemq sfAgent plugin provides metrics related to message traffic distribution and other internal transactions among the brokers. Metrics collected by the plugin are organized across the following categories Broker stats: contain transactional data and metrics related to broker stateTopic stats: provide metrics for analyzing internal transactions associated with each topicQueue stats: provide metrics for analyzing internal transactions associated with each queueJVM stats: contain all JVM related metrics like garbage collection details, memory pools, loaded/unloaded classes etc. Activemq logger plugin collects general logs comprising state change and broker specific information generated by the activemq message broker "},{"title":"Prerequisites","type":1,"pageTitle":"Monitoring ActiveMQ Message Broker running on Instances","url":"docs/Integrations/activemq#prerequisites","content":"Activemq Metric Plugin is based on Jolokia agent which requires JMX monitoring to be enable locally. Following property needs to be included during the start of activemq process -Dcom.sun.management.jmxremote Copy JCMD command must be installed in the machine "},{"title":"Configuration","type":1,"pageTitle":"Monitoring ActiveMQ Message Broker running on Instances","url":"docs/Integrations/activemq#configuration","content":"sfAgent section provides steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: <Profile_key>tags: Name: <instance_name> appName: <app_name> projectName: <project_name>metrics: plugins: - name: activemq enabled: true interval: 300 config: process: activemq port: 8161 documentsTypes: - brokerStats - topicStats - queueStats - jvmStatslogging: plugins: - name: activemq-log enabled: true config: log_path: <..activemq logpath..> log_level: - error - warning - info - warn Copy "},{"title":"Parameters required in metrics plugin","type":1,"pageTitle":"Monitoring ActiveMQ Message Broker running on Instances","url":"docs/Integrations/activemq#parameters-required-in-metrics-plugin","content":"process: Activemq process name (It should be part of java main class) port: Broker Port documentTypes: User can either leave this empty to collect all documentTypes or mention specific documentTypes to collect. Available options for plugin type activemq are brokerStats, topicStats, queueStats, jvmStats Logger plugin requires log path to be specified. Wildcard characters are supported "},{"title":"Viewing data and dashboards","type":1,"pageTitle":"Monitoring ActiveMQ Message Broker running on Instances","url":"docs/Integrations/activemq#viewing-data-and-dashboards","content":"Data collected by plugins can be viewed in SnappyFlow’s browse data section Metrics plugin: activemqdocumentType: brokerStats, topicStats, queueStats, jvmStats Dashboard template: ActiveMQ Logs Plugin: activemqdocumentType: activemq-logs "},{"title":"See Also","type":1,"pageTitle":"Monitoring ActiveMQ Message Broker running on Instances","url":"docs/Integrations/activemq#see-also","content":"Zookeeper Elasticsearch Kafka-REST Kafka-Connect "},{"title":"Monitoring Apache Server on Instances","type":0,"sectionRef":"#","url":"docs/Integrations/apache/overview","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Monitoring Apache Server on Instances","url":"docs/Integrations/apache/overview#overview","content":"Apache Server’s monitoring involves monitoring of the following elements: Apache Access Logs Apache Error Logs Apache Server Health "},{"title":"Pre-requisites","type":1,"pageTitle":"Monitoring Apache Server on Instances","url":"docs/Integrations/apache/overview#pre-requisites","content":"Ensure Apache access logs are in format expected by sfAgent parser. Edit configuration file and set log format as follows: LogFormat \"%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\" %D\"combined CustomLog \"logs/access_log\" combined Copy After configuring log format, the expected log entry would be: 45.112.52.50 - - [28/Jun/2020:23:34:10 -0700] \"GET / HTTP/1.1\" 302 242 \"-\" \"Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36\" 271 Copy Apache configuration file can be found in these paths: Ubuntu: /etc/apache2/mods-enabled/status.conf Centos: /etc/httpd/conf/httpd.conf Check if Apache status module is enabled This is required to monitor Apache server health Apache web server exposes metrics through its status module, mod_status. If apache server is running and mod_status is enabled, apache server’s status page should be available at http://127.0.0.1/server-status. Or http://localhost/server-status. Alternatively, you can check is mod_status is enabled by running the following commands: Ubuntu(or Debian based systems): sudo apache2ctl -M | grep status_module Copy Centos/RHEL/Fedora sudo httpd -M | grep status_module Copy if output of above command is status_module , then apache status module is enabled. If mod_status is not enabled , follow next step to enable it Enable Apache status module In order to enable mod_status , edit the status module’s configuration file (on Debian platforms), or your main Apache configuration file (all other Unix-like platforms). Debian users can find the status module’s configuration file in /etc/apache2/mods-enabled/status.conf Users of other platforms (such as Red Hat–based systems) will find their main configuration file in /etc/apache2/apache2.conf, /etc/httpd/conf/httpd.conf, or /etc/apache2/httpd.conf. In the main configuration file, allow access from local or from a specific ip address as shown below: <Location /server-status> SetHandler server-status Require local # Require all granted Require ip x.x.x.x </Location> Copy Check your configuration file for errors with the following command: apachectl configtest Copy Perform a graceful restart to apply the changes without interrupting live connections: (apachectl -k graceful or service apache2 graceful) Copy After enabling mod_status and restarting Apache, status page is accessible at http://localhost/server-status or http://ipaddress/server-status. "},{"title":"Configuration","type":1,"pageTitle":"Monitoring Apache Server on Instances","url":"docs/Integrations/apache/overview#configuration","content":"sfAgent section provides steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: <profile key> generate_name: true tags: Name: <unique instance name or will be generated from IP> appName: <add application name> projectName: <add project name> metrics: plugins: - name: nginx enabled: true interval: 300 config: port: 80 secure: false location: ‘stats’ logging: plugins: - name: nginx-access enabled: true config: geo_info: true log_path: /var/log/nginx/access.log, /var/log/nginx/access_log ua_parser: true - name: nginx-error enabled: true config: log_level: - emerg - alert - error log_path: /var/log/nginx/error.log, /var/log/nginx/error_log Copy "},{"title":"Viewing data and dashboards","type":1,"pageTitle":"Monitoring Apache Server on Instances","url":"docs/Integrations/apache/overview#viewing-data-and-dashboards","content":"Data generated by plugin can be viewed in browse data page inside the respective application under plugin=jvm and documentType=jvm Dashboard for this data can be instantiated by Importing dashboard template JVM to the application dashboard. "},{"title":"Overview","type":0,"sectionRef":"#","url":"docs/Integrations/kubernetes/overview","content":"Overview SnappyFlow provides various approaches to monitor Kubernetes applications. Choose one to continue sfPod# sfPod is a collector that is installed on Kubernetes and runs as a DaemonSet on each worker node.It monitors the following elements of a Kubernetes environment: Host, Pod & Container metricsResources such as deployments, Daemon Sets etc.Kubernetes core services metricsCluster logsMonitor Prometheus exporters running on any of the application pods sfKubeAgent# sfKubeAgent is sfAgent packaged as a container and run as a sidecar within a Kubernetes application pod. It can be configured to collect both application metrics and logs similar to the way sfAgent does. Prometheus Integration# Centralized Logging#","keywords":""},{"title":"Kubernetes Monitoring with sfPod","type":0,"sectionRef":"#","url":"docs/Integrations/kubernetes/kubernetes_monitoring_with_sfPod","content":"","keywords":""},{"title":"sfPod overview","type":1,"pageTitle":"Kubernetes Monitoring with sfPod","url":"docs/Integrations/kubernetes/kubernetes_monitoring_with_sfPod#sfpod-overview","content":"sfPod is a collector that is installed on Kubernetes and runs as a DaemonSet on each worker node. It monitors the following elements of a Kubernetes environment: Kubernetes metrics Cluster metrics Host metrics Pod metrics Container metrics Events Kubernetes services health– Kubelet, Kube-Proxy, API Server, Controller Manager, Core DNS Kubernetes cluster logs Poll Prometheus Exporters running on application pods Pod Application Logs "},{"title":"sfPod installation","type":1,"pageTitle":"Kubernetes Monitoring with sfPod","url":"docs/Integrations/kubernetes/kubernetes_monitoring_with_sfPod#sfpod-installation","content":"Below is short video explaining the sfPOD installation steps  "},{"title":"Step 1: Create a Cloud profile","type":1,"pageTitle":"Kubernetes Monitoring with sfPod","url":"docs/Integrations/kubernetes/kubernetes_monitoring_with_sfPod#step-1-create-a-cloud-profile","content":"Create a cloud profile in SnappyFlow (or use an existing profile) and copy the profile key to use it while installing the sfPod in your cluster. "},{"title":"Step 2: Add Snappyflow helm chart","type":1,"pageTitle":"Kubernetes Monitoring with sfPod","url":"docs/Integrations/kubernetes/kubernetes_monitoring_with_sfPod#step-2-add-snappyflow-helm-chart","content":"Login to any node that has network connectivity to Kubernetes master node and run the following commands helm repo add snappyflow https://snappyflow.github.io/helm-charts helm repo list helm repo update Copy note <my-cluster-name> Replace with any name, Cluster is discovered by this name on the Snappyflow. <profile key> Replace with key copied from SnappyFlow. "},{"title":"Restricted sfPod Configuration","type":1,"pageTitle":"Kubernetes Monitoring with sfPod","url":"docs/Integrations/kubernetes/kubernetes_monitoring_with_sfPod#restricted-sfpod-configuration","content":"By default, sfPod is installed in a full configuration mode where it monitors all the elements. For a restricted configuration i.e. monitor only cluster logs or cluster metrics, user can use the flags outlined below: --set config.cluster_monitoring=true/false If true monitoring of cluster metrics and cluster logs is enabled. If this field is made false, cluster monitoring is switched off and only Prometheus polling and Centralized Application Log Monitoring are enabled --set config.node_agent.drop_cluster_logs=true => If true, monitoring of Kubernetes cluster logs is disabled. --set config.<doc_type>= false sfPod organizes data collected by plugin/documentType. Example of some of the document types that are collected by sfPod include – pod, node, container, cluster_stats etc. User can disable collection of a documentType using this configuration. The detailed list of document types can be reviewed in Browse Data page of a Kubernetes cluster --set config.app_view By default sfPod sends pod and container metrics of tagged pods (I,e pods that have projectName and appName tags) to both Cluster Index and Project Index leading to duplication of metric data. This feature is enabled to enhance correlation of Application and Infra data. This feature can be switched-off if the flag= true. "},{"title":"Monitoring Kafka Clusters running on Instances","type":0,"sectionRef":"#","url":"docs/Integrations/kafka","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Monitoring Kafka Clusters running on Instances","url":"docs/Integrations/kafka#overview","content":"Monitoring Kafka cluster requires running sfAgent on all the nodes in the clusters. sfAgent collects metrics and logs from each of the nodes and the aggregate view is presented in SnappyFlow. Following aspects of Kafka node is monitored by sfAgent Kafka Metrics jmxStats: JVM health which includes heap utilization, GC Time, Loaded/Unloaded Classes, Memory pools and thread analysiskafkaStats: Aggregate metrics for the cluster which include - data in/out rate, message in-rate, Fetch follower request rate, Processing times for producer, consumer and follower requests etc.partitionStats: Size of each partition by topictopicStats: Topic metrics which includes data in/out rate, message in rate, producer request rate, failed producer request rate, failed fetch request rate etc..consumerStats: Offset and Lag by topic by partition  Kafka Logs kafkaGeneralLogs "},{"title":"Prerequisites","type":1,"pageTitle":"Monitoring Kafka Clusters running on Instances","url":"docs/Integrations/kafka#prerequisites","content":"Kafka Metric Plugin is based on Jolokia agent which requires JMX monitoring to be enable locally. Following property needs to be included during the start of kafka process -Dcom.sun.management.jmxremote Copy Jcmd has to be installed on the instance Kafka ships with log4j support . Log4j property file (log4j.properties) which is present in root folder of kafka. has to be set as follows Enable root logger and file appender where file appender can be of any type based on rolling strategy log4j.rootLogger=INFO, logfilelog4j.appender.logFile=org.apache.log4j.DailyRollingFileAppender Copy Specify custom log file name along with its path, layout properties and data pattern log4j.appender.logFile.DatePattern='.'yyyy-MM-dd-HHlog4j.appender.logFile.File=<..logpath..>log4j.appender.logFile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logFile.layout.ConversionPattern=[%d] %p %m (%c)%n Copy After configuring log4j properties, emitted log would look like [2020-07-09 11:15:23,376] INFO [GroupCoordinator 1]: Group consgrp-tst with generation 1588 is now empty (__consumer_offsets-5) (kafka.coordinator.group.GroupCoordinator) Copy "},{"title":"Configuration","type":1,"pageTitle":"Monitoring Kafka Clusters running on Instances","url":"docs/Integrations/kafka#configuration","content":"sfAgent section provides steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: <Profile_key>tags: Name: <instance_name> appName: <app_name> projectName: <project_name>metrics: plugins: - name: kafkajmx enabled: true interval: 60 config: process: kafka.Kafka - name: kafkatopic enabled: true interval: 60 config: documentsTypes: - kafkaStats - partitionStats - topicStats - consumerStats port: 9092 process: kafka.Kafkalogging: plugins: - name: kafka-general enabled: true config: log_level: - error - info - notice - debug log_path: <log_path> Copy Kafka metrics monitoring on instances requires two separate plugins – KafkaJMX to collect JVM health metrics and kafkaTopic plugin to collect broker and topic metrics. kafkaTopic plugin requires the following parameters: process: Kafka process name (It should be part of java main class)port: Broker Port "},{"title":"Viewing data and dashboards","type":1,"pageTitle":"Monitoring Kafka Clusters running on Instances","url":"docs/Integrations/kafka#viewing-data-and-dashboards","content":"Data collected by plugins can be viewed in SnappyFlow’s browse data section Metrics plugin: kafka documentType: jmxStats, kafkaStats, partitionStats, topicStats, consumerStats Dashboard template: Kafka Logs Plugin: kafka documentType: kafkaGeneralLogs "},{"title":"Test Matrix","type":1,"pageTitle":"Monitoring Kafka Clusters running on Instances","url":"docs/Integrations/kafka#test-matrix","content":"OS\tJDK versionubuntu 18.04 JDK 11 openjdk version \"11.0.11\" 2021-04-20 OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04) OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing) ubuntu 18.04 JDK 8 openjdk version \"1.8.0_292\" OpenJDK Runtime Environment (build 1.8.0_292-8u292-b10-0ubuntu1~18.04-b10) OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode) Centos 7 JDK 11 openjdk version \"11.0.12\" 2021-07-20 LTS OpenJDK Runtime Environment 18.9 (build 11.0.12+7-LTS) OpenJDK 64-Bit Server VM 18.9 (build 11.0.12+7-LTS, mixed mode, sharing) Centos 7 JDK 8 openjdk version \"1.8.0_302\" OpenJDK Runtime Environment (build 1.8.0_302-b08) OpenJDK 64-Bit Server VM (build 25.302-b08, mixed mode) "},{"title":"See Also","type":1,"pageTitle":"Monitoring Kafka Clusters running on Instances","url":"docs/Integrations/kafka#see-also","content":"Zookeeper Elasticsearch Kafka-REST Kafka-Connect "},{"title":"Untitled","type":0,"sectionRef":"#","url":"docs/Integrations/kubernetes/Untitled","content":"Untitled","keywords":""},{"title":"Prometheus Exporter","type":0,"sectionRef":"#","url":"docs/Integrations/kubernetes/prometheus_exporter","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Prometheus Exporter","url":"docs/Integrations/kubernetes/prometheus_exporter#overview","content":"sfPod scans all pods in the namespaces that it has access to for specific labels snappyflow/projectname and snappyflow/appname If a pod is tagged with SnappyFlow labels, sfPod then looks for standard Prometheus annotations Label\tValueprometheus.io/scrape\tIf true, the pod is considered for Prometheus scraping, else it is excluded. prometheus.io/port\tThis label defines a list of ports that sfPod will scan. sfPod will also apply the appropriate parser. If this label is empty, sfPod scans all exposed container ports. Default value is empty. prometheus.io/path\tDefine the path as /metrics. Empty by default. If sfPod finds data on these ports, the data is scanned, parsed and sent to SnappyFlow  "},{"title":"Monitoring pods using Prometheus exporter","type":1,"pageTitle":"Prometheus Exporter","url":"docs/Integrations/kubernetes/prometheus_exporter#monitoring-pods-using-prometheus-exporter","content":""},{"title":"Pre-requisites","type":1,"pageTitle":"Prometheus Exporter","url":"docs/Integrations/kubernetes/prometheus_exporter#pre-requisites","content":"Ensure sfPod is running and has access privileges to namespace in which application pod is running Ensure sfPod has access to ports exposing Prometheus exporters  "},{"title":"Enable access to Prometheus exporter","type":1,"pageTitle":"Prometheus Exporter","url":"docs/Integrations/kubernetes/prometheus_exporter#enable-access-to-prometheus-exporter","content":"Add Prometheus exporter container as a sidecar in the application pod. Pls see example below for Prometheus exporter pod. sfPod needs to access the Prometheus exporter on the exported port, which should be exposed in pod’s service PostgreSQL Statefulset YAML PostgreSQL Service YAML After setup of Prometheus exporter container, please verify connectivity using: curl service_ip: 9187 curl service_ip: 5432 Copy "},{"title":"Tag applications with Labels","type":1,"pageTitle":"Prometheus Exporter","url":"docs/Integrations/kubernetes/prometheus_exporter#tag-applications-with-labels","content":"note Applying labels are key to monitoring in SnappyFLow. Endpoints are organized in a hierarchy as per the labels defined. Add labels snappyflow/projectName and snappyflow/appName if the application pods are already running, use the following kubectl commands to tag your application pods with the appropriate tags: kubectl label pods <pod_name> snappyflow/projectname=<project_name> --namespace<appnamespace>kubectl label pods <pod_name> snappyflow/appname=<app_name> --namespace<appnamespace> Copy To automatically apply the right labels for the new pods which get created due to various reasons such as upgrades, restarts etc, apply labels to pod templates. If you are using helm chart, a best practice is to define labels in values.yaml and use the label parameters in pod template section of Deployment, StatefulSet, DaemonSet or other Kubernetes controller. "},{"title":"List of Prometheus exporters supported by sfPod","type":1,"pageTitle":"Prometheus Exporter","url":"docs/Integrations/kubernetes/prometheus_exporter#list-of-prometheus-exporters-supported-by-sfpod","content":"Plugins\tExporter Links\tservice_discovery_regex\tDocker imageapache\tExporter Link\t[\"apache_accesses_total\",\"apache_+\"]\tDocker image elasticsearch\tExporter Link\t[\"elasticsearch_+\"]\tDocker image haproxy\tExporter Link\t[\"haproxy_+\"]\tDocker image jmx\tExporter Link\t[\"jmx_exporter_build_info\",\"jmx_+\",\"java_lang_+\"]\tDocker image kafka-connect-j9\tExporter Link\t[\"kafka_connect+\",\"java_lang_+\",\"java_lang_memorymanager_valid_j9+\"]\tDocker image kafka-connect\tExporter Link\t[\"kafka_connect+\",\"java_lang_+\",\"java_lang_garbagecollector_collectiontime_g1_young_generation\"]\tDocker image kafka-jmx\tExporter Link\t[\"kafka_server_+\",\"kafka_network_+\",\"java_lang_+\"]\tDocker image kafka-rest-j9\tExporter Link\t[\"kafka_rest+\",\"java_lang_+\",\"java_lang_memorymanager_valid_j9+\"]\tDocker image kafka-rest\tExporter Link\t[\"kafka_rest+\",\"java_lang_+\",\"java_lang_garbagecollector_collectiontime_g1_young_generation\"]\tDocker image kafka\tExporter Link\t[\"kafka_topic_+\"]\tDocker image linux\tExporter Link\t[\"node_cpu_+\",\"node_disk_+\",\"node_procs_+\"]\tDocker image mongod\tExporter Link\t[\"mongodb_+\"]\tDocker image mysql\tExporter Link\t[\"mysql_global_+\",\"mysql_version_+\"]\tDocker image nginx\tExporter Link\t[\"nginx_+\"]\tDocker image nodejs\tNo exporter. Using code instrumentation\t[\"nodejs_+\"] postgres\tExporter Link\t\"pg_stat_+\"\tDocker image zookeeper-jmx\tExporter Link\t[\"zookeeper_+\",\"java_lang_\"]\tDocker image "},{"title":"Monitoring JAVA applications running in Kubernetes","type":0,"sectionRef":"#","url":"docs/Integrations/java/java_kubernetes","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Monitoring JAVA applications running in Kubernetes","url":"docs/Integrations/java/java_kubernetes#overview","content":"Java applications running in Kubernetes can be monitored in SnappyFlow using two approaches: sfKubeAgent as sidecar container. Prometheus exporter  "},{"title":"Java monitoring with sfKubeAgent","type":1,"pageTitle":"Monitoring JAVA applications running in Kubernetes","url":"docs/Integrations/java/java_kubernetes#java-monitoring-with-sfkubeagent","content":"In this option, the Java application should be run with Jolokia agent and sfKubeAgent running as a sidecar container and fetches metrics via Jolokia port. Refer to sfKubeAgent Overview "},{"title":"Prerequisites","type":1,"pageTitle":"Monitoring JAVA applications running in Kubernetes","url":"docs/Integrations/java/java_kubernetes#prerequisites","content":"Copy Jolokia JAR into docker image Run the java application with Jolokia JAR in docker image: -javaagent:/<path_jolokia_jar>/jolokia-jvm-<version>-agent.jar Copy "},{"title":"Configurations","type":1,"pageTitle":"Monitoring JAVA applications running in Kubernetes","url":"docs/Integrations/java/java_kubernetes#configurations","content":"Run sfKubeAgent with JVMJolokia plugin, which is specified using the config map shown below: apiVersion: v1 kind: ConfigMap metadata: name: jvm-configmap data: config.yaml: |- key: <profile_key> metrics: plugins: name: jvmjolokia enabled: true interval: 300 config: ip: 127.0.0.1 protocol: http port: <userDefinedJolokiaPort> context: jolokia monitorDeadlocks: false deadLockMonitoringInterval: 300 Copy The example illustrates instantiating sfKubeAgent with jvm-configmap. sfAKubeAgent talks to the Java application via userDefinedJolokiaPort (this example used 8778) kind: Pod apiVersion: v1 metadata: name: my-first-pod-1 labels: snappyflow/appname: <app_name> snappyflow/projectname: <project_name> spec:containers: name: java-container image: <docker_id>/<docker_image>:<tag> ports: name: jolokiaport containerPort: <userDefinedJolokiaPort> Snappyflow's sfkubeagent container name: java-sfagent image: snappyflowml/sfagent:latest imagePullPolicy: Always command: /app/sfagent -enable-console-log env: name: APP_NAME value: <app_name> name: PROJECT_NAME value: <project_name> volumeMounts: name: configmap-jvm mountPath: /opt/sfagent/config.yaml subPath: config.yaml volumes: name: configmap-jvm configMap: name: jvm-configmap Copy "},{"title":"Viewing data and dashboards","type":1,"pageTitle":"Monitoring JAVA applications running in Kubernetes","url":"docs/Integrations/java/java_kubernetes#viewing-data-and-dashboards","content":"Data generated by plugin can be viewed in browse data page inside the respective application under plugin=jvm_jolokia and documentType=jvm Dashboard for this data can be instantiated by Importing dashboard template JVM to the application dashboard  "},{"title":"Troubleshooting","type":1,"pageTitle":"Monitoring JAVA applications running in Kubernetes","url":"docs/Integrations/java/java_kubernetes#troubleshooting","content":"Check if the Jolokia port is accessible From inside the application container, run a curl command to the userDefinedJolokiaPort. curl http://localhost:<userDefinedJolokiaPort> Copy Check the logs in sfKubeAgent container for any errors "},{"title":"JVM Monitoring with Prometheus exporter","type":1,"pageTitle":"Monitoring JAVA applications running in Kubernetes","url":"docs/Integrations/java/java_kubernetes#jvm-monitoring-with-prometheus-exporter","content":"Refer to Prometheus Exporter Overview. Prometheus exporter is deployed as a sidecar container in the application pod and connects to the JMX target exposed by the application to scrape the metrics. sfPod polls Prometheus exporter to scrape the metrics. "},{"title":"Pre-requisites","type":1,"pageTitle":"Monitoring JAVA applications running in Kubernetes","url":"docs/Integrations/java/java_kubernetes#pre-requisites","content":"sfPod can access Prometheus exporter at Service IP: userDefinedPrometheusPort "},{"title":"Configurations","type":1,"pageTitle":"Monitoring JAVA applications running in Kubernetes","url":"docs/Integrations/java/java_kubernetes#configurations-1","content":"Run Java application with JMX options: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port= <userDefinedJMXPort> -Dcom.sun.management.jmxremote.authenticate=false - Dcom.sun.management.jmxremote.ssl=false Copy Start the Prometheus exporter with java -jar jmx_prometheus_httpserver.jar <userDefinedPrometheusPort> <exporterConfigFile> Copy Configurations are passed using config map: apiVersion: v1 kind: ConfigMap metadata: labels: snappyflow/appname: <app_name> snappyflow/projectname: <project_name> data: jmx-config.yaml: | --- jmxUrl: service:jmx:rmi:///jndi/rmi://127.0.0.1:<userDefinedJMXPort>/jmxrmi ssl: false rules: - pattern: '.*' Copy Prometheus exporter interfaces to JMX via userDefinedJMXPort. Example below uses 555S as the port. Prometheus exporter exposes userDefinedPrometheusPort for scraping. Example uses 5556 as the port Pod definition YAML that illustrates the configuration for Java application and exporter kind: Pod apiVersion: v1 metadata: name: my-first-pod labels: snappyflow/appname: <app_name> snappyflow/projectname: <project_name> spec: containers: name: app-container image: <docker_id>/<docker_image>:<tag> command: sh -c -x java -jar -Dcom.sun.management.jmxremote - Dcom.sun.management.jmxremote.port=<userDefinedJMXPort> - Dcom.sun.management.jmxremote.authenticate=false - Dcom.sun.management.jmxremote.ssl=false <application_jar> name: \"exporter-container\" image: \"bitnami/jmx-exporter:latest\" imagePullPolicy: command: sh -c -x java -jar jmx_prometheus_httpserver.jar <userDefinedPrometheusPort> /tmp/jmx-config.yaml ports: name: exporter-port containerPort: <userDefinedPrometheusPort> volumeMounts: name: configmap-jmx mountPath: /tmp volumes: name: configmap-jmx configMap: name: jmx-configmap Copy "},{"title":"Viewing data and dashboards","type":1,"pageTitle":"Monitoring JAVA applications running in Kubernetes","url":"docs/Integrations/java/java_kubernetes#viewing-data-and-dashboards-1","content":"Data generated by plugin can be viewed in “browse data” page inside the respective application under plugin=‘kube-prom-jmx‘ and documentType=‘jmxStats’Dashboard for this data can be instantiated by Importing dashboard template “JVM” to the application dashboard.  "},{"title":"Troubleshooting","type":1,"pageTitle":"Monitoring JAVA applications running in Kubernetes","url":"docs/Integrations/java/java_kubernetes#troubleshooting-1","content":"Check if the JMX port is accessible .From inside the application container, run a curl command to the userDefinedJMXPort. curl http://localhost:<userDefinedJMXPort> Copy Check if metrics are getting scraped. From inside the exporter container, run a curl command to the userDefinedPrometheusPort curl http://localhost:<userDefinedPrometheusPort>/metrics Copy "},{"title":"Monitoring Application Pods with sfKubeAgent","type":0,"sectionRef":"#","url":"docs/Integrations/kubernetes/sfkubeagent_installation","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Monitoring Application Pods with sfKubeAgent","url":"docs/Integrations/kubernetes/sfkubeagent_installation#overview","content":"sfKubeAgent is sfAgent packaged as a container and run as a sidecar within a Kubernetes application pod. It can be configured to collect both application metrics and logs similar to the way sfAgent does. "},{"title":"Integrating sfKubeAgent to application pods","type":1,"pageTitle":"Monitoring Application Pods with sfKubeAgent","url":"docs/Integrations/kubernetes/sfkubeagent_installation#integrating-sfkubeagent-to-application-pods","content":"Instantiate sfKubeAgent docker image in the pod Mount sfKubeAgent config map to the container. Config.yaml file used here is similar to the one used for sfAgent. Configurations for specific applications or log types can be found in Integrations section Pass parameters projectName and appName through container’s yaml file. These are mandatory tags and SnappyFlow uses these tags to organize the end-points in a project/ application hierarchy Mount log paths that need to be monitored to sfKubeAgent container in the correct path "},{"title":"Example:","type":1,"pageTitle":"Monitoring Application Pods with sfKubeAgent","url":"docs/Integrations/kubernetes/sfkubeagent_installation#example","content":"Below is an example of sfKubeAgent yaml that monitors JVM and Syslog in an application pod. "},{"title":"Pod description YAML","type":1,"pageTitle":"Monitoring Application Pods with sfKubeAgent","url":"docs/Integrations/kubernetes/sfkubeagent_installation#pod-description-yaml","content":"kind: PodapiVersion: v1metadata: name: jvm-pod labels: snappyflow/appname: test snappyflow/projectname: test-new-1spec: containers: - name: java-container image: ruchira27/jolokia:latest ports: - name: jolokiaport containerPort: 8778 # Snappyflow's sfkubeagent container - name: java-sfagent image: snappyflowml/sfagent:latest imagePullPolicy: Always command: - /app/sfagent - -enable-console-log env: - name: APP_NAME value: test - name: PROJECT_NAME value: test-new-1 volumeMounts: - name: configmap-jmx mountPath: /opt/sfagent/config.yaml subPath: config.yaml - name: varlog mountPath: /var/log volumes: - name: configmap-jmx configMap: name: jmx-configmap - name: varlog hostPath: path: /var/log Copy "},{"title":"Config Map","type":1,"pageTitle":"Monitoring Application Pods with sfKubeAgent","url":"docs/Integrations/kubernetes/sfkubeagent_installation#config-map","content":"apiVersion: v1kind: ConfigMapmetadata: name: jmx-configmapdata: config.yaml: |- key: Hc0cioeml0Sv7b7MbC+N56DKjygUlcvtP3wLtoUQitk3hw3/SevFv5loicDL9cCJDz3fImeLCuR1MrM/un4z+G2gELVeapNVCh96RhqSDvrV4MV9jMiuGi8RCa8MEj6KzAsvxnBPotbYKiM+11cm0xWOZ7K5G0C6J6T+SLX2/xk9us3BN2MhnBCH1N3xGhlDrNAy7j+KLSKsroiZcDw87iFjSaUzt0ADhCEwEJV3JBLZc2xpSM+n1hm3e4HHnVhaXcOi3Fcb9qD280Ya15t7eTsJywHyhKPcNKXpqF0OGVolLEUDc2vwklHGHIZXHF9hY/+/anS9+VSfhVpBNKVsDb+hDCLJbB8uBivJ9idRcnMvGkhir4kAUcsryCgvpay0ghqKZkjQ7zuhzKYW4/szHoXv+8g/Gn+nnxu3yFAa4aTOq6/AMNCA49S9EmU9Tn2yr+dUhiheWhKWFCTc8jd7vowehcPstNW1t8+SMfERkTqSKo1I/PSG0MGm3vrAa2yfU2GwnsyJnROSF/ylSY5JjTBlmfp7ZozKO8XPc7q+vaMwKEQzcDSqpSE26gOVMxrkYD2ksE/BQPbO2X1YTwlOqHSbr9Z0E5XOJXBSmgT7it7BgBCNro0/YcpALdoyEsJr4FBzM0K4ZwZNpnbDrbs0UIKLISaSGkYGAGBtuEXrusQ= metrics: plugins: - name: jvmjolokia enabled: true interval: 300 config: ip: 127.0.0.1 protocol: http port: 8778 context: jolokia monitorDeadlocks: false deadLockMonitoringInterval: 300 logging: plugins: - name: linux-syslog enabled: true config: log_level: - error - warning - info log_path: /var/log/auth.log,/var/log/messages,/var/log/secure Copy "},{"title":"Monitoring MongoDB","type":0,"sectionRef":"#","url":"docs/Integrations/mongodb","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Monitoring MongoDB","url":"docs/Integrations/mongodb#overview","content":"MongoDB plugin is an sfAgent plugin used to collect mongodb metrics. It uses serverStatus, dbstats command to retrieve the statistics. In that it collects the metrics like number of collections in db, number of insert , delete, update operations, read latency, write latency and other DB stats. "},{"title":"Prerequisites","type":1,"pageTitle":"Monitoring MongoDB","url":"docs/Integrations/mongodb#prerequisites","content":"sfAgent requires access to MongoDB system tables to collect metrics. This will require adding user credentials to sfAgent plugin configuration. User role and credentials can be created with the procedure mentioned below Open /etc/mongod.conf file and comment security config if it exists. This is needed to make changes to config file and will be uncommented in subsequent step #security: #authorization: enabled Copy Start mongodb service with command service mongod start Check mongodb status is active using command service mongod status Create mongostatRole and listDatabases roles Use admin Execute the command below db.createRole( { role: \"mongostatRole\", privileges: [ { resource: { cluster: true }, actions: [ \"serverStatus\" ] }], roles: [] }) db.createRole( { role: \"listDatabases\", privileges: [ { resource: { cluster: true }, actions: [ \"listDatabases\" ] }], roles: [] }) Copy Create new user with clusterMonitor , mongostatRole and listDatabases roles db.createUser( { user:\"Newuser\", pwd: \"pass\", roles : [ { role : \"mongostatRole\", db : \"admin\" }, { role : \"listDatabases\", db :\"admin\" }, { role : \"clusterMonitor\", db : \"admin\" } ] } ) Copy Exit mongo using exit command. Open /etc/mongod.conf file and uncomment security config if exists else add the config. security: authorization: enabled Copy Restart mongodb using command service mongod restart Use the created mongo user credentials in sfagent config and start sfagent. "},{"title":"Configuration","type":1,"pageTitle":"Monitoring MongoDB","url":"docs/Integrations/mongodb#configuration","content":"sfAgent section provides steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: <Profile_key>tags: Name: <instance_name> appName: <app_name> projectName: <project_name>metrics: plugins: - name: mongodb enabled: true interval: 300 config: port: 27017 host: localhost username: Newuser password: pass Copy Viewing data and dashboards Data collected by plugins can be viewed in SnappyFlow’s browse data section Metrics plugin: mongodbdocumentType: dbStats, systemInfo, operationalMetricsDashboard template:  "},{"title":"See Also","type":1,"pageTitle":"Monitoring MongoDB","url":"docs/Integrations/mongodb#see-also","content":"MySQL PostgresDB "},{"title":"Centralized Logging of Application Pod Logs","type":0,"sectionRef":"#","url":"docs/Integrations/kubernetes/centralized_logging_of_application_pod_logs","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Centralized Logging of Application Pod Logs","url":"docs/Integrations/kubernetes/centralized_logging_of_application_pod_logs#overview","content":"SnappyFlow can collect & parse application logs from pods in 2 ways: Collect logs locally by running sfKubeAgent as a sidecar container inside application pod Collect logs centrally through sfPod, which is explained in this page  "},{"title":"Procedure for Centralized Logging","type":1,"pageTitle":"Centralized Logging of Application Pod Logs","url":"docs/Integrations/kubernetes/centralized_logging_of_application_pod_logs#procedure-for-centralized-logging","content":"User runs a busybox sidecar container in the application pod with log files mounted to the container. Busybox tails & streams the application logs to stdout Add SnappyFlow labels: snappyflow/projectName and snappyflow/appName: These are mandatory labels for SnappyFlow monitoring. sfPod collects logs only from pods that have these labels and collected logs are organized under projectName/appName hierarchy in SnappyFlow snappyflow/component: This label is used to signal to sfPod on which parser to apply to parse the logs. List of standard parsers packaged with sfPod. If no label is present, sfPod will apply SnappyFlow’s generic parser which collects the whole log line as a message. sfPod runs as daemon-set in all the Kubernetes data nodes. It picks up logs from stdout of tagged pods, parses the logs based on component tag and ships parsed logs to SnappyFlow under projectName/appName hierarchy. "},{"title":"How to tag application Pods with project and application name labels","type":1,"pageTitle":"Centralized Logging of Application Pod Logs","url":"docs/Integrations/kubernetes/centralized_logging_of_application_pod_logs#how-to-tag-application-pods-with-project-and-application-name-labels","content":""},{"title":"Running Pods","type":1,"pageTitle":"Centralized Logging of Application Pod Logs","url":"docs/Integrations/kubernetes/centralized_logging_of_application_pod_logs#running-pods","content":"Use the following kubectl commands to tag your application pods with the appropriate tags: kubectl label pods <pod_name> snappyflow/projectname=<project_name> --namespace <appnamespace>kubectl label pods <pod_name> snappyflow/appname=<app_name> --namespace <appnamespace> Copy "},{"title":"Automatically apply labels to new Pods","type":1,"pageTitle":"Centralized Logging of Application Pod Logs","url":"docs/Integrations/kubernetes/centralized_logging_of_application_pod_logs#automatically-apply-labels-to-new-pods","content":"To automatically apply right labels for new pods which get created due to various reasons such as upgrade, restarts etc. apply labels to pod templates. If you are using helm chart, best practice is to define labels in values.yaml and use these labels parameter in pod template section of Deployment, StatefulSet, Daemonset or other Kubernetes controller. Below is one example values.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: sfapm-ui labels: app: sfapm role: uispec: replicas: 1 selector: matchLabels: app: sfapm role: ui template: metadata: labels: app: sfapm role: ui snappyflow/appname: demo-application snappyflow/projectname: demo-project Copy "},{"title":"Example","type":1,"pageTitle":"Centralized Logging of Application Pod Logs","url":"docs/Integrations/kubernetes/centralized_logging_of_application_pod_logs#example","content":""},{"title":"Centralized logging for nginx-access logs","type":1,"pageTitle":"Centralized Logging of Application Pod Logs","url":"docs/Integrations/kubernetes/centralized_logging_of_application_pod_logs#centralized-logging-for-nginx-access-logs","content":"Configure Nginx to drop logs in the required format in /var/log/nginx folder using config map Add busy box container to tail logs from access logs and stream to stdout Signal to sfPod to use “nginx” parser using label “component” Nginx Pod YAML kind: Pod apiVersion: v1 metadata: name: my-first-pod labels: snappyflow/appname: <app_name> snappyflow/projectname: <project_name> snappyflow/component: nginx spec: containers: - name: nginx-container image: nginx:latest imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP volumeMounts: - name: varlog mountPath: /var/log/nginx - name: nginx-config mountPath: /etc/nginx/nginx.conf subPath: nginx.conf - name: busybox-container image: busybox command: [\"/bin/sh\", \"-c\"] args: [\"tail -n+1 -f /var/log/nginx/access1.log\"] volumeMounts: - name: varlog mountPath: /var/log/nginx volumes: - name: nginx-config configMap: name: nginx-config - name: varlog emptyDir: {} Copy Config map for Nginx configuration apiVersion: v1 kind: ConfigMap metadata: name: nginx-configmap labels: snappyflow/appname: <app_name> snappyflow/projectname: <project_name> data: nginx.conf: | worker_processes 5; events { worker_connections 4096; } http { default_type application/octet-stream; log_format upstream_time '$remote_addr:$remote_port $remote_user [$time_local] ' '\"$request\" $status $body_bytes_sent ' '\"$http_referer\" \"$http_user_agent\" \"$http_referer\" ' 'rt=$request_time uct=$upstream_connect_time uht=$upstream_header_time urt=$upstream_response_time'; server { listen 80; error_log /var/log/nginx/error1.log; access_log /var/log/nginx/access1.log upstream_time; location /nginx_status { stub_status; } } } Copy "},{"title":"overview","type":0,"sectionRef":"#","url":"docs/Integrations/mysql/overview","content":"overview MySQL monitoring on SnappyFlow is available for the following platforms Instances# Kubernetes#","keywords":""},{"title":"Overview","type":0,"sectionRef":"#","url":"docs/Integrations/nginx/overview","content":"Overview NGINX monitoring on SnappyFlow is available for the following platforms Instances# Kubernetes#","keywords":""},{"title":"Coming Soon!","type":0,"sectionRef":"#","url":"docs/Integrations/os/linux/netstat","content":"Coming Soon!","keywords":""},{"title":"SnappyFlow Linux Integrations","type":0,"sectionRef":"#","url":"docs/Integrations/os/linux/overview","content":"SnappyFlow Linux Integrations With SnappyFlow, you can monitor Linux Instances with sfAgent Linux OS running on Instances LSOF PSUtil Netstat","keywords":""},{"title":"Monitoring Linux OS","type":0,"sectionRef":"#","url":"docs/Integrations/os/linux/linux_os","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Monitoring Linux OS","url":"docs/Integrations/os/linux/linux_os#overview","content":"OS monitoring is the most commonly needed and most important aspect of monitoring. SnappyFlow provides a comprehensive monitoring of Linux OS through multiple plugins. Linux base metric plugin provides following data: CPU Static and Dynamic Metrics Memory Metrics Disk IO Metrics Network IO Metrics TCP Metrics Syslog logging plugin  "},{"title":"Configuration","type":1,"pageTitle":"Monitoring Linux OS","url":"docs/Integrations/os/linux/linux_os#configuration","content":"Refer to sfAgent section for steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: <profile_key> tags: Name: <name> appName: <app_name> projectName: <project_name> metrics: plugins: - name: linux enabled: true interval: 30 logging: plugins: - name: linux-syslog enabled: true config: log_level: - error - warning - info log_path: /var/log/syslog,/var/log/auth.log,/var/log/messages,/var/log/secur Copy "},{"title":"Viewing data and dashboards","type":1,"pageTitle":"Monitoring Linux OS","url":"docs/Integrations/os/linux/linux_os#viewing-data-and-dashboards","content":"Data collected by plugins can be viewed in SnappyFlow’s browse data section under metrics or logs or trace depending on the plugin Linux metrics data plugin= linux documentType cpu_static cpu_utilram_util disk_stats nic_stats tcp_stats Syslog data plugin= linux-syslog documentType= syslog  "},{"title":"Test Matrix","type":1,"pageTitle":"Monitoring Linux OS","url":"docs/Integrations/os/linux/linux_os#test-matrix","content":"Centos: 7.x RHEL: 7.x Ubuntu: 14.x, 16.x "},{"title":"See Also","type":1,"pageTitle":"Monitoring Linux OS","url":"docs/Integrations/os/linux/linux_os#see-also","content":"LSOF PSUTIL NETSTAT Custom plugins using StatsD Prometheus Integration "},{"title":"MySQL on Instances","type":0,"sectionRef":"#","url":"docs/Integrations/mysql/mysql_instances","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"MySQL on Instances","url":"docs/Integrations/mysql/mysql_instances#overview","content":"MySQL on instances is monitored using sfAgent configured with MySQL plugin  "},{"title":"Metrics plugin","type":1,"pageTitle":"MySQL on Instances","url":"docs/Integrations/mysql/mysql_instances#metrics-plugin","content":"Collects metric data organized in following documentType under metrics index:  serverDetails databaseDetails tableDetails  "},{"title":"Logger plugin","type":1,"pageTitle":"MySQL on Instances","url":"docs/Integrations/mysql/mysql_instances#logger-plugin","content":"collects general logs and slow query logs. General logs are sent to log index whereas slow queries are sent to metrics index under documentType:mysqlSlowQueryLogs  "},{"title":"Pre-requisites ","type":1,"pageTitle":"MySQL on Instances","url":"docs/Integrations/mysql/mysql_instances#pre-requisites","content":""},{"title":"Enable MySQL configurations","type":1,"pageTitle":"MySQL on Instances","url":"docs/Integrations/mysql/mysql_instances#enablemysqlconfigurations","content":"Logging needs to be configured in the mysql.conf.d/mysqld.cnf file. In the configuration file uncomment and configure the variables shown below:  show_compatibility_56 = On #neeeded for metrics log-error=/var/log/mysqld.log pid-file=/var/run/mysqld/mysqld.pid general_log_file=/var/log/mysql/mysql.log general_log=1 Copy This file can be located by executing the command as shown below:  mysqld --verbose --help | grep -A 1 \"Default options\" Copy E.g. output is /etc/my.cnf /etc/mysql/my.cnf ~/my.cnf. User needs to check each of the files for the configuration Alternatively, login to mysql with root user and execute below commands  SET GLOBAL general_log = 'ON'; SET GLOBAL general_log_file= '/path/filename'; Copy "},{"title":"Enable Slow Query Logs  ","type":1,"pageTitle":"MySQL on Instances","url":"docs/Integrations/mysql/mysql_instances#enable-slow-query-logs","content":"In mysqld.cnf file, uncomment and configure the variables shown below:  slow_query_log= 1 slow_query_log_file=/var/log/mysql/mysql-slow.log Copy Or, login to mysql with root user and execute below commands  SET GLOBAL slow_query_log = 'ON'; SET GLOBAL long_query_time = 100; SET GLOBAL slow_query_log_file = '/path/filename'; Copy note By Default /var/log/mysql directory is not present in centos, so we must create and provide ownership of that directory as mysql chown -R mysql:mysql /var/log/mysql Copy "},{"title":"Set access permissions","type":1,"pageTitle":"MySQL on Instances","url":"docs/Integrations/mysql/mysql_instances#set-access-permissions","content":"Username used for DB access should have appropriate permissions  grant select on information_schema.* to 'username' identified by 'password'; grant select on performance_schema.* to 'username' identified by 'password'; Copy note Root user has these permissions by default  "},{"title":"Configuration ","type":1,"pageTitle":"MySQL on Instances","url":"docs/Integrations/mysql/mysql_instances#configuration","content":"Refer to sfAgent section for steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory  metrics: plugins: - name: mysql enabled: true interval: 60 config: documentsTypes: - databaseDetails - serverDetails - tableDetails host: 127.0.0.1 password: USERad@123$ port: 3306 user: root logging: plugins: - name: mysql-error enabled: true config: log_level: - error - warning - note log_path: /var/log/mysql/error.log, /var/log/mysql/mysql-error.log, /var/log/mysqld.err, /var/log/mysqld.log - name: mysql-general enabled: true config: log_path: /var/log/mysql/mysql.log , /var/log/mysql.log, /var/log/mysqld.log, /var/lib/mysql/ip-*.log - name: mysql-slowquery enabled: true config: log_path: /var/lib/mysql/ip-*slow.log, /var/log/mysql/mysql-slow.log Copy Viewing data and dashboards  Data generated by plugin can be viewed in browse data page inside the respective application under plugin=mysql and documentType= serverDetails Dashboard for this data can be instantiated by Importing dashboard template MySQL to the application dashboard "},{"title":"Monitoring Nginx on Instances","type":0,"sectionRef":"#","url":"docs/Integrations/nginx/nginx_instance","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Monitoring Nginx on Instances","url":"docs/Integrations/nginx/nginx_instance#overview","content":"Nginx monitoring involves monitoring of the following elements: Nginx Access Logs Nginx Error Logs Nginx Server Health "},{"title":"Pre-requisites","type":1,"pageTitle":"Monitoring Nginx on Instances","url":"docs/Integrations/nginx/nginx_instance#pre-requisites","content":"Ensure Nginx access logs are in format expected by sfAgent parser. Edit nginx conf file /etc/nginx/nginx.conf and set log format as follows: '$remote_addr $remote_user [$time_local] ' '\"$request\" $status $body_bytes_sent ' '\"$http_referer\" \"$http_user_agent\" ua=\"$upstream_addr\" ' 'rt=$request_time uct=$upstream_connect_time uht=$upstream_header_time urt=$upstream_response_time'; Copy Sample: log_format snappyflow '$remote_addr $remote_user [$time_local] ' '\"$request\" $status $body_bytes_sent ' '\"$http_referer\" \"$http_user_agent\" ua=\"$upstream_addr\" ' 'rt=$request_time uct=$upstream_connect_time uht=$upstream_header_time urt=$upstream_response_time'; access_log /var/log/nginx/access.log snappyflow buffer=16k flush=5s; Copy After configuring log format, the expected log entry would be: 172.31.72.81 - [01/Jul/2020:03:36:04 +0000] \"POST /owners/6/edit HTTP/1.1\" 504 167 \"-\" \"Apache-HttpClient/4.5.7 (Java/1.8.0_252)\" ua=\"-\" rt=60.004 uct=- uht=- urt=60.004 Copy Description of log fields is as follows: log_format snappyflow '$remote_addr:$remote_port $remote_user .... 'rt=$request_time uct=$upstream_connect_time uht=$upstream_header_time urt=$upstream_response_time rs=$request_length'; Copy Enable Nginx status module: This is required to monitor Nginx server health Open source Nginx exposes several basic metrics about server activity on a simple status page, provided that you have HTTP Stub Status Module enabled. To check if the module is already enabled, run: nginx -V 2>&1 | grep -o with-http_stub_status_module Copy The status module is enabled if you see with-http_stub_status_module as output in the terminal. In order to enable mod_status , you will need to enable the status module. You can use the --with-http_stub_status_module configuration parameter when building Nginx from source: ./configure \\ … \\ --with-http_stub_status_module make sudo make install Copy After verifying the module is enabled, you will also need to modify your Nginx configuration to set up a locally accessible URL (e.g., /stats) for the status page: server { location /stats { stub_status; access_log off; allow 127.0.0.1; deny all; } } Copy note The server blocks of Nginx config are usually found not in the master configuration file (e.g., /etc/nginx/nginx.conf) but in supplemental configuration files that are referenced by the master config. To find the relevant configuration files, first locate the master config by running: nginx -t Open the master configuration file listed, and look for lines that begin with “include” near the end of the http block, e.g.: include /etc/nginx/conf.d/*.conf; In one of the referenced config files you should find the main server block, which you can modify as above to configure Nginx metrics reporting. After changing any configurations, reload the configs by executing: nginx -s reload Now you can view the status page to see your metrics:http://127.0.0.1/stats "},{"title":"Configuration","type":1,"pageTitle":"Monitoring Nginx on Instances","url":"docs/Integrations/nginx/nginx_instance#configuration","content":"sfAgent section provides steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: <profile key> generate_name: true tags: Name: <unique instance name or will be generated from IP> appName: <add application name> projectName: <add project name> metrics: plugins: - name: nginx enabled: true interval: 300 config: port: 80 secure: false location: ‘stats’ logging: plugins: - name: nginx-access enabled: true config: geo_info: true log_path: /var/log/nginx/access.log, /var/log/nginx/access_log ua_parser: true - name: nginx-error enabled: true config: log_level: - emerg - alert - error log_path: /var/log/nginx/error.log, /var/log/nginx/error_log Copy "},{"title":"Viewing data and dashboards","type":1,"pageTitle":"Monitoring Nginx on Instances","url":"docs/Integrations/nginx/nginx_instance#viewing-data-and-dashboards","content":"Data generated by plugin can be viewed in “browse data” page inside the respective application under plugin=jvm and documentType=jvmDashboard for this data can be instantiated by Importing dashboard template JVM to the application dashboard. "},{"title":"PSUtil Monitoring","type":0,"sectionRef":"#","url":"docs/Integrations/os/linux/psutil","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"PSUtil Monitoring","url":"docs/Integrations/os/linux/psutil#overview","content":"PSUtil Metric plugin is an agent-based plugin that collects below data for each process running on the machine Process IDUsernameCPU (%)CPU timeMemory (%)Resident Memory (%)Virtual Memory (%)Elapsed timeProcessorState Code "},{"title":"Agent Configuration","type":1,"pageTitle":"PSUtil Monitoring","url":"docs/Integrations/os/linux/psutil#agent-configuration","content":"Refer to sfAgent section for steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: <profile_key> tags: Name: <name> appName: <app_name> projectName: <project_name> metrics: plugins: -name:psutil enabled:true interval:60 config: numprocess:10 sortby:pcpu Copy "},{"title":"Configuring parameters","type":1,"pageTitle":"PSUtil Monitoring","url":"docs/Integrations/os/linux/psutil#configuring-parameters","content":"note You can configure plugin to collect top 10 process which used High CPU (%) as in sample configuration. numprocess: Number of processes for which metrics have to be collected. Set numprocess to 0 or leave it empty to get metrics for all processes. Default is 15. sortby: Sorts the process by sortby field and selects the top N processes. Default value is pcpu. E.g. you can collect top 10 processes by CPU Util if the sortby field is pcpu. Possible values are, uname - Username pid - ProcessId psr - Processor pcpu - CPUPercent cputime - CPUTime pmem - Memory Percent rsz - Resident Memory vsz - Virtual Memory etime - Elapsed Time s - State code Copy "},{"title":"Viewing data and dashboards","type":1,"pageTitle":"PSUtil Monitoring","url":"docs/Integrations/os/linux/psutil#viewing-data-and-dashboards","content":"Data collected by plugin can be viewed in SnappyFlow’s browse data section under metrics plugin= psuti documentType = processStats Dashboard of psutil data can be rendered using Template= PSUTIL  "},{"title":"Test Matrix","type":1,"pageTitle":"PSUtil Monitoring","url":"docs/Integrations/os/linux/psutil#test-matrix","content":"Centos: 7.x RHEL: 7.x Ubuntu: 14.x, 16.x "},{"title":"See Also","type":1,"pageTitle":"PSUtil Monitoring","url":"docs/Integrations/os/linux/psutil#see-also","content":"Linux monitoringLSOFNETSTATCustom plugins using StatsDPrometheus Integration "},{"title":"SnappyFlow Integrations","type":0,"sectionRef":"#","url":"docs/Integrations/overview","content":"SnappyFlow Integrations SnappyFlow support a wide range of build in integrations to help you get started quickly. Linux Postgres Java MongoDB ActiveMQ Kafka Zookeper Kubernetes MySQL Nginx Apache StatsD","keywords":""},{"title":"LSOF (List of Open Files)","type":0,"sectionRef":"#","url":"docs/Integrations/os/linux/lsof","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"LSOF (List of Open Files)","url":"docs/Integrations/os/linux/lsof#overview","content":"LSOF (list open files) Metric plugin collects data for number of files opened by a process of different file descriptor types. "},{"title":"Prerequisite - Install lsof Command","type":1,"pageTitle":"LSOF (List of Open Files)","url":"docs/Integrations/os/linux/lsof#prerequisite----install-lsof-command","content":"lsof command needs to be installed before running this plugin. To install lsof in Centos / RHEL sudo yum install lsof Copy To install lsof in Ubuntu sudo apt-get install lsof Copy To verify installation, run the below command. lsof -v Copy Expected output: lsof version information: revision: 4.87 latest revision: ftp://lsof.itap.purdue.edu/pub/tools/unix/lsof/ latest FAQ: ftp://lsof.itap.purdue.edu/pub/tools/unix/lsof/FAQ latest man page: ftp://lsof.itap.purdue.edu/pub/tools/unix/lsof/lsof_man constructed: Tue Oct 30 16:28:19 UTC 2018 constructed by and on: mockbuild@x86-01.bsys.centos.org compiler: cc compiler version: 4.8.5 20150623 (Red Hat 4.8.5-36) (GCC) compiler flags: -DLINUXV=310000 -DGLIBCV=217 -DHASIPv6 -DHASSELINUX -D_FILE_OFFSET_BITS=64 -D_LARGEFILE64_SOURCE -DHAS_STRFTIME -DLSOF_VSTR=\"3.10.0\" -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic loader flags: -L./lib -llsof -lselinux system info: Linux x86-01.bsys.centos.org 3.10.0-693.17.1.el7.x86_64 #1 SMP Thu Jan 25 20:13:58 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux Copy note version may vary depending upon the Linux distribution. "},{"title":"Agent Configuration","type":1,"pageTitle":"LSOF (List of Open Files)","url":"docs/Integrations/os/linux/lsof#agent-configuration","content":"Refer to sfAgent section for steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: <profile_key> tags: Name: <name> appName: <app_name> projectName: <project_name> metrics: plugins: - name: lsof enabled: true interval: 600 config: completeStats: false numProcess: 5 sortFilter: DIR Copy "},{"title":"Configuration Details","type":1,"pageTitle":"LSOF (List of Open Files)","url":"docs/Integrations/os/linux/lsof#configuration-details","content":"LSOF plugin runs in two different modes: summary and complete stats. In summary mode, plugin returns only the count of files open of each file descriptor type (like DIR, CHR, REG etc.) at an aggregate level. Set completeStats: false for summary mode In completeStats mode, plugin returns the entire list of open files in the machine, process wise along with the process id. Set completeStats: true for this mode. Since the list of all opened files can be huge in number, plugin is by default configured in summary mode  Other configuration parameters include: numProcess: Number of top N processes with maximum number of files opened. For example, if numProcess is set to 5, it returns top 5 process stats with maximum number of files opened. Set numProcess to 0 to get all process details. Default value is 10. sortFilter: Selection of top N processes only for a particular file descriptor type. Following are the options available: none, CHR, DIR, REG, FIFO, IP, netlink, socket, a_inode. Default value is none. For example, to get top 10 processes with maximum directories opened, numProcess should be set to 10 and sortFilter as DIR. Set sort filter to none if no sorting is required.  note All the traffic related (IPv4, IPv6) file types are combined as IPv4/6 type and unix sockets into socket type "},{"title":"Viewing data and dashboards","type":1,"pageTitle":"LSOF (List of Open Files)","url":"docs/Integrations/os/linux/lsof#viewing-data-and-dashboards","content":"Data collected by plugins can be viewed in SnappyFlow’s browse data section under metrics section plugin: lsof documentType: lsofSummary, lsofstats Dashboard template: LSOF  "},{"title":"Test Matrix","type":1,"pageTitle":"LSOF (List of Open Files)","url":"docs/Integrations/os/linux/lsof#test-matrix","content":"Centos: 7.x RHEL: 7.x Ubuntu: 14.x, 16.x "},{"title":"See Also","type":1,"pageTitle":"LSOF (List of Open Files)","url":"docs/Integrations/os/linux/lsof#see-also","content":"Linux monitoringLSOFPSUTILNETSTATCustom plugins using StatsDPrometheus Integration "},{"title":"overview","type":0,"sectionRef":"#","url":"docs/Integrations/postgres/overview","content":"overview Postgres monitoring on SnappyFlow is available for the following platforms Instances# Kubernetes#","keywords":""},{"title":"sfAgent installation on Windows","type":0,"sectionRef":"#","url":"docs/Integrations/os/windows/sfagent_windows","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"sfAgent installation on Windows","url":"docs/Integrations/os/windows/sfagent_windows#overview","content":"Monitoring of Windows based application requires installation of a lightweight agent, sfAgent on Windows. sfAgent provides following features: Monitoring of various services based on specified configurationsLog parsing and collectionTrace Java, Python and Golang applications (check out sfTracing for details) "},{"title":"Supported Platforms","type":1,"pageTitle":"sfAgent installation on Windows","url":"docs/Integrations/os/windows/sfagent_windows#supported-platforms","content":"Windows Server 2012Windows Server 2016Windows Server 2019 "},{"title":"Install sfAgent on Windows","type":1,"pageTitle":"sfAgent installation on Windows","url":"docs/Integrations/os/windows/sfagent_windows#install-sfagent-on-windows","content":"Download the sfAgent executable from the link below Dowload sfAgent Run sfAgent.exe executable with Administrator privileges and complete the installation "},{"title":"Configure sfAgent on Windows","type":1,"pageTitle":"sfAgent installation on Windows","url":"docs/Integrations/os/windows/sfagent_windows#configure-sfagent-on-windows","content":"Navigate to sfAgent installed location (C:\\Program Files (x86)\\sfAgent)Open file sample.yamlAdd Key and edit configuration for metrics and loggerSave it and rename sample.yaml as config.yaml "},{"title":"Prerequisites","type":1,"pageTitle":"sfAgent installation on Windows","url":"docs/Integrations/os/windows/sfagent_windows#prerequisites","content":"Powershell.exe must be available in %PATH environment variableFor winjvm plugin, java should be installed and java path should be set in %PATH environment variable "},{"title":"Run sfAgent service","type":1,"pageTitle":"sfAgent installation on Windows","url":"docs/Integrations/os/windows/sfagent_windows#run-sfagent-service","content":"Open task manager and service tabSearch for service “sfAgent” and right click on it and click start to start serviceTo stop right click on running service and click stop "},{"title":"Standard Plugins and Log Parsers","type":1,"pageTitle":"sfAgent installation on Windows","url":"docs/Integrations/os/windows/sfagent_windows#standard-plugins-and-log-parsers","content":"sfAgent for Windows includes plugins and log parsers for a number of standard applications and operating system utilities. (documentation coming soon!) Category\tServicesWindows[Windows Server 2012 and above]\tCPU and RAM static and dynamic parameters, Windows WinPSUtil Web Tier\tIIS Server (Server Monitoring, Access & Error Logs) App Tier\tWinJVM, Apache Tomcat Database andDataflowElements\tMySQL, MS-SQL "},{"title":"sfAgent Installation on Linux","type":0,"sectionRef":"#","url":"docs/Integrations/os/linux/sfagent_linux","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"sfAgent Installation on Linux","url":"docs/Integrations/os/linux/sfagent_linux#overview","content":"It is a lightweight agent installed on VMs to collect metrics, logs and tracing data. "},{"title":"Supported Platforms","type":1,"pageTitle":"sfAgent Installation on Linux","url":"docs/Integrations/os/linux/sfagent_linux#supported-platforms","content":"ubuntu 18 lts ubuntu 16 lts centos 7 RHEL 7 "},{"title":"Installation","type":1,"pageTitle":"sfAgent Installation on Linux","url":"docs/Integrations/os/linux/sfagent_linux#installation","content":"Run the following commands to install sfAgent on VMs: wget https://raw.githubusercontent.com/snappyflow/apm-agent/master/install.sh -O install.shchmod +x install.shsudo ./install.sh Copy note sfAgent executes commands such docas iostat or jcmd to fetch metrics. In order to specific path to sfAgent use -p or --include-paths Example: ./install.sh -p /opt/jdk1.8.0_211/bin/ Copy To install sfAgent on multiple end-points using Ansible playbook, refer the following script at https://github.com/snappyflow/apm-agent "},{"title":"Pre-requisites","type":1,"pageTitle":"sfAgent Installation on Linux","url":"docs/Integrations/os/linux/sfagent_linux#pre-requisites","content":"sfAgent requires certain pre-requisites for monitoring. Common pre-requisites are mentioned below. Further, all pre-requisites and configurations needed for monitoring a specific application are mentioned under Integrations section. For Linux OS monitoring, install iostat sudo apt-get install sysstat Copy or sudo yum install sysstat Copy For JVM monitoring, install java headless service for jcmd & jmap command sudo apt-get install –y openjdk-12-jdk-headless Copy or sudo yum -y install java-1.8.0-openjdk-devel-1.8.0* Copy "},{"title":"Configure sfAgent on Linux","type":1,"pageTitle":"sfAgent Installation on Linux","url":"docs/Integrations/os/linux/sfagent_linux#configure-sfagent-on-linux","content":"sfAgent is configured through its config.yaml file. There are sections for metrics and logs where appropriate plugins with their configurations have to added to these sections. Below is an example: key: <add profile key here> generate_name: true tags: Name: <add name tag> appName: <add application name tag> projectName: <add project name tag> metrics: plugins: - name: linux enabled: true interval: 30 logging: plugins: - name: linux-syslog enabled: true config: log_level: - error - warning - info log_path: /var/log/auth.log,/var/log/messages,/var/log/secure - name: nginx-access enabled: true config: geo_info: true log_path: /var/log/nginx/access.log, /var/log/nginx/access_log ua_parser: true - name: nginx-error enabled: true config: log_level: - emerg - alert - error log_path: /var/log/nginx/error.log, /var/log/nginx/error_log Copy sfAgent can be either configured or manually. In an automatic configuration step, sfAgent discovers services running in a VM and automatically generates a default configuration for monitoring the discovered services. User can further modify the configurations as needed. Detailed configuration for a specific application types are present in Integrations section. Follow the steps below for automatic discovery & configuration  Run following commands to discover services and generate config: sudo su cd /opt/sfagent ./sfagent -generate-config cp config-generated.yaml config.yaml Copy Add the profile key and SnappyFlow tags in the configuration file. Copy profile key from SnappyFlow and update key: Set values for Name:, appName:, projectName: under tags: section Verify configuration and restart sfAgent ./sfagent -check-config service sfagent restart Copy "},{"title":"Upgrade sfAgent on Linux","type":1,"pageTitle":"sfAgent Installation on Linux","url":"docs/Integrations/os/linux/sfagent_linux#upgrade-sfagent-on-linux","content":"Run following commands to upgrade sfAgent: wget https://raw.githubusercontent.com/snappyflow/apm-agent/master/install.sh -O install.sh chmod +x install.sh sudo ./install.sh --upgrade Copy "},{"title":"Monitoring Nginx in Kubernetes","type":0,"sectionRef":"#","url":"docs/Integrations/nginx/nginx_kubernetes","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Monitoring Nginx in Kubernetes","url":"docs/Integrations/nginx/nginx_kubernetes#overview","content":"Nginx monitoring involves monitoring of the following elements: Nginx Access Logs Nginx Error Logs Nginx Server Health "},{"title":"Pre-reading","type":1,"pageTitle":"Monitoring Nginx in Kubernetes","url":"docs/Integrations/nginx/nginx_kubernetes#pre-reading","content":"Refer to the links below for generic approach to monitoring application metrics and logs in Kubernetes environment sfKubeAgent Prometheus Exporter Centralized Log Monitoring  Refer to Nginx monitoring on instances for sfAgent configurations "},{"title":"Configuration","type":1,"pageTitle":"Monitoring Nginx in Kubernetes","url":"docs/Integrations/nginx/nginx_kubernetes#configuration","content":""},{"title":"Configure Nginx server to enable monitoring","type":1,"pageTitle":"Monitoring Nginx in Kubernetes","url":"docs/Integrations/nginx/nginx_kubernetes#configure-nginx-server-to-enable-monitoring","content":"Configure format of access logs so that it can be parsed by SnappyFlow Enable Nginx status module to monitor Nginx server health These configurations can be achieved with the below ConfigMap: apiVersion: v1 kind: ConfigMap metadata: name: nginx-configmap labels: snappyflow/appname: <app_name> snappyflow/projectname: <project_name> data: nginx.conf: | worker_processes 5; events { worker_connections 4096; } http { default_type application/octet-stream; log_format upstream_time '$remote_addr:$remote_port $remote_user [$time_local] ' '\"$request\" $status $body_bytes_sent ' '\"$http_referer\" \"$http_user_agent\" \"$http_referer\" ' 'rt=$request_time uct=$upstream_connect_time uht=$upstream_header_time urt=$upstream_response_time'; server { listen 80; error_log /var/log/nginx/error1.log; access_log /var/log/nginx/access1.log upstream_time; location /nginx_status { stub_status; } } } Copy "},{"title":"sfKubeAgent","type":1,"pageTitle":"Monitoring Nginx in Kubernetes","url":"docs/Integrations/nginx/nginx_kubernetes#sfkubeagent","content":"sfKubeAgent is deployed as sidecar container in the NGINX pod and can be used to monitor Nginx server health as well as Access Logs & Error Logs. Below YAML files provide example for setting up NGINX monitoring with sfKubeAgent. sfKubeAgent ConfigMap (sfAgent-config.yaml) apiVersion: v1 kind: ConfigMap metadata: name: sfagent-configmap labels: snappyflow/appname: <app_name> snappyflow/projectname: <project_name> data: config.yaml: |+ --- key: \"<profile_key>\" metrics: plugins: - name: kube-sfagent-nginx enabled: true interval: 300 config: location: nginx_status port: 80 secure: false logging: plugins: - name: nginx-access enabled: true config: log_path: \"/var/log/nginx/access1.log\" - name: nginx-error enabled: true config: log_path: \"/var/log/nginx/error1.log\" Copy Pod description YAML running NGINX and sfKubeAgent apiVersion: v1 metadata: name: my-first-pod labels: snappyflow/appname: <app_name> snappyflow/projectname: <project_name> spec: containers: - name: nginx-container image: nginx:latest imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP volumeMounts: - name: varlog mountPath: /var/log/nginx - name: nginx-config mountPath: /etc/nginx/nginx.conf subPath: nginx.conf # Snappyflow's sfkubeagent container - name: nginx-sfagent image: snappyflowml/sfagent:latest imagePullPolicy: Always command: - /app/sfagent - -enable-console-log env: - name: APP_NAME value: <app_name> - name: PROJECT_NAME value: <project_name> volumeMounts: - name: nginx-sfagent-config mountPath: /opt/sfagent/config.yaml subPath: config.yaml - name: varlog mountPath: /var/log/nginx volumes: - name: nginx-sfagent-config configMap: name: sfagent-configmap - name: nginx-config configMap: name: nginx-configmap - name: varlog emptyDir: {} Copy "},{"title":"Centralized logging","type":1,"pageTitle":"Monitoring Nginx in Kubernetes","url":"docs/Integrations/nginx/nginx_kubernetes#centralized-logging","content":"Log monitoring (both access and error logs) can be implemented through Centralized Logging approach as well which does not require sfKubeAgent. Centralized logging however requires running a busybox container as a sidecar container to stream logs to container’s stdout. Add the label - snappyflow/component: nginx, which signals to apply Nginx to container’s stdout. "},{"title":"Viewing data and dashboards","type":1,"pageTitle":"Monitoring Nginx in Kubernetes","url":"docs/Integrations/nginx/nginx_kubernetes#viewing-data-and-dashboards","content":"Data generated by plugin can be viewed in browse data page inside the respective application under plugin=jvm and documentType=jvm Dashboard for this data can be instantiated by Importing dashboard template JVM to the application dashboard. "},{"title":"Archival","type":0,"sectionRef":"#","url":"docs/Log_management/archival","content":"","keywords":""},{"title":"Coming Soon!","type":1,"pageTitle":"Archival","url":"docs/Log_management/archival#coming-soon","content":""},{"title":"Monitoring Apache ZooKeeper running on Instances","type":0,"sectionRef":"#","url":"docs/Integrations/zookeeper","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Monitoring Apache ZooKeeper running on Instances","url":"docs/Integrations/zookeeper#overview","content":"Zookeeper afAgent Metric plugin helps in analyzing the efficiency of zookeeper infrastructure by providing key metrics like node count, packet count, latency, watch count etc. "},{"title":"Prerequisites","type":1,"pageTitle":"Monitoring Apache ZooKeeper running on Instances","url":"docs/Integrations/zookeeper#prerequisites","content":"Zookeeper Plugin is based on Jolokia agent which requires JMX monitoring to be enable locally. Following property needs to be included during the start of Zookeeper process -Dcom.sun.management.jmxremote Copy JCMD command must be installed in the machine Zookeeper ships with log4j support. Log4j property file (log4j.properties) is present in root folder of Zookeeper and has to be set as follows Enabling root logger and file appender where file appender can be of any type based on rolling strategy log4j.rootLogger=INFO, logfilelog4j.appender.logFile=org.apache.log4j.DailyRollingFileAppender Copy Specifying custom log file name along with its path , layout properties and data pattern log4j.appender.logFile.DatePattern='.'yyyy-MM-dd-HHlog4j.appender.logFile.File= <..logpath..>log4j.appender.logFile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logFile.layout.ConversionPattern=[%d] %p %m (%c)%n Copy After configuring log4j properties, emitted log would look like [2020-07-09 11:15:23,376] INFO Accepted socket connection from /10.233.115.193:34962 (org.apache.zookeeper.server.NIOServerCnxnFactory) Copy "},{"title":"Configuration","type":1,"pageTitle":"Monitoring Apache ZooKeeper running on Instances","url":"docs/Integrations/zookeeper#configuration","content":"sfAgent section provides steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: <Profile_key>tags: Name: <instance_name> appName: <app_name> projectName: <project_name>metrics: plugins: - name: zookeeperjmx enabled: true interval: 60 config: dataDir: /tmp/zookeeper/ documentsTypes: - jmxStats - zookeeperStats port: 2181logging: plugins: - name: zookeeper-general enabled: true config: log_level: - error - info - debug - notice log_path: /home/kafka/kafka_2.12-2.6.2/bin/../logs/*.log Copy Viewing data and dashboards Data collected by plugins can be viewed in SnappyFlow’s browse data section Metrics plugin: zookeepr documentType: jmxStats, zookeeperStats Dashboard template: Zookeeper Logs Plugin: zookeeper documentType: zookeeper "},{"title":"Test Matrix","type":1,"pageTitle":"Monitoring Apache ZooKeeper running on Instances","url":"docs/Integrations/zookeeper#test-matrix","content":"OS\tJDK versionubuntu 18.04 JDK 11 openjdk version \"11.0.11\" 2021-04-20 OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04) OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing) ubuntu 18.04 JDK 8 openjdk version \"1.8.0_292\" OpenJDK Runtime Environment (build 1.8.0_292-8u292-b10-0ubuntu1~18.04-b10) OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode) Centos 7 JDK 11 openjdk version \"11.0.12\" 2021-07-20 LTS OpenJDK Runtime Environment 18.9 (build 11.0.12+7-LTS) OpenJDK 64-Bit Server VM 18.9 (build 11.0.12+7-LTS, mixed mode, sharing) Centos 7 JDK 8 openjdk version \"1.8.0_302\" OpenJDK Runtime Environment (build 1.8.0_302-b08) OpenJDK 64-Bit Server VM (build 25.302-b08, mixed mode) "},{"title":"See Also","type":1,"pageTitle":"Monitoring Apache ZooKeeper running on Instances","url":"docs/Integrations/zookeeper#see-also","content":"Kafka Elasticsearch Kafka-REST Kafka-Connect ActiveMQ "},{"title":"Postgres on Instances","type":0,"sectionRef":"#","url":"docs/Integrations/postgres/postgres_instances","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Postgres on Instances","url":"docs/Integrations/postgres/postgres_instances#overview","content":"PostgreSQL on instances is monitored using sfAgent configured with postgres plugin "},{"title":"Metrics plugin","type":1,"pageTitle":"Postgres on Instances","url":"docs/Integrations/postgres/postgres_instances#metrics-plugin","content":"Collects metric data organized in following documentTypes in metrics index: serverDetails databaseDetails tableDetails IndexDetails queryDetails  "},{"title":"Logger plugin","type":1,"pageTitle":"Postgres on Instances","url":"docs/Integrations/postgres/postgres_instances#logger-plugin","content":"Collects general logs and slow query logs. General logs are sent to log index under documentType: postgres-general and slow queries logs are parsed and data is sent metrics index in documentType: postgres-slowquery "},{"title":"Pre-requisites","type":1,"pageTitle":"Postgres on Instances","url":"docs/Integrations/postgres/postgres_instances#pre-requisites","content":""},{"title":"Enable PostgreSQL general logs","type":1,"pageTitle":"Postgres on Instances","url":"docs/Integrations/postgres/postgres_instances#enable-postgresql-general-logs","content":"Logging needs to be configured in the postgresql.conf file. This file can be located by executing the command shown below: postgres=# show config_file; config_file ---------------------------------- /data/pgsql/data/postgresql.conf (1 row) Copy In postgresql.conf file, uncomment and configure the variables shown below:  log_min_messages = warning # set level as appropriate log_line_prefix = '< %m > ' Copy "},{"title":"Enable Slow Query Logs","type":1,"pageTitle":"Postgres on Instances","url":"docs/Integrations/postgres/postgres_instances#enable-slow-query-logs","content":"Configuring log_min_duration_statement = 200 will log any query which takes more than 200ms to execute which. Set the value to appropriate value "},{"title":"Set access permissions","type":1,"pageTitle":"Postgres on Instances","url":"docs/Integrations/postgres/postgres_instances#set-access-permissions","content":"Username used for DB access should have appropriate permissions grant SELECT ON pg_stat_database to <username>; grant pg_monitor to <username>; Copy note root user has these permissions by default "},{"title":"Configuration","type":1,"pageTitle":"Postgres on Instances","url":"docs/Integrations/postgres/postgres_instances#configuration","content":"Refer to sfAgent section for steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: <profile_key> tags: Name: <name> appName: <app_name> projectName: <project_name> metrics: plugins: - name: postgres enabled: true interval: 60 config: documentsTypes: - databaseDetails - indexDetails - queryDetails - serverDetails - tableDetails host: 127.0.0.1 password: <password> port: 5432 user: <username> logging: plugins: - name: postgres-general enabled: true config: log_level: - error - warning - info - log log_path: /var/log/postgresql/postgresql-10-main.log - name: postgres-slowquery enabled: true config: log_path: /var/log/postgresql/postgresql-10-main.log Copy "},{"title":"Viewing data and dashboards","type":1,"pageTitle":"Postgres on Instances","url":"docs/Integrations/postgres/postgres_instances#viewing-data-and-dashboards","content":"Data generated by plugin can be viewed in browse data page inside the respective application under plugin=postgres and documentType= serverDetails, databaseDetails, tableDetails, IndexDetails, queryDetails, postgres-slowquery Dashboard for this data can be instantiated by Importing dashboard template PostgreSQL to the application dashboard "},{"title":"MySQL in Kubernetes","type":0,"sectionRef":"#","url":"docs/Integrations/mysql/mysql_kubernetes","content":"","keywords":""},{"title":"MySQL monitoring with sfKubeAgent","type":1,"pageTitle":"MySQL in Kubernetes","url":"docs/Integrations/mysql/mysql_kubernetes#mysql-monitoring-with-sfkubeagent","content":"In this approach, sfKubeAgent is run as a side-car inside MySQL pod. The example below shows the config-map for sfKubeAgent container, config-map for MySQL container and pod yaml. "},{"title":"Config map for MySQL","type":1,"pageTitle":"MySQL in Kubernetes","url":"docs/Integrations/mysql/mysql_kubernetes#config-map-for-mysql","content":"apiVersion: v1 kind: ConfigMap metadata: name: mysql-configmap data: mysql.cnf: | [mysqld] show_compatibility_56 = On query_cache_type = 1 query_cache_size = 16M query_cache_limit = 1M general_log_file = /var/log/mysql/mysql.log general_log = 1 slow_query_log = 1 slow_query_log_file = /var/log/mysql/mysql-slow.log Copy "},{"title":"Config map for MySQL sfKubeAgent","type":1,"pageTitle":"MySQL in Kubernetes","url":"docs/Integrations/mysql/mysql_kubernetes#config-map-formysqlsfkubeagent","content":"apiVersion: v1kind: ConfigMapmetadata: name: mysql-sfkubeagent-configmapdata: config.yaml: |- key: <enter profile key> metrics: plugins: - name: mysql enabled: true interval: 30 config: host: \"127.0.0.1\" password: \"<enter password>\" user: \"root\" documentsTypes: - databaseDetails - serverDetails - tableDetails logging: plugins: - name: mysql-general enabled: true config: log_path: \"/var/log/mysql/mysql.log, /var/log/mysql.log, /var/log/mysqld.log\" - name: mysql-error enabled: true config: log_level: - error - warn - note log_path: \"/var/log/mysql/error.log, /var/log/mysql/mysql-error.log, /var/log/mysqld.err\" - name: mysql-slowquery enabled: true config: log_path: \"/var/lib/mysql/ip-*slow.log, /var/log/mysql/mysql-slow.log\" Copy "},{"title":"MySQL Pod YAML","type":1,"pageTitle":"MySQL in Kubernetes","url":"docs/Integrations/mysql/mysql_kubernetes#mysql-pod-yaml","content":"kind: PodapiVersion: v1metadata: name: mysql-pod labels: snappyflow/appname: <app_name> snappyflow/projectname: <project_name>spec: containers: - name: mysql-container image: \"mysql:5.7.14\" imagePullPolicy: IfNotPresent ports: - name: tcp containerPort: 3306 env: - name: MYSQL_ROOT_PASSWORD value: <enter password> - name: MYSQL_ROOT_USER value: root volumeMounts: - name: varlog mountPath: /var/log/mysql - name: configmap-mysql mountPath: /etc/mysql/conf.d # Snappyflow's sfkubeagent container - name: sfagent-container image: snappyflowml/sfagent:latest imagePullPolicy: Always command: - /app/sfagent - -enable-console-log env: - name: APP_NAME value: <app_name> - name: PROJECT_NAME value: <project_name> volumeMounts: - name: configmap-sfkubeagent-mysql mountPath: /opt/sfagent/config.yaml subPath: config.yaml - name: varlog mountPath: /var/log/mysql volumes: - name: configmap-mysql configMap: name: mysql-configmap - name: configmap-sfkubeagent-mysql configMap: name: mysql-sfkubeagent-configmap - name: varlog emptyDir: {} Copy "},{"title":"Viewing data and dashboards","type":1,"pageTitle":"MySQL in Kubernetes","url":"docs/Integrations/mysql/mysql_kubernetes#viewing-data-and-dashboards","content":"Metric data generated by plugin can be viewed in browse data page inside the respective application under metrics section with plugin=mysql and documentType= serverDetails, databaseDetails, tableDetails. Data from slow query logs can be found in metrics section with plugin=mysql-slowquery and documentType=SlowQueryLogsDashboard for this data can be instantiated by Importing dashboard template MySQL to the application dashboard "},{"title":"MySQL monitoring with Prometheus","type":1,"pageTitle":"MySQL in Kubernetes","url":"docs/Integrations/mysql/mysql_kubernetes#mysql-monitoring-with-prometheus","content":"Refer to Prometheus Exporter overview to understand how SnappyFlow monitors using Prometheus exporters. "},{"title":"Pre-requisites","type":1,"pageTitle":"MySQL in Kubernetes","url":"docs/Integrations/mysql/mysql_kubernetes#pre-requisites","content":"Prometheus exporter is deployed as a side-car in the application container and the exporter port is accessible to sfPod "},{"title":"Configurations","type":1,"pageTitle":"MySQL in Kubernetes","url":"docs/Integrations/mysql/mysql_kubernetes#configurations","content":""},{"title":"MySQL Service YAML","type":1,"pageTitle":"MySQL in Kubernetes","url":"docs/Integrations/mysql/mysql_kubernetes#mysql-service-yaml","content":"apiVersion: v1 kind: Service metadata: labels: snappyflow/appname: <app_name> snappyflow/projectname: <project_name> snappyflow/component: mysql name: mysql-prom-service spec: ports: - name: mysql port: 3306 protocol: TCP targetPort: mysql - name: mysql-exporter port: 9104 protocol: TCP targetPort: mysql-exporter selector: app: <app_name> sessionAffinity: None type: ClusterIP Copy "},{"title":"MySQL Pod YAML","type":1,"pageTitle":"MySQL in Kubernetes","url":"docs/Integrations/mysql/mysql_kubernetes#mysql-pod-yaml-1","content":"kind: Pod apiVersion: v1 metadata: name: mysql-prometheus-pod labels: snappyflow/appname: <app-name> snappyflow/projectname: <project-name> snappyflow/component: mysql spec: containers: - name: mysql-container image: \"mysql:5.7.14\" imagePullPolicy: IfNotPresent ports: - name: tcp containerPort: 3306 env: - name: MYSQL_ROOT_PASSWORD value: <password> - name: MYSQL_ROOT_USER value: root volumeMounts: - name: varlog mountPath: /var/log/mysql - name: configmap-mysql mountPath: /etc/mysql/conf.d # Prometheus exporter - name: mysql-exporter image: prom/mysqld-exporter:v0.10.0 imagePullPolicy: Always ports: - name: mysql-exporter containerPort: 9104 command: - sh - -c - DATA_SOURCE_NAME=\"root:$MYSQL_ROOT_PASSWORD@(localhost:3306)/\" /bin/mysqld_exporter env: - name: MYSQL_ROOT_PASSWORD value: <password> volumes: - name: configmap-mysql configMap: name: mysql-configmap - name: varlog emptyDir: {} Copy "},{"title":"Viewing data and dashboards","type":1,"pageTitle":"MySQL in Kubernetes","url":"docs/Integrations/mysql/mysql_kubernetes#viewing-data-and-dashboards-1","content":"Data generated by plugin can be viewed in browse data page inside the respective application under plugin=kube-prom-mysql and documentType= serverDetails, tabledetails . Data from slow query logs can be found in metrics section with plugin=mysql-slowquery and documentType=mysqlSlowQueryLogsDashboard for this data can be instantiated by Importing dashboard template MySQL_Kube_Prom to the application dashboard "},{"title":"Analyzing ETL Jobs with SnappyFlow","type":0,"sectionRef":"#","url":"docs/Log_management/etl_jobs","content":"","keywords":""},{"title":"Step 1: Drop logs from ETL Jobs","type":1,"pageTitle":"Analyzing ETL Jobs with SnappyFlow","url":"docs/Log_management/etl_jobs#step-1-drop-logs-from-etl-jobs","content":"SnappyFlow allows for a job to have up to a 3-level hierarchy- Job, Stage, Task. Logs in JSON format have to be dropped whenever a job/stage/task is started, completed or terminated. This log can be parsed using SnappyFlow’s ETL parser. Log format for a Job: { \"jobName\": <Job-name>, \"jobId\": <Unique JobId>, \"time\": <Time in epoch milliseconds format> \"type\": \"job\", \"status\": <status: started, success, failed, aborted> } Copy Log format for a Stage:  { \"jobName\": <Job-name>, \"jobId\": <Unique JobId>, \"stageId\": <stageId>, \"stageName\": <stageName> \"time\": <Time in epoch milliseconds format> \"type\": \"stage\", \"status\": <status can be started, success, failed, aborted> } Copy Log format for a Task: { \"jobName\": <Job-name>, \"jobId\": <Unique JobId>, “stageId”: <staged>, “stageName”: <stageName> \"time\": <Time in epoch milliseconds format> \"type\": \"task\", \"status\": <status can be started, success, failed, aborted>} Copy "},{"title":"Step 2: Forward logs to SnappyFlow","type":1,"pageTitle":"Analyzing ETL Jobs with SnappyFlow","url":"docs/Log_management/etl_jobs#step-2-forward-logs-to-snappyflow","content":"Add the following log parser to logging section of sfAgent’s config.yaml: logging: plugins: - name: etlRaw enabled: true config: log_path: <log file path> Copy Restart sfAgent with the new configuration. service sfagent restart Copy Check if documents have been received in SnappyFlow. You will find 3 documents under metrics with plugin name as “etlRaw” and documentType as “job”, “stage” and “task” depending on your hierarchy. "},{"title":"Step 3: Generate an access URL for use by summarization module","type":1,"pageTitle":"Analyzing ETL Jobs with SnappyFlow","url":"docs/Log_management/etl_jobs#step-3-generate-an-access-url-for-use-by-summarization-module","content":"Logs shipped to SnappyFlow are in a raw form and they cannot be directly used for reporting and analysis. Therefore user has to export this raw data to a summarization script that transforms the data and sends it back to SnappyFlow into a new document. Import a ETL template into your dashboard. Go to “Scratchpad” pane Click on ‘Export API Endpoint’ option in the component and create component URL for all 3 components for interval, say Last 5 mins.  Click on the ‘API Endpoints’ option for the project to view the API List. Copy the URL’s for the 3 components and the Authentication token. These need to be provided in Step 4  "},{"title":"Step 4: Run summarization script as a cronjob","type":1,"pageTitle":"Analyzing ETL Jobs with SnappyFlow","url":"docs/Log_management/etl_jobs#step-4-run-summarization-script-as-a-cronjob","content":"Install the pip utility from the below link. Refer to the link for Installation and Usage instructions. ​ sfapmetl · PyPI The python script takes a config file path as input Set values for key, appName, projectName, Name. Provide the component Url’s for Job, stage and Task and authKey (from Step 3) The data will be available in the dashboard under the plugin ‘etlReport’ and documentType - job, stage and task. "},{"title":"Step 5: Review ETL Dashboards","type":1,"pageTitle":"Analyzing ETL Jobs with SnappyFlow","url":"docs/Log_management/etl_jobs#step-5-review-etl-dashboards","content":"You will now see the summarized data in dashboard under etlReport for job, stage and tasks. Select a particular job and choose a timeline to see job duration trends over the selected time period. Clicking on a particular job id provides a drilled down view of stages and tasks within that job.  "},{"title":"Log Signatures","type":0,"sectionRef":"#","url":"docs/Log_management/log_signatures","content":"","keywords":""},{"title":"Coming Soon!","type":1,"pageTitle":"Log Signatures","url":"docs/Log_management/log_signatures#coming-soon","content":""},{"title":"sfPoller Setup","type":0,"sectionRef":"#","url":"docs/New_Pages/sfpoller_setup","content":"sfPoller Setup Coming Soon!","keywords":""},{"title":"C#","type":0,"sectionRef":"#","url":"docs/Tracing/csharp","content":"","keywords":""},{"title":"Coming Soon","type":1,"pageTitle":"C#","url":"docs/Tracing/csharp#coming-soon","content":""},{"title":"Go","type":0,"sectionRef":"#","url":"docs/Tracing/go","content":"","keywords":""},{"title":"Coming Soon!","type":1,"pageTitle":"Go","url":"docs/Tracing/go#coming-soon","content":""},{"title":"Log Onboarding","type":0,"sectionRef":"#","url":"docs/Log_management/log_overview","content":"","keywords":""},{"title":"Document format in SnappyFlow","type":1,"pageTitle":"Log Onboarding","url":"docs/Log_management/log_overview#document-format-in-snappyflow","content":"SnappyFlow stores all information in JSON format to the datastore. A sample log document is shown below: \"node\": \"ip-172-31-14-187\",\"_plugin\": \"linux-syslog\",\"ident\": \"sshd\",\"_tag_Name\": \"demo-presto-worker-0\",\"level\": \"info\",\"@timestamp\": \"2020-10-15T18:35:13.000000000Z\",\"time\": \"1602786913000\",\"pid\": \"14153\",\"_documentType\": \"syslog\",\"host\": \"ip-172-31-14-187\",\"_tag_uuid\": \"0aa46894f321\",\"_tag_projectName\": \"presto\",\"file\": \"/var/log/auth.log\",\"signatureKey\": \"8276318930445510094\",\"_tag_appName\": \"presto\",\"message\": \"Invalid user ofandino from 152.32.180.15 port 56712\" Copy "},{"title":"Types of search","type":1,"pageTitle":"Log Onboarding","url":"docs/Log_management/log_overview#types-of-search","content":"Searching across all fields and valuesRange queries for numeric dataWildcard searchRegular expression searchLogical operations like AND, OR, NOT to build complex searches SnappyFlow datastore, processes fields which contain string values differently. The string is stored as a list of tokens. Each token is a unique word in the string. For example if the field “message” has a value “user:admin CMD=rm –rf temp PWD=/home/admin PATH=var.log.secure”. Copy This string is converted as a list of tokens as follows: “user:admin CMD rm rf temp PWD home admin PATH var.log.secure”. Copy Note that in the above tokenization, character “:” and character “.” Are treated differently. They are considered as alpha-numeric, for the purpose of tokenization and are retained, if they are preceded and succeeded by alpha-numeric characters between the “:”. A search of string user\\:admin will be successful in the above document. Note that “:” was escaped using “\\” in the search string, as “:” is considered a reserved character. See below for more on reserved characters. Also note, that a search string temp home will also match the above string, as the words temp and home are present in the above string, even though they do not appear in consecutive positions. A phrase search “temp home” (the search string is encapsulated between “), will match only if temp and home appear together in the string and are in the same order. "},{"title":"SnappyFlow Query language operator support","type":1,"pageTitle":"Log Onboarding","url":"docs/Log_management/log_overview#snappyflow-query-language-operator-support","content":"Operator\tDescription\tExample\tExplanation:\tSearch for a value within a field\tlevel:info\tGet all documents where field “level” has value “info” &&\tAND operation\tinfo && ident:sshd\tGet all documents where value “info” is present in any of the fields AND “ident” field has value “sshd” ||\tOR operation\tlevel:warn || level:error\tGet all documents where “level” field has value “warn” or “level” field has value “error” \"\"\tPhrase searches\tmessage: \"Invalid\tGet all documents where “message” field has a phrase “Invalid user”. Note: searches are case insensitive. “Invalid user” will match only if token “Invalid” and token “user” are present in the string in the same order. >\tGreater than\tpid:>14153\tGet all documents where field “pid” has values greater than 14153 <\tLesser than\tpid:<14153\tGet all documents where field “pid” has values less than 14153 >=\tGreater than or equal\tpid:>=14153\tGet all documents where field “pid” has values greater than or equal to 14153 <=\tLesser than or equal\tpid:<14153\tGet all documents where field “pid” has values less than or equal to 14153 ()\tGrouping\t(pid:(>14000 && <=15000) || level:error) && ident:sshd\tGet all documents where field “pid” is in the range 14000 – 14999 OR field “level” has value “error”. From the above search get only those documents where field “ident” has value “sshd” -\tNOT operation\tlevel:-(info || warn)\tGet all documents where field “level” does not contain value “info” or “warn” ?\tSingle character wildcard\t_plugin: sys???\tGet all documents where the field plugin has a word sys followed by 3 characters. *\tZero or more characters wildcard\tmessage: var\tGet all documents where “message” field contains a string var preceded by any characters and succeeded by any characters. For example in the message “user:admin CMD=rm –rf temp PWD=/home/admin PATH=var.log.secure”, var matches var.log.secure //\tPattern searches\tmessage: /[0-9]+.[0-9]+.[0-9]+.[0-9]+/\tGet all documents which contain an IP address pattern. In the sample log document with the message field containing \"Invalid user ofandino from 152.32.180.15 port 56712\" , the regex pattern will match 152.32.180.15. \\ Escape sequence\tmessage: sudo\\:linux\tSome of the special characters need to be escaped if they are part of a search string. Special characters to be escaped are: & | \" = : ( ) [ ] - ? * / \\ exists:\tField name search\texists:pid\tGet all logs Note: Field names are case sensitive i.e. latency: 20 and LATENCY: 20 will give different results. Field values are case insensitive i.e. name: KEVIN and name: kevin will give the same results. Applying range queries i.e. key: >=200 etc. to text fields give unpredictable results. Make sure to apply such queries on numeric fields only. Range queries cannot be used without specifying the field name i.e. >=20 is not a valid query. Wildcards cannot be used in phrase searches i.e. \"*error\" or \"er??r\" is not allowed. Using a wildcard at the beginning of a word e.g. *ing is particularly heavy, because all terms in the index need to be examined, just in case they match. Regex patterns must be enclosed in forward slashes. Any string present between a pair of forward slashes will be treated as a Java regex pattern. Search Regex does not support all regex meta-characters. For details, https://www.elastic.co/guide/en/elasticsearch/reference/current/regexp-syntax.htmlPatterns are anchored by default i.e. they must match an entire Elasticsearch token. "},{"title":"Examples","type":1,"pageTitle":"Log Onboarding","url":"docs/Log_management/log_overview#examples","content":""},{"title":"Basic Search","type":1,"pageTitle":"Log Onboarding","url":"docs/Log_management/log_overview#basic-search","content":"Datastore has following documents { \"pid\": 3245, \"upstream_response_time\": 10, \"URL\": \"https://www.elastic.co/guide/en/elasticsearch/reference\"}{\"pid\": 2445, \"upstream_response_time\": 4, \"URL\": \"https://www.elastic.co/guide/en/machine-learning\" }{\"pid\": 3246, \"upstream_response_time\": 2, \"URL\": \"https://docker-hub/pricing\"}{\"message\": \"docker image built\", \"pid\": 1000} Copy Search Query & Logic\tResult\tResults explainedpid: 3?4?\tMatches documents 1, 3.\tGet all documents with pid field value matching the pattern 3?4? (? matches any character) upstream_response_time:>5 && elasticsearch\tMatches document 1\tGet all documents where field upstream_response_time key has a value greater than 5 AND the string elasticsearch is present in any of the fields. elastic && machine-learning\tNo documents are matched.\tGet all documents where strings elastic AND machine-learning are present in any of the fields. Though string elastic is present in documents 1, 2; it does not appear as a standalone term. This is because, special character “.” is handled differently in tokenization and is tokenized as www.elastic.co. If the search query is modified as www.elastic.co && machine-learning, document 2 will match the search. Alternatively, search elasstic && machine-learning, will also return the same result https docker hub pricing\tMatches documents 3 and 4.\tGet all documents which contain the words https OR docker OR hub or pricing in any order. Matches documents 3 and 4. Document 3 has all the terms and Document 4 has the term docker. If the intent is to search for a document with all the terms in the same order, then the search should be modified to “https docker hub pricing”. Note the phrase is enclosed in double quotes. This search will match only document 3. Also note the words http docker hub pricing are connected with special characters in document 3. But the search is on the tokenized version of the document and hence all special characters are removed. "},{"title":"Logical Operations and wild card usage","type":1,"pageTitle":"Log Onboarding","url":"docs/Log_management/log_overview#logical-operations-and-wild-card-usage","content":"Datastore contains following documents {\"message\": \"Disconnected from 118.24.197.243 port 35662 [preauth]\"}{\"message\": \"Unregistered Authentication Agent for unix-session:7 (system bus name :1.89, object path /org/freedesktop/PolicyKit1/AuthenticationAgent, locale en_IN) (disconnected from bus)\"}{\"responseCode\": \"400\", \"responseMessage\": Null}{\"message\": \"request received from IP1 and redirected to IP2\", \"responseCode\": \"200\"}{\"message\": \"ValueError(…)\"}{\"message\": \"ArithmeticException(…)\"} Copy Examples Search Query:\"disconnected from\" Get all documents that contain the terms disconnected and from. The terms should appear together in the same order in the document. Results and explanation:Matches documents 1 and 2. Notice that in document 1, the word disconnected appears as Disconnected. Since search is always case-insensitive, document 1 is also matched. Search Query:message: (disconnected && from && port) Get all documents that contain the words disconnected and from and port Results and explanation:Matches document 1 Note: words need not appear together and they may appear in any order. Search Query:message: (disconnect* port) Get all documents that contain word starting with disconnect or a word port. Results and explanation:Matches documents 1 and 2. This is interpreted asmessage: (disconnect || port) disconnect matches all terms which start with the word disconnect and have zero or more characters after it i.e. disconnecting, disconnected and disconnect Search Query:message: (disconnected && -port) Get all documents that has term disconnected and does not have the term port Results and explanation:Matches document 2 -(responseCode: 400 || message: (exception || error)) 2 This is a complete negation of the above search i.e. NOT operator is applied to above search Search Query:responseCode: 400 || message: (exception || error) Results and explanation:Matches 3, 5 and 6 exception matches any word that contains the string exception and similarly error. The term ArithmeticException(...) matches exception and ValueError(...) matches error Search Query:-(responseCode: 400 || message: (exception || error)) This search is a total negation of the previous search. Results and explanation:Matches document 4 "},{"title":"Regex Patterns","type":1,"pageTitle":"Log Onboarding","url":"docs/Log_management/log_overview#regex-patterns","content":"Datastore contains following documents {\"message\": \"No identification string for 118.24.197.243\"}{\"message\": \"No identification string for 119:25.200.255\"}{\"message\": \"Received bad request from 119:25.200.255\"}{\"message\": \"pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=203.195.182.3\"}{\"message\": \"Authentication failure for user admin\"} Copy Examples Search Query: message: /[0-9]+.[0-9]+.[0-9]+.[0-9]+/ Copy Get all documents where message field contains an IP address pattern.Results and explanation: Matches documents 1,2,3,4 Search Query: (message: /119.25.[0-9]+.[0-9]+/) Copy Get all documents where message field contains an IP Address pattern with a network address 119.25 Results and explanation: Matches document 2 and 3auth* && failure &&-/[0-9]+.[0-9]+.[0-9]+.[0-9]+/ 5Any key’s value needs to consist of auth*, failure but not an IP i.e. [0-9]+.[0-9]+.[0-9]+.[0- 9]+ Copy Search Query: auth* && failure && -/[0-9]+.[0-9]+.[0-9]+.[0-9]+/ Copy Get all documents where an IP address pattern is NOT present in any of the fields and contains a word starting with auth in any of the fields AND contains the word failure in any of the fields. Results and explanation: Matches documents 5 "},{"title":"Getting Started","type":0,"sectionRef":"#","url":"docs/Quick_Start/getting_started","content":"","keywords":""},{"title":"Setup SaaS Account","type":1,"pageTitle":"Getting Started","url":"docs/Quick_Start/getting_started#setup-saas-account","content":"Go to www.snappyflow.io Register for a free trial. A demo account will be created with a pre-configured sample application Request an upgrade to Full Trial by clicking on the link provided in the top bar. You will get an email stating “your trial environment is ready” once SnappyFlow team approves your trial request.  "},{"title":"Setup On-Prem SaaS","type":1,"pageTitle":"Getting Started","url":"docs/Quick_Start/getting_started#setup-on-prem-saas","content":"On-Prem SaaS can be deployed within your public cloud accounts using scripts provided by SnappyFlow team Please reach out to support@snappyflow.io. A support engineer will understand your data ingest rates and provide an appropriately sized solution "},{"title":"Important terminologies and concepts","type":1,"pageTitle":"Getting Started","url":"docs/Quick_Start/getting_started#important-terminologies-and-concepts","content":"sfAgent sfPoller sfPod sfKubeAgent Profile Key Tagging Approach "},{"title":"sfAgent","type":1,"pageTitle":"Getting Started","url":"docs/Quick_Start/getting_started#sfagent","content":"Monitoring of applications running on VM or bare-metal requires installation of a lightweight sfAgent. sfAgent provides following features: Discovery of servicesAuto-recommendation of monitoring configuration based on discovered servicesMonitoring of various services based on specified configurations Log parsing and collectionOrchestration of tracing (check out sfTracing for details) Installation procedures For sfAgent on Linux For sfAgent on Windows "},{"title":"sfPoller","type":1,"pageTitle":"Getting Started","url":"docs/Quick_Start/getting_started#sfpoller","content":"Poller appliance is installed within user’s cloud account and can be used to Monitor cloud services such as RDS, ELB, Lamba, ECS, Azure App Service etc. Monitor Databases Perform Synthetic Monitoring of APIs using postman like collections Stream logs from applications to sfPoller, apply parsing rules and forward logs to SnappyFlow.  Procedure for sfPoller setup "},{"title":"sfPod","type":1,"pageTitle":"Getting Started","url":"docs/Quick_Start/getting_started#sfpod","content":"Daemon set installed on Kubernetes cluster and monitors the following elements: Host, Pod & Container metrics Resources such as deployments, Daemon Sets etc. Kubernetes core services metrics Cluster logs Monitor Prometheus exporters running on any of the application pods  Procedure for sfPod setup "},{"title":"sfKubeAgent","type":1,"pageTitle":"Getting Started","url":"docs/Quick_Start/getting_started#sfkubeagent","content":"sfAgent equivalent and installed as a side-car container within a Kubernetes pod and can be configured to monitor metrics and logs of other containers running on pods. Procedure for setting up sfKubeAgent "},{"title":"Profile Key","type":1,"pageTitle":"Getting Started","url":"docs/Quick_Start/getting_started#profile-key","content":"Every user account has a unique system generated profile key. Data sent by collectors to SnappyFlow need to have the correct profile key and tags to be allowed into SnappyFlow. This key has to be copied by the user and pasted into the configuration file of sfAgent or within sfPoller’s UI "},{"title":"Tagging Approach","type":1,"pageTitle":"Getting Started","url":"docs/Quick_Start/getting_started#tagging-approach","content":"SnappyFlow mandates that all end-points should be assigned two tags - _tag_projectName and _tag_appName. These tags have to be added to configuration files of sfAgent or within sfPoller’s UI. Pls see the video that explains how end-points should be organized hierarchically in SnappyFlow and how tags should be assigned  "},{"title":"Let's Start Monitoring","type":1,"pageTitle":"Getting Started","url":"docs/Quick_Start/getting_started#lets-start-monitoring","content":"Try out one of the simple exercises to familiarize yourself with the product Monitor a Linux instance# Monitor a Kubernetes Cluster# Monitor a Windows instance# Trace an application# "},{"title":"Tracing Java Applications","type":0,"sectionRef":"#","url":"docs/Tracing/java_v2","content":"","keywords":""},{"title":"Available Platforms","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java_v2#available-platforms","content":"InstanceKubernetesDockerECSAWS Lambda Tracing JAVA applications on instances is a 2 step process involving installation of SnappyFlow agent on the instance and configuring the app to allow metrics to be collected. Install agentConfigure application for tracing Configuring the app allows the collection of metrics by the SnappyFlow trace agent. There are multiple ways to configure JAVA apps depending on the type of JAVA application. Choose the type below or follow the command line instructions for generic JAVA applications. Command Line# Use the following arguments while starting your application using java –jar command, in IDE, Maven or Gradle script: java -javaagent:/opt/sfagent/sftrace/java/sftrace-java-agent.jar -Dsftrace.service_name=<my-service> -jar <application jar> Copy Note: If service_name is not provided, an auto discovered service name will be added. Service_name is used to identify and filter the traces related to an application and should be named appropriately to distinctly identify it. Service name must only contain characters from the ASCII alphabet, numbers, dashes, underscores and spaces. Additional features available for Spring Boot Applications# By default, transaction names of unsupported Servlet API-based frameworks are in the form of $method unknown route. To modify this and to report the transactions names in the form of $method $path, use the following in javaagent configuration. This option is applicable only for spring-boot based applications. -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true Copy Normalizing Transaction URLs# If your URLs contain path parameters like /user/$userId, it can lead to an explosion of transaction types. This can be avoided by using URL groups. For example, if the application supports urls like: /owners, /owners/<owner_id>, /owners/<owner_id>/edit, /owners/<owner_id>/pets, Copy then url groups would be configured as: url_groups=/owners/*,/owner/*/edit,/owners/*/pets Copy Apache Tomcat# Add the agent configuration in setenv.sh. If this file is not present, create the file in below folder <tomcat installation path>/bin Copy Refer to tomcat_setenv.sh for tracing specific configuration that needs to be copied to setenv.sh file. Make the file executable using chmod +x bin/setenv.sh and start the server Additional features available for Spring Boot Applications# By default, transaction names of unsupported Servlet API-based frameworks are in the form of $method unknown route. To modify this and to report the transactions names in the form of $method $path, use the following in javaagent configuration. This option is applicable only for spring-boot based applications. -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true Copy Normalizing Transaction URLs# If your URLs contain path parameters like /user/$userId, it can lead to an explosion of transaction types. This can be avoided by using URL groups. For example, if the application supports urls like: /owners, /owners/<owner_id>, /owners/<owner_id>/edit, /owners/<owner_id>/pets, Copy then url groups would be configured as: url_groups=/owners/*,/owner/*/edit,/owners/*/pets Copy Add the agent configuration in setenv.sh. If this file is not present, create the file in below folder <tomcat installation path>/bin Copy Refer to tomcat_setenv.sh for tracing specific configuration that needs to be copied to setenv.sh file. Make the file executable using chmod +x bin/setenv.sh and start the server JBOSS/EAP# Standalone Mode# Add the agent configuration in standalone.conf file and start the server Refer to JBOSS_standalone.conf for tracing specific configuration. Copy from section with “SFTRACE-CONFIG” in comments Domain Mode# Add the agent configuration in domain.xml and start the server Refer to JBOSS_domain.xml for tracing specific configuration. Copy from section with “SFTRACE-CONFIG” in comments After updating the configuration, restart the application. Additional features available for Spring Boot Applications# By default, transaction names of unsupported Servlet API-based frameworks are in the form of $method unknown route. To modify this and to report the transactions names in the form of $method $path, use the following in javaagent configuration. This option is applicable only for spring-boot based applications. -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true Copy Normalizing Transaction URLs# If your URLs contain path parameters like /user/$userId, it can lead to an explosion of transaction types. This can be avoided by using URL groups. For example, if the application supports urls like: /owners, /owners/<owner_id>, /owners/<owner_id>/edit, /owners/<owner_id>/pets, Copy then url groups would be configured as: url_groups=/owners/*,/owner/*/edit,/owners/*/pets Copy Example of running java application via command line using these parameters# java -javaagent:/opt/sfagent/sftrace/java/sftrace-java-agent.jar -Dsftrace.service_name=my-service -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true -Delastic.apm.url_groups=/owners/*,/owner/*/edit,/owners/*/pets -jar <application jar> Copy "},{"title":"Custom Monitoring using StatsD","type":0,"sectionRef":"#","url":"docs/Integrations/statsd/custom_monitoring","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#overview","content":"StatsD is a popular standard for developing infrastructure and application plugins. A wide suite of standard plugins are available from Statsd community and can be accessed here sfAgent Statsd plugin integrates to Statsd client in the following way: Runs a daemon to listen to UDP port for data being sent by statsd client and accumulates all metrics being sent in the last N seconds (called flushinterval) Translates the data from statsd format to SnappyFlow’s format Forwards the data to SnappyFlow with necessary tags  "},{"title":"Prerequisites","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#prerequisites","content":"Create a rules file for a statsd client or contact support@snappyflow.io to create the rules file for a specific statsd client.  "},{"title":"Configuration","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#configuration","content":"User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: <profile_key> tags: Name: <name> appName: <app_name> projectName: <project_name> metrics: plugins: - name: statsd enabled: true config: port:8125 flushinterval:30 ruleFile: /path/to/rules/file Copy port: The UDP port on which statsd client sends metrics. sfAgent runs a statsd server listening on this port for the UDP datagrams. Default value is 8125. flushInterval: SnappyFlow’s statsd plugin collects all the metrics received in the last N seconds and sends the data to SnappyFlow as a single document ruleFile: User generated statsd rules file path or please contact support@snappyflow.io to create a rule file for a specific statsd client. "},{"title":"Operating Instructions","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#operating-instructions","content":"Validate the statsd configuration and the rules. It is mandatory to run this command after any change is made in the statsd rules file, followed by restarting the sfAgent service. sudo /opt/sfagent/sfagent -check-config Copy "},{"title":"Creating Rules File","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#creating-rules-file","content":"statsd metrics are expected in the format shown below namespace.prefix.[type.]metric:value|metricType Copy Example ClusterA.Kafka1.Topic1.Lag:500|g Copy In this case, namespace= ClusterA,prefix= Kafka1,type= Topic1,metric= Lag,value= 500,metricType= g(gauge) Copy The field type is optional. If this field is present, it will enforce a nested json else the resulting json will be flat Example Kafka1.General.numTopic:5|g Copy In this case, namespace= Kafka1,prefix= General,metric= numTopic,value= 5,metricType= g (gauge) Copy namespace= Kafka1,prefix= General,metric= numTopic,value= 5,metricType= g (gauge) note In special cases where namespace is not present and the metrics start directly with prefix, set namespace: none. Supported datatypes are float, double, long, integer. "},{"title":"Rule to create nested json: \"NESTED\"","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#rule-to-create-nested-json-nested","content":"Syntax <json_key> = NESTED(namespace: <namespace>, prefix: <prefix_name>, key: <type_key>, metric: [<list of metrics along with datatypes>]) Copy <json_key>: key of the final nested json. <namespace>: This rule is applied to all metrics having this namespace <prefix>: This rule is applied to all metrics having this prefix. <key>: adds a key:value pair in the nested json <metric>: Specify all the metrics to collect for this prefix. Example DB.host1.disk1.readLatency:20|g DB.host1.disk1.writeLatency:50|g Copy Rule latency = NESTED(namespace: DB, prefix: host1, key: diskName, metric:[readLatency:float, writeLatency:float]) Copy Output \"latency\": [ { \"diskName\": disk1, \"readLatency\":20, \"writeLatency\": 50 }, { \"diskName\": disk2, \"readLatency\":25, \"writeLatency\": 45 } ] Copy "},{"title":"Rule to create flat json: \"FLAT\"","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#rule-to-create-flat-json-flat","content":"Syntax <json_key> = FLAT(namespace: <namespace>, prefix: <prefix_name>, metric: <metric_name>) Copy <namespace>: This rule is applied to all metrics having this namespace <prefix>: This rule is applied to all metrics having this prefix. <metric>: Specify all the metrics to collect for this prefix. Example Kafka1.System.cpuutil:10|g,Kafka1.System.ramutil:20|g, Copy Rule computeMetrics = FLAT(namespace: Kafka1, prefix: System, metric: [cpuutil:float, ramutil:float]) Copy Output \"cpuutil\": 10, “ramutil”:20 Copy "},{"title":"\"RENDER\" Rule:","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#render-rule","content":"Extraction rules mentioned above, extract a set of metrics from statsd datagrams. These extracted metrics are grouped together in documents and shipped to SnappyFlow. Render rules describe grouping of metrics into documentType Syntax RENDER(_documentType: <doctype>, m1, m2,…mn) where m1..mn can be metric names or Rule names Copy Example RENDER(documentType: system, computeMetrics, latency) will create a documentType { plugin: statsd documentType: system \"cpuutil\": 10, “ramutil”: 20 \"latency\": [ { \"diskName\": disk1, \"readLatency\":20, \"writeLatency\": 50 }, { “diskName”: disk2, “readLatency”:25, “writeLatency”: 45 } ] } Copy "},{"title":"Tagging","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#tagging","content":"sfAgent statsD plugin is capable of parsing and forwarding the tags contained in the statsd metric datagrams. Tags are expressed in different formats based on the intended destination being Datadog, Influx or Graphite. Add TAGTYPE rule in the statsd rules file to enable the parsing. Default TAGTYPE is None i.e. no custom tags present. In each of the formats below, the tags are recognized and passed forward into SnappyFlow documents TAGTYPE = Datadog Sample metric: Cluster1.kafka1.cpuUtil:35|c|#_tag_appName:testApp1,_tag_projectName:apmProject,_documentType:cpuStats Copy TAGTYPE = Influx Sample metric: Cluster1.Kafka1.cpuUtil,_tag_appName=testApp1,_tag_projectName=apmProject,_documentType=cpuStats:35|c Copy TAGTYPE = Graphite Sample metric: Cluster1.Kafka1.cpuUtil;_tag_appName=testApp1;_tag_projectName=apmProject;_documentType=cpuStats:35|c Copy "},{"title":"Sidekiq Use-case","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#sidekiq-use-case","content":"This section shows to monitor sidekiq using statsd with sfAgent. "},{"title":"Description","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#description","content":"We will use a simple ruby on rails application which shows endangered sharks’ data. There are two sidekiq worker configured, one to add the data and another to remove the sharks data named as AddEndangeredWorker and RemoveEndangeredWorker respectively. Sidekiq statsd client is also configured to get the metrics. For this example, sidekiq-statsd by phstc is used as the client.  "},{"title":"Installation","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#installation","content":"Skip this part if the statsd client is already configured. Follow this documentation to setup the ruby on rails application, if neededTo add the statsd client: Create a new file sidekiq.rb under config/initializers/ and add the configuration specified here. Install the [sidekiq-statsd gem](https://github.com/phstc/sidekiq-statsd\" /l \"installation) and run the application.  "},{"title":"Sample Metrics","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#sample-metrics","content":"Metrics are generated upon worker activation in the application. Add endangered worker metrics production.worker.AddEndangeredWorker.processing_time:1113|msproduction.worker.AddEndangeredWorker.success:1|cproduction.worker.enqueued:0|gproduction.worker.retry_set_size:0|gproduction.worker.processed:69|gproduction.worker.failed:0|gproduction.worker.queues.default.enqueued:0|gproduction.worker.queues.default.latency:0|g Copy Remove endangered worker metrics production.worker.RemoveEndangeredWorker.processing_time:1472|msproduction.worker.RemoveEndangeredWorker.success:1|cproduction.worker.enqueued:0|gproduction.worker.retry_set_size:0|gproduction.worker.processed:107|gproduction.worker.failed:0|gproduction.worker.queues.default.enqueued:0|gproduction.worker.queues.default.latency:0|g Copy "},{"title":"Rules","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#rules","content":"Follow the Rules User Guide section to understand the rules. TAGTYPE = None worker = NESTED(namespace: production, prefix: worker, key: worker_name, metric:[processing_time:double, success:float]) queues = NESTED(namespace: production, prefix: worker.queues, key: queue_name, metric:[enqueued:float, latency:float]) processedJobs = FLAT(namespace: production, prefix: worker, metric: processed:integer) RENDER(_documentType: sidekiq, worker, queues, processedJobs) Copy "},{"title":"sfAgent Configuration","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#sfagent-configuration","content":"Content of the /opt/sfagent/config.yaml. The rules file is /opt/sfagent/statsd-rules.txt key: <profile_key>tags: Name: <instance-name> appName: <app-name> projectName: <project-name>metrics: plugins: - name: statsd enabled: true config: port: 8125 flushInterval: 10 ruleFile: '/opt/sfagent/statsd-rules.txt' Copy Output { \"_documentType\": \"sidekiq\", \"_tag_Name\": \"vm\", \"queues\": [ { \"latency\": 0, \"queue_name\": \"default\", \"enqueued\": 0 } ], \"_plugin\": \"statsD\", \"processedJobs\": 107, \"worker\": [ { \"processing_time\": 1472, \"worker_name\": \"RemoveEndangeredWorker\", \"success\": 1 }, { \"processing_time\": 1113, \"worker_name\": \"AddEndangeredWorker\", \"success\": 1 } ], \"_tag_projectName\": \"statsDProject\", \"_tag_uuid\": \"080027957dd8\", \"time\": 1616132931981, \"_tag_appName\": \"statsDApp\"} Copy "},{"title":"See Also","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#see-also","content":"Linux monitoring LSOF NETSTAT Prometheus Integration "},{"title":"Tracing Java Applications","type":0,"sectionRef":"#","url":"docs/Tracing/java","content":"","keywords":""},{"title":"Instances","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#instances-1","content":"Install sfAgent which automatically installs sfTrace agent as well. Link the application with sfTrace Java Agent "},{"title":"Command Line","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#command-line","content":"Use the following arguments while starting your application using java –jar command, in IDE, Maven or Gradle script: java -javaagent:/opt/sfagent/sftrace/java/sftrace-java-agent.jar -Dsftrace.service_name=<my-service> -jar <application jar> Copy Note: If service_name is not provided, an auto discovered service name will be added. Service_name is used to identify and filter the traces related to an application and should be named appropriately to distinctly identify it. Service name must only contain characters from the ASCII alphabet, numbers, dashes, underscores and spaces. Additional features available for Spring Boot Applications# By default, transaction names of unsupported Servlet API-based frameworks are in the form of $method unknown route. To modify this and to report the transactions names in the form of $method $path, use the following in javaagent configuration. This option is applicable only for spring-boot based applications. -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true Copy Normalizing Transaction URLs# If your URLs contain path parameters like /user/$userId, it can lead to an explosion of transaction types. This can be avoided by using URL groups. For example, if the application supports urls like: /owners, /owners/<owner_id>, /owners/<owner_id>/edit, /owners/<owner_id>/pets, Copy then url groups would be configured as: url_groups=/owners/*,/owner/*/edit,/owners/*/pets Copy Example of running java application via command line using these parameters# java -javaagent:/opt/sfagent/sftrace/java/sftrace-java-agent.jar -Dsftrace.service_name=my-service -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true -Delastic.apm.url_groups=/owners/*,/owner/*/edit,/owners/*/pets -jar <application jar> Copy "},{"title":"Apache Tomcat","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#apache-tomcat","content":"Add the agent configuration in setenv.sh. If this file is not present, create the file in below folder <tomcat installation path>/bin Copy Refer to tomcat_setenv.sh for tracing specific configuration that needs to be copied to setenv.sh file. Make the file executable using chmod +x bin/setenv.sh and start the server Additional features available for Spring Boot Applications# By default, transaction names of unsupported Servlet API-based frameworks are in the form of $method unknown route. To modify this and to report the transactions names in the form of $method $path, use the following in javaagent configuration. This option is applicable only for spring-boot based applications. -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true Copy Normalizing Transaction URLs# If your URLs contain path parameters like /user/$userId, it can lead to an explosion of transaction types. This can be avoided by using URL groups. For example, if the application supports urls like: /owners, /owners/<owner_id>, /owners/<owner_id>/edit, /owners/<owner_id>/pets, Copy then url groups would be configured as: url_groups=/owners/*,/owner/*/edit,/owners/*/pets Copy Example of running java application via command line using these parameters# java -javaagent:/opt/sfagent/sftrace/java/sftrace-java-agent.jar -Dsftrace.service_name=my-service -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true -Delastic.apm.url_groups=/owners/*,/owner/*/edit,/owners/*/pets -jar <application jar> Copy "},{"title":"JBOSS EAP","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#jboss-eap","content":"Standalone Mode# Add the agent configuration in standalone.conf file and start the server Refer to JBOSS_standalone.conf for tracing specific configuration. Copy from section with “SFTRACE-CONFIG” in comments Domain Mode# Add the agent configuration in domain.xml and start the server Refer to JBOSS_domain.xml for tracing specific configuration. Copy from section with “SFTRACE-CONFIG” in comments After updating the configuration, restart the application. Additional features available for Spring Boot Applications# By default, transaction names of unsupported Servlet API-based frameworks are in the form of $method unknown route. To modify this and to report the transactions names in the form of $method $path, use the following in javaagent configuration. This option is applicable only for spring-boot based applications. -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true Copy Normalizing Transaction URLs# If your URLs contain path parameters like /user/$userId, it can lead to an explosion of transaction types. This can be avoided by using URL groups. For example, if the application supports urls like: /owners, /owners/<owner_id>, /owners/<owner_id>/edit, /owners/<owner_id>/pets, Copy then url groups would be configured as: url_groups=/owners/*,/owner/*/edit,/owners/*/pets Copy Example of running java application via command line using these parameters# java -javaagent:/opt/sfagent/sftrace/java/sftrace-java-agent.jar -Dsftrace.service_name=my-service -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true -Delastic.apm.url_groups=/owners/*,/owner/*/edit,/owners/*/pets -jar <application jar> Copy "},{"title":"Docker","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#docker-1","content":"Refer to java_Dockerfile. Look at sections with SFTRACE-CONFIG description. Installation steps are provided. copy the trace agent to the container and start the container by attaching the agent to the application. Additionally, user has to add SnappyFlow configurations for profile_key, projectName, appName to the docker file Once updated, build and start the container. "},{"title":"Kubernetes","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#kubernetes-1","content":"sfTrace is run as an initContainer in the application pod. User can deploy this either using a manifest yaml or a Helm chart. "},{"title":"Example of Manifest yaml","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#example-of-manifest-yaml","content":"java_k8s_standalone_deployment.yaml  "},{"title":"Example of a Helm chart","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#example-of-a-helm-chart","content":"Update values.yaml: Refer to java_k8s_with_helm_chart_values.yaml to configure agent specific properties. Look at sections with SFTRACE-CONFIG description Update deployment.yaml: Refer to java_k8s_with_helm_chart_deployment.yaml to copy trace agent to the container and start the container by attaching the agent. Look at sections with SFTRACE-CONFIG description "},{"title":"ECS","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#ecs-1","content":""},{"title":"Create the Task definition","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#create-the-task-definition","content":"Open Amazon ECS, in navigation pane, choose task definition and click on Create New Task Definition and select the launch type as EC2 or Fargate, click on Next step. Give the Task definition Name Task Role, choose an IAM role that provides permissions for containers in your task to make calls to AWS APIs on your behalf and Network Mode Click on Add containers. Give a Container name, and give the Image of your Java Application. Set Memory limit and port mappings as per your task requirements. In the environment section, for Entry Point give sh , -c For Command paste the following lines mkdir /sfagent && wget -O /sfagent/sftrace-agent.tar.gzhttps://github.com/snappyflow/apm-agent/releases/download/latest/sftrace-agent.tar.gz && cd /sfagent && tar -xvzf sftrace-agent.tar.gz && java -javaagent:/sfagent/sftrace/java/sftrace-java-agent.jar -jar <your_jar_name> Copy Note:Some EC2 task definitions may be running on host containers that don’t recoginise the wget command in such case, add below lines in the above command, apt update && apt -y upgrade. Add the following Environment Variables:- SFTRACE_PROJECT_NAME <project_name>SFTRACE_APP_NAME <app_name>SFTRACE_SERVICE_NAME <service_name>SFTRACE_PROFILE_KEY <profile_key> Copy The below environment variables are only applicable for springmvc and optional. ELASTIC_APM_DISABLE_INSTRUMENTATIONS spring-mvcELASTIC_APM_USE_PATH_AS_TRANSACTION_NAME \"true\" Copy "},{"title":"Create the Cluster","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#create-the-cluster","content":"In the Navigation pane, select Clusters and click on Create ClusterSelect the template as per your requirementGive a Cluster name and give instance, networking Configurations IAM role as per your task requirements "},{"title":"Create the Service","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#create-the-service","content":"Click on the Cluster Name you created in the step2Click on Create , Select the Launch type matching to your task definition. Select the Task Definition Name and Version in the Drop down matching to the task definition you created in step 1Give a Service Name and select other requirements as per your task compatibilityClick on next step and start your service "},{"title":"Postgres in Kubernetes","type":0,"sectionRef":"#","url":"docs/Integrations/postgres/postgres_kubernetes","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Postgres in Kubernetes","url":"docs/Integrations/postgres/postgres_kubernetes#overview","content":"PostgreSQL running in Kubernetes can be monitored in SnappyFlow using two approaches: sfKubeAgent as sidecar containerPrometheus exporter  "},{"title":"PostgreSQL monitoring with sfKubeAgent","type":1,"pageTitle":"Postgres in Kubernetes","url":"docs/Integrations/postgres/postgres_kubernetes#postgresql-monitoring-with-sfkubeagent","content":"sfKubeAgent is run as a sidecar with the configMap shown below. The config map instantiates plugins for metrics, general logs and slow queries. apiVersion: v1 kind: ConfigMap metadata: name: postgres-configmap data: config.yaml: |- key: <profile_key> metrics: plugins: - name: postgres enabled: true interval: 60 config: documentsTypes: #user can enable all or only needed documents - databaseDetails - indexDetails8 - queryDetails - serverDetails - tableDetails host: 127.0.0.1 user: <userName> password: <password> port: 5432 logging: plugins: - name: postgres-general enabled: true config: log_level: - error - warning - info - log log_path: /var/log/postgres/*.log - name: postgres-slowquery enabled: true config: log_path: /var/log/postgres/*.log Copy The example of PostgreSQL pod with Postgres and sfKubeAgent containers is shown below: kind: Pod apiVersion: v1 metadata: name: postgres-pod labels: snappyflow/appname: <app_name> snappyflow/projectname: <project_name> spec: containers: - name: postgres-container securityContext: {} image: \"postgres:9.6\" args: [\"-c\", \"log_statement=all\", \"-c\", \"log_min_messages=warning\", \"-c\", \"log_min_duration_statement=200\", \"-c\",\"log_directory=/var/log/postgres\",\"-c\",\"log_line_prefix=< %m > \",\"-c\",\"log_filename=postgresql-%Y-%m-%d_%H%M%S.log\",\"-c\",\"log_truncate_on_rotation=off\",\"-c\",\"log_rotation_age=1d\",\"-c\",\"logging_collector=on\"] imagePullPolicy: IfNotPresent ports: - name: tcp containerPort: 5432 protocol: TCP env: - name: POSTGRES_PASSWORD value: <password> - name: POSTGRES_USER value: <userName> volumeMounts: - name: varlog mountPath: /var/log/postgres # Snappyflow's sfkubeagent container - name: sfagent-container image: snappyflowml/sfagent:latest imagePullPolicy: Always command: - /app/sfagent - -enable-console-log env: - name: APP_NAME value: <app_name> - name: PROJECT_NAME value: <project_name> volumeMounts: - name: configmap-postgres mountPath: /opt/sfagent/config.yaml subPath: config.yaml - name: varlog mountPath: /var/log/postgres volumes: - name: configmap-postgres configMap: name: postgres-configmap - name: varlog emptyDir: {} Copy "},{"title":"Viewing data and dashboards","type":1,"pageTitle":"Postgres in Kubernetes","url":"docs/Integrations/postgres/postgres_kubernetes#viewing-data-and-dashboards","content":"Data generated by plugin can be viewed in “browse data” page inside the respective application under plugin=postgres and documentType= serverDetails, databaseDetails, tableDetails, IndexDetails Dashboard for this data can be instantiated by Importing dashboard template PostgreSQL to the application dashboard  "},{"title":"PostgreSQL monitoring with Prometheus","type":1,"pageTitle":"Postgres in Kubernetes","url":"docs/Integrations/postgres/postgres_kubernetes#postgresql-monitoring-with-prometheus","content":"Refer to Prometheus Exporter overview to understand how SnappyFlow monitors using Prometheus exporters. "},{"title":"Pre-requisites","type":1,"pageTitle":"Postgres in Kubernetes","url":"docs/Integrations/postgres/postgres_kubernetes#pre-requisites","content":"Prometheus exporter is deployed as a side-car in the application container and the exporter port is accessible to sfPod  "},{"title":"Configurations","type":1,"pageTitle":"Postgres in Kubernetes","url":"docs/Integrations/postgres/postgres_kubernetes#configurations","content":"kind: Pod apiVersion: v1 metadata: name: postgres-pod labels: snappyflow/appname: <app_name> snappyflow/projectname: <project_name> snappyflow/component: postgresql spec: containers: - name: postgres-exporter image: bitnami/postgres-exporter ports: - name: pg-exporter containerPort: 9187 command: [\"/bin/sh\", \"-c\"] args: ['DATA_SOURCE_NAME=\"postgresql://<user_name>:<password>@localhost:5432/<dbname>?sslmode=disable\" /opt/bitnami/postgres-exporter/bin/postgres_exporter'] - name: postgres-container securityContext: {} image: \"postgres:9.6\" args: [\"-c\", \"log_statement=all\", \"-c\", \"log_min_messages=warning\", \"-c\", \"log_min_duration_statement=200\", \"-c\",\"log_line_prefix=< %m > \"] imagePullPolicy: IfNotPresent ports: - name: tcp containerPort: 5432 protocol: TCP env: - name: POSTGRES_PASSWORD value: <password> - name: POSTGRES_USER value: <user_name> - name: POSTGRES_DB value: <dbname> Copy "},{"title":"Viewing data and dashboards","type":1,"pageTitle":"Postgres in Kubernetes","url":"docs/Integrations/postgres/postgres_kubernetes#viewing-data-and-dashboards-1","content":"Data generated by plugin can be viewed in browse data page inside the respective application under plugin=kube-prom-postgres and documentType= psql Dashboard for this data can be instantiated by Importing dashboard template PostgreSQL_Prom to the application dashboard  "},{"title":"PostgreSQL Pod Centralized Logging","type":1,"pageTitle":"Postgres in Kubernetes","url":"docs/Integrations/postgres/postgres_kubernetes#postgresql-pod-centralized-logging","content":"Pls refer to Centralized Logging Overview to understand how SnappyFlow implements centralized logging Centralized logging approach requires the application pod to stream logs to stdout, which is achieved by running a busy box container as shown below. kind: PodapiVersion: v1metadata: name: postgres-pod labels: snappyflow/appname: <app_name> snappyflow/projectname: <project_name> snappyflow/component: postgresqlspec: containers: - name: postgres-exporter image: bitnami/postgres-exporter ports: - name: pg-exporter containerPort: 9187 command: - /bin/sh - '-c' args: - >- DATA_SOURCE_NAME=\"postgresql://<user_name>:<password>@localhost:5432/<dbname>?sslmode=disable\" /opt/bitnami/postgres-exporter/bin/postgres_exporter - name: postgres-container securityContext: {} image: 'postgres:9.6' args: - '-c' - log_statement=all - '-c' - log_min_messages=warning - '-c' - log_min_duration_statement=200 - '-c' - 'log_line_prefix=< %m > ' - '-c' - log_directory=/var/log/postgres - '-c' - log_filename=postgresql.log - '-c' - logging_collector=on imagePullPolicy: IfNotPresent ports: - name: tcp containerPort: 5432 protocol: TCP env: - name: POSTGRES_PASSWORD value: <password> - name: POSTGRES_USER value: <user_name> - name: POSTGRES_DB value: <dbname> volumeMounts: - name: postgres-log mountPath: /var/log/postgres - name: postgres-general image: busybox command: - /bin/sh - '-c' args: - tail -n+1 -f /var/log/postgres/*.log volumeMounts: - name: postgres-log mountPath: /var/log/postgres volumes: - name: postgres-log emptyDir: {} Copy "},{"title":"Tracing Ruby Applications","type":0,"sectionRef":"#","url":"docs/Tracing/ruby","content":"","keywords":""},{"title":"Instances","type":1,"pageTitle":"Tracing Ruby Applications","url":"docs/Tracing/ruby#instances-1","content":"sfTrace Ruby Agent automatically instruments applications, based on web frameworks Ruby on Rails and other Rack-compatible applications. It uses the hooks and signals provided by these frameworks to trace the application. Installation steps# Install sfAgent (if not already installed)Confirm that /opt/sfagent/config.yaml is configured with correct profile key and tags. Trace setup for Ruby on Rails Applications# Install sftrace agent by either adding the gem to Gemfile gem 'sftrace-agent' and then execute the command bundle install or install the agent yourself using the command gem install sftrace-agent.Add the agent configuration file in application’s config folder. Refer to application.rb for tracing specific configuration. Search for SFTRACE-CONFIG in sample application.rb  "},{"title":"Kubernetes","type":1,"pageTitle":"Tracing Ruby Applications","url":"docs/Tracing/ruby#kubernetes-1","content":"Follow the steps below to enable tracing in Ruby on Rails applications running as a Kubernetes pod Follow the first 2 steps in Trace setup for Ruby on Rails Applications to update the application with agent specific configuration. sfTrace ruby agent configuration can be set to the application running in Kubernetes pod. This can be done in 2 ways: Option 1: manifest deployment# Refer to ruby_k8s_manifest_deployment.yaml to copy trace agent configuration to the application container and start the container with trace agent configurations. Search for SFTRACE-CONFIG in sample deployment yaml file Once updated, deploy the pod. Option 2: Deploy using helm chart# Step 1: Update values.yaml Refer to k8s_with_helm_chart_values.yaml to configure agent specific properties. Search for SFTRACE-CONFIG in sample values.yaml file Step 2: Update deployment.yaml Refer to ruby_k8s_with_helm_chart_deployment.yaml to copy trace agent to the application container and start the container using the trace agent configurations. Search for SFTRACE-CONFIG in sample deployment yaml file Once updated, deploy the pod. "},{"title":"Tracing in SnappyFlow","type":0,"sectionRef":"#","url":"docs/Tracing/overview","content":"Tracing in SnappyFlow SnappyFlow supports distributed tracing compliant with Opentracing standard. Tracing allows users to visualize the sequence of steps a transaction (whether API or non-API such as a Celery job) takes during its execution. This analysis is extremely powerful and allows pinpointing the source of problems such as abnormal time being spent on an execution step or identifying point of failure in a transaction. SnappyFlow refers to distributed tracing as sfTrace. Select your language# Java# Python# Ruby# Go# Node.js# C##","keywords":""},{"title":"Feature Extraction","type":0,"sectionRef":"#","url":"docs/Log_management/feature_extraction","content":"","keywords":""},{"title":"Examples","type":1,"pageTitle":"Feature Extraction","url":"docs/Log_management/feature_extraction#examples","content":"Example 1 EV(message, /\\d+\\.\\d+\\.\\d+\\.\\d+/, string, ip_addr) Copy Extract IP address from field message. {\"message\": \"DHCPACK from 172.31.32.1 (xid=0x381e913e)\",“ip_addr”: “172.31.32.1”}{\"message\": \"DHCPREQUEST on eth0 to 172.31.32.1 port 67 (xid=0x381e913e)\", “ip_addr”:“172.31.32.1”}{\"message\": \"Received disconnect from 167.114.226.137 port 47545:11: [preauth]\", “ip_addr”:“167.114.226.137”}{\"message\": \"Disconnected from 167.71.217.175 port 46180 [preauth]\",“ip_addr”: “167.71.217.175”}{\"message\": \"Received disconnect from 51.91.159.46 port 33914:11: [preauth]\",“ip_addr”:“51.91.159.46”} Copy Example 2 EV(log_msg, /\\d+(.\\d+)*(?=ms)/, float, delay) Copy Extract delay values from the field “log_msg”. Delay value is identified by the pattern (a) one digit string, immediately followed by string “ms” OR (b) two digit strings, each separated by “.” and second digit string is immediately followed by string “ms”. Save extracted value in field named delay. {\" log_msg\": \"Received request from 10.11.100.29 and re-directed to 33.229.79.17 in 10.34ms\", “delay”: 10.34}{\"log_msg\": \"Latency time is 5ms\",“delay”: 5}{\"log_msg\": \"Process 2131 completed in 50s\"} – *nothing is extracted from this message* Copy Example 3 (\"received request from\" && \"re-directed\") with EV(log_msg, /\\d+.\\d+.\\d+.\\d+/, string, -,rd_ip_addr) Extract IP addresses from log_msg field. Skip the first extraction and save the second extraction to field named rd_ip_addr. {\"log_msg\": \"Received request from 10.11.100.29 and re-directed to 33.229.79.17 in 10.34ms\",“rd_ip_addr”: \"33.229.79.17”} Copy "},{"title":"Extract Group","type":1,"pageTitle":"Feature Extraction","url":"docs/Log_management/feature_extraction#extract-group","content":"Extract Group (EG) construct allow users to specify a pattern with multiple groups and extract the value of each group into a separate field. In Extract Value a single pattern contained a single group, but this single group could match multiple values in the field. This concept will be better understood through examples. EG construct uses, following parameters: Parameter\tUsefield\tfield name to which the extraction is applied, in this case message field of the log pattern\tREGEX pattern which identifies the extraction groups. Pattern is enclosed in a pair of “/”. In the REGEX pattern the group to be extracted is enclosed in parenthesis “()”. name:type\ta comma separated list of name:type tuples which specify name of the extracted group and its type. Type can be int, float or string. If no type is specified, it is assumed to be string. Example 1 EG(message, /(\\d+.\\d+.\\d+.\\d+) - \\[(.*)\\] \"(\\w+) ([^\\s]+) ([^\\s\"]+)\" (\\d+) (\\d+) \"-\" \"(.*)\" \"-\"/, host, httpd_timestamp, method, path, header, code:int, size:int, agent) Copy The example illustrates extracting information from an nginx access log, which contains the host sending the request, time at which the request was received, HTTP method, request Path, header version, response code, size and the agent making request. The response code, and size are integer values, whereas the other extractions are string type values. Patterns corresponding to these groups are highlighted in the above EG construct. Note: the group REGEX patterns are enclosed in “()”, strings matching each of those patterns are extracted and placed in the field name provided in the same order they appear. {\"message\": \"172.31.31.45 - [06/May/2020:11:44:41] \\\"GET /owners/2 HTTP/1.1\\\" 200 4964\\\"-\\\" \\\"Apache-HttpClient/4.5.7 (Java/1.8.0_242)\\\" \\\"-\\\" rt=14.717 uct=0.000 uht=14.716urt=14.716\",\"host\": “172.31.31.45”,\"httpd_timestamp\": \"06/May/2020:11:44:41\",\"method\": \"GET\",\"path\": \"/owners/2\",\"header\": \"HTTP/1.1\",\"code\": 302,\"size\": 4964,\"agent\": \"Apache-HttpClient/4.5.7 (Java/1.8.0_242)\"}{\"message\": \"172.31.81.81 - [06/May/2020:11:44:41] \\\"POST /owners/new HTTP/1.1\\\" 20124 \\\"-\\\" \\\"Mozilla/5.0 (Windows NT 10.0; WOW64)\\\" \\\"-\\\" rt=1.088 uct=0.000 uht=1.088urt=1.088\",\"host\": “172.31.81.81”, \"httpd_timestamp\": \"06/May/2020:11:44:41\",\"method\": \"POST\",\"path\": \"/owners/new\",\"header\": \"HTTP/1.1\",\"code\": 201,\"size\": 24,\"agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64)\"} Copy Example 2 EG(message, /TTY=\\w+ ; PWD=([^\\s]+) ; USER=(\\w+) ; COMMAND=(.*)$/, path, username, cmd) Copy {\"message\": \"root : TTY=unknown ; PWD=/home/centos ; USER=root ; COMMAND=/bin/rm –rfjmeter.log\",\"pwd\": \"/home/centos\",\"username\": \"root\",\"cmd\": \"/bin/rm -rf jmeter.log\"}{\"message\": \"centos : TTY=unknown ; PWD=/home/kevin ; USER=adam ; COMMAND=/bin/rm -rf jmeter.log \",\"pwd\": \"/home/kevin\",\"username\": \"adam\",\"cmd\": \"/bin/rm -rf jmeter.log\"} Copy "},{"title":"Extract Pair","type":1,"pageTitle":"Feature Extraction","url":"docs/Log_management/feature_extraction#extract-pair","content":"Applications often use logs to dump their internal statistics for debugging purposes. Most often, such logs will contain expressions like: <name1>:<value1>, <name2>:<value2>, ….. Copy OR <name1>=<value1>; <name2>=<value2>;…. Copy In such logs, the field-value pairs can be identified with a value separator and a pair delimiter. In the first example value separator is “:” and pair delimiter is “,”; whereas in the second example, value separator is “=” and pair delimiter is “;”. General syntax used in Extract Pairs is: EP(field, /pair_delimiter/, /value_separator/, convert=[..], include=[..], exclude=[..]) Parameter\tUsefield\tfield name to which the extraction is applied, example: *message* field of the log pair_delimiter\tREGEX pattern which identifies the field-value pair delimiter. Pattern is enclosed in a pair of “/”. value_separator\tREGEX pattern which identifies the separator between field and value. Pattern is enclosed in a pair of “/”. convert=[]\ta list of field:type tuples, which specify how to convert the field values. If nothing is specified, all fields are considered have a string type value. Note: the field names are the names extracted from the log message itself. Example is convert=(field1:int,field2:float). This is an optional field, all values are converted to string type, if not specified. include=[]\tif only selected fields from the extraction are to be added to the log document, list those field names. Note: include and exclude lists can not both be present at the same time. An optional field and can be omitted. exclude=[]\tif selected fields from the extraction are not to be added to the log document, list those field names. Note: include and exclude lists can not both be present at the same time. An optional field and can be omitted. Example 1 EP(message, /,/, /=/, convert=[price:int]) Copy From the field message, extract field-value pairs where pair_delimiter is “,” and value separator is “=”. Convert the value for field “price” to integer. {\"message\": \"name=Kevin,user_id=3212,order_id=234,price=240.56\",\"name\": \"Kevin\", \"user_id\": \"3212\", \"order_id\": \"234\", \"price\": 241}{\"message\": \"name=larry,user_id=1111,order_id=100,price=123\",\"name\": \"larry\",\"user_id\": \"1111\",\"order_id\": \"100\",\"price\": 123}{\"message\": \"process completed in 20s, and details=HEXA3413\",\"and details\": \"HEXA3413\"} Copy Note: in the above example Field “price”, if present is converted to Integer. In the 3 rd message field, “price” is not presentIn the 3 rd message, notice that the field name is taken as “and details”. This is because the pair delimiter was specified as “,” and anything between “,” (pair delimiter) and “=” (value separator) is taken as a field name Example 2 EP(message, /,|(.*,\\s+and)/, /=/, exclude=[name], convert=[price:float, order_id]) Copy From the field message, extract field-value pairs delimited by text matching the pattern /,|(.*,\\s+and)/ and has a value separator “=”. Note that the pair delimiter has a REGEX pattern which defines multiple delimiters. Each delimiter is separated by “|”. Options specified here are:“,” – comma OR “.*,\\s+and’ - a string with any number of characters(.*), followed by a “,” followed by any number of white space characters(\\s+) and followed by the string “and”. Copy Following log messages will illustrate the use of this extraction {\"message\": \"name=Kevin,user_id=3212,order_id=234,price=240.56\",\"user_id\": \"3212\",\"order_id\": 234,\"price\": 240.56}{\"message\": \"name=larry,user_id=1111,order_id=100,price=123\",\"user_id\": \"1111\", \"order_id\": 100,\"price\": 123}{\"message\": \"process completed in 20s, and details=HEXA3413\",\"details\": \"HEXA3413\"} Copy In the 1 st and 2 nd example messages, pair delimiter used was “,”.In the 3 rd example message, the pair delimiter is “process completed in 20s, and”Also note, in messages 1 and 2, the field “name” was extracted, but was not added to the output document, because of the “exclude” option. Example 3 message: (user_id && price) with EP(message, /,/, /=/, include=[price]) Copy {\"message\": \"name=Kevin,user_id=3212,order_id=234,price=240.56\",\"price\": “240.56”}{\"message\": \"name=larry,user_id=1111,order_id=100,price=123\",\"price\": “123”}{\"message\": \"process completed in 20s, and details=HEXA3413\"} Copy Since include option had the field “price”, only those documents where field “price” was found were updated. 3 rd document though matched the delimiter and separator, did not have the field “price”. Example 4 EP(message, /(.*PID.*?(?=\\w+=))|(\\})|(\\{)|(\\s(?=\\w+=))/, /=/, exclude=[ startTime, endTime], convert=[count, minimum: float, mean: float, maximum: float]) Copy Another example using a complex delimiter, with exclude option and convert options. Multiple pair delimiters are used. {\"message\": \"StatisticsLogTask - PID1 - context=Execution Fill {subContext=Order Update section=Top Level startTime=2019-12-17 23:59 endTime=2019-12-20 00:00} count=3minimum=1.0 mean=5.0 maximum=21.0\",\"context\": \"Execution Fill\", \"subContext\": \"Order Update\",\"section\": \"Top Level\", \"count\": 3,\"minimum\": 1,\"maximum\": 21,\"mean\": 5}{\"message\": \"StatisticsLogTask - PID2 - context=Execution Fill {subContext=Order Placed section=Mid Level startTime=2019-12-17 23:59 endTime=2019-12-20 00:00 count=6minimum=0.813 mean=7.9 maximum=13.0}\",\"context\": \"Execution Fill\", \"subContext\": \"Order Placed\",\"section\": \"Mid Level\", \"count\": 6,\"minimum\": 0.813, \"maximum\": 13, \"mean\": 7.9} Copy In the extraction, multiple pair delimiters were specified /(.*PID.*?(?=\\w+=))|(\\})|(\\{)|(\\s(?=\\w+=))/ Copy StatisticsLogTask - PID1 - context=Execution Fill (subContext=Order Update section=Top Level startTime=2019-12-17 23:59 endTime=2019-12-20 00:00} count=3 minimum=1.0 mean=5.0 maximum=21.0 StatisticsLogTask - PID2 - context=Execution Fill (subContext=Order Place section=Mid Level startTime=2019-12-17 23:59 endTime=2019-12-20 00:00} count=6 minimum=0.813 mean=7.3 maximum=13.0 Value pairs identified by the pair delimiter pattern is shown in the above picture. Though startTime and endTime are extracted, but they are excluded. "},{"title":"Tools and Tips","type":1,"pageTitle":"Feature Extraction","url":"docs/Log_management/feature_extraction#tools-and-tips","content":"Java REGEX testing tools can be used to validate the REGEX patterns used for extractions. A web tool like https://www.freeformatter.com/java-regex-tester.html can be used for this. Pattern used in extract values (EV) should match the sub-strings you intend to extract. For example, to extract the timestamps from the following message, a pattern like below can be used /\\d+-\\d+- \\d+\\s+\\d+:\\d+/ Copy Extract Value Pattern \\d+-\\d+-\\d+\\s+\\d+:\\d+ Copy String: StatisticsLogTask - PID1 - context=Execution Fill {subContext=Order Update section=Top Level startTime=2019-12-17 23:59 endTime=2019-12-20 00:00} count=3 minimum=1.0 mean=5.0 maximum=21.0 Copy StatisticsLogTask - PID2 - Context = Execution Fill (subContext=Order Place section=Mid Level startTime=2019-12-17 23:59 endTime=2019-12-20 00:00 count=6 minimum=0.813 mean=7.9 maximum=13.0 Copy Match #\tGroup index\tStart index\tEnd index\tGroup content1\t0\t103\t119\t2019-12-17 23:59 2\t0\t128\t144\t2019-12-20 00:00 Extract Group allows to apply a REGEX to the field and extract only the groups identified (i.e. patterns enclosed in parentheses). As an example the following group REGEX when applied to the string below will result in the following groups. Extract group pattern (\\d+\\.\\d+\\.\\d+\\.\\d+) - - \\[[^\\]]* \\+\\d+\\] \\\\\\\\\\\\\\\"POST \\/topics\\/(\\w+)-(\\w+) HTTP/1.0\\\\\\\\\\\\\\\"(\\d+) (\\d+) (\\d+) Copy String: 10.233.117.0 - - [26/Oct/2020:22:30:00 +0000] \\\\\\\"POST /topics/metric-grqqwwi7 HTTP/1.0\\\\\\\"200 12602 12 (io.confluent.rest-utils.requests) Copy Copy the pattern and string to the java-regex-tester. Observe the resultant matches and groups. In Extract Group, the values extracted in non-zero group index are used. 10.233.177.0 - - [26/Oct/2020:22:30:00 +0000] \\\\\\\"Post /topica/metric-grqqwwi7 HTTP/1.0\\\\\\\" 200 12602 12(io.confluent.reat-utile.requests) Copy Match #\tGroup index\tStart index\tEnd index\tGroup content1\t0\t0\t106\t10.233.117.0--[26/Oct/2000:22:30:00 +0000] \\\\\"POST /topics/metric-grqqwwi7 HTTP/1.0\\\\* 200 1260212 1\t1\t0\t12\t10.233.117.0 1\t2\t63\t69\tmetric 1\t3\t70\t78\tgrqqwwi7 1\t4\t92\t95\t200 1\t5\t96\t101\t12602 1\t6\t103\t106\t12 To extract value pairs, pair-delimiter should match the boundaries and all other symbols which are needed to split the string into an array of pairs. Each pair is again split, based on value- separator pattern SnappyFlow provides following built-in regex patterns, these built-in patterns can be used in place of a REGEX required in EV, EP or EG feature extraction constructs. Built-in pattern names are encapsulated in “$” $hostname$ - hostname string, e.g.apmmanager.snappyflow.io$url$ - URL, e.g. https://apmmanager.snappyflow.io$file_path$ - UNIX file path, e.g. /usr/share/nginx/html/theme-chrome.js$float$ - floating point number, e.g.31.45$integer$ - integer number, e.g. 19345$alphanumeric$ - alpha-numeric characters, e.g. admin1$alphanumericspecial$ - alpha-numeric with hyphen and underscore, e.g. date_time$string$ - a string encapsulated in ‘ (single quote) or “ (double quote)$date$ - date string, e.g. 02-04-2020$datetime$ - date with time string, e.g. 02-04-20 21:41:59$time$ - time string, e.g. 21:41:59$ipv4addr$ - IPv4 Address, e.g. 172.31.22.98 Copy Example extraction with built-in regex patterns message: \"*72281 open() \"/usr/share/nginx/html/theme-chrome.js\" failed (2: No such file or directory), client: 172.31.22.98, server: _, request: \"GET /theme-chrome.js HTTP/1.1\", host: \"apmmanager.snappyflow.io\", referrer: “https://apmmanager.snappyflow.io/” Extractionmessage:\"No such file or directory\" with EG(message,/($integer$) open\\(\\) \\\"($file_path$)\\\" failed \\(2: No such file or directory\\), client: ($ipv4addr$), server: ([^,]*), request: \\\"([^\\\"]*)\\\", host: \\\"($hostname$)\\\", referrer: \\\"($url$)\\\"/,m_size:int, m_file, m_client, m_server, m_request, m_host, m_referrer) The extraction operation above will extract the fields m_size: 72281, m_file: /usr/share/nginx/html/theme-chrome.js, m_client: 172.31.22.98, m_server: _, m_request: GET /theme-chrome.js HTTP/1.1, m_host: apmmanager.snappyflow.io and m_referrer: https://apmmanager.snappyflow.io/ Copy "},{"title":"Tracing node.js applications","type":0,"sectionRef":"#","url":"docs/Tracing/nodejs","content":"","keywords":""},{"title":"Instances","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#instances-1","content":""},{"title":"Node.JS Express","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#nodejs-express","content":"Install nodejs dependencies and save it in package.json using npm install --save elastic-apm-node@^3.20.0npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries \"elastic-apm-node\": \"^3.20.0\"\"sf-apm-lib\": \"^1.0.2\" Copy And run ‘npm install’ to install dependencies Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variable in .env file and load it using require('dotenv').config() and access it in code using process.env.<ENV_VAR> Add initilization code at start of the file Get Snappyflow trace config using const Snappyflow = require('sf-apm-lib');var sfObj = new Snappyflow(); // Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml. // Add below part to manually configure the initialization let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; sfObj.init(profileKey, projectName, appName); // Manual override let sfTraceConfig = sfObj.getTraceConfig(); Copy Initialize apm object using var apm; try { apm = require('elastic-apm-node').start({ serviceName: '<SERVICE_NAME>', // Specify your service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'] }) } catch (e) { console.log(e); } Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in add container section of task definitions. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in Snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to: View dashboard -> Click on Tracing on left side bar -> Click on view transaction -> Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-express "},{"title":"Node.JS Script","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#nodejs-script","content":"Install nodejs dependencies and save it in package.json using npm install --save elastic-apm-node@^3.20.0 npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries: \"elastic-apm-node\": \"^3.20.0\" \"sf-apm-lib\": \"^1.0.2\" Copy And run ‘npm install’ to install dependencies Add initilization code at start of the file Get Snappyflow trace config using const Snappyflow = require('sf-apm-lib'); let projectName = <SF_PROJECT_NAME>; //replace with appropriate project name let appName = <SF_APP_NAME>; //replace with appropriate application name let profileKey = <SF_PROFILE_KEY>; //replace with key copied from SF profile var sfObj = new Snappyflow(); sfObj.init(profileKey, projectName, appName); let sfTraceConfig = sfObj.getTraceConfig(); Copy Initialize apm object using var apm;try { apm = require('elastic-apm-node').start({ serviceName: '<SERVICE_NAME>', // Specify service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'] }) } catch (e) { console.log(e); } Copy Create a custom transaction and span within transaction using following code var trans = apm.startTransaction('json transaction', 'reference-app'); var span = apm.startSpan('parse json'); try { JSON.parse('{\"app\": \"test\"}') } catch (e) { apm.captureError(e); // Capture the error using apm.captureError(e) method.} // when we've processed, stop the custom span if (span) span.end() trans.result = err ? 'error' : 'success'; // end the transaction trans.end(); Copy For more info refer https://www.elastic.co/guide/en/apm/agent/nodejs/current/custom-transactions.html https://www.elastic.co/guide/en/apm/agent/nodejs/current/custom-spans.html Run you script using node <file_name.js> you should see trace data in Snappyflow server. For viewing trace in Snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to: View dashboard -> Click on Tracing on left side bar -> Click on view transaction -> Go to real time tab Refer sample script file at: https://github.com/snappyflow/tracing-reference-apps/blob/master/refapp- express/node_trace_script.js "},{"title":"Node.JS Sails","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#nodejs-sails","content":"Install nodejs dependencies and save it in package.json using npm install --save elastic-apm-node@^3.20.0 npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries \"elastic-apm-node\": \"^3.20.0\" \"sf-apm-lib\": \"^1.0.2\" Copy And run ‘npm install’ to install dependencies Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variable in .env file and load it using require('dotenv').config() and access it in code using process.env.<ENV_VAR> Add initilization code at start of the file in globals.js present in config folder. Get Snappyflow trace config using: const Snappyflow = require('sf-apm-lib'); var sfObj = new Snappyflow(); // Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml. // Add below part to manually configure the initialization let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; sfObj.init(profileKey, projectName, appName); // Manual override let sfTraceConfig = sfObj.getTraceConfig(); Copy Initialize apm object using: var apm; try { apm = require('elastic-apm-node').start({ serviceName: '<SERVICE_NAME>', // Specify your service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'] }) } catch (e) { console.log(e); } Copy Attach apm object to globals – This is required so we can use apm variable in other files as part of global sails object. module.exports.globals = { _: require('@sailshq/lodash'), async: false, models: true, sails: true, apm : apm, logger: logger }; Copy Also add middleware in http.js file present in config folder. Which allows to instrument our code. module.exports.http = { middleware: { order: [ 'elasticAPM' ], elasticAPM: (function () { return function (err, req, res, next) { apm.middleware.connect(); if (typeof err !== 'undefined') apm.captureError(err); return next(); }; })() } }; Copy Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/RefappNodeSail "},{"title":"Kubernetes","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#kubernetes-1","content":""},{"title":"Node.JS Express","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#nodejs-express-1","content":"Install nodejs dependencies and save it in package.json using npm install --save elastic-apm-node@^3.20.0 npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries: \"elastic-apm-node\": \"^3.20.0\" \"sf-apm-lib\": \"^1.0.2\" Copy And run ‘npm install’ to install dependencies Add initilization code at start of the file in app.js Get Snappyflow trace config using const Snappyflow = require('sf-apm-lib'); var sfObj = new Snappyflow(); // Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml. // Add below part to manually configure the initialization let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; sfObj.init(profileKey, projectName, appName); // Manual override let sfTraceConfig = sfObj.getTraceConfig(); Copy Initialize apm object using var apm; try { apm = require('elastic-apm-node').start({ serviceName: '<SERVICE_NAME>', // Specify your service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'] }) } catch (e) { console.log(e); } Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in Kubernetes deployment file. https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/ Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in Snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to: View dashboard -> Click on Tracing on left side bar -> Click on view transaction -> Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-express "},{"title":"Node.JS Sails","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#nodejs-sails-1","content":"Install nodejs dependencies and save it in package.json using npm install --save elastic-apm-node@^3.20.0 npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries: \"elastic-apm-node\": \"^3.20.0\" \"sf-apm-lib\": \"^1.0.2\" Copy And run ‘npm install’ to install dependencies Add initilization code at start of the file in globals.js present in config folder. Get Snappyflow trace config using const Snappyflow = require('sf-apm-lib'); var sfObj = new Snappyflow(); // Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml. // Add below part to manually configure the initialization let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; sfObj.init(profileKey, projectName, appName); // Manual override let sfTraceConfig = sfObj.getTraceConfig(); Copy Initialize apm object using var apm; try { apm = require('elastic-apm-node').start({ serviceName: '<SERVICE_NAME>', // Specify your service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'] }) } catch (e) { console.log(e); } Copy Attach apm object to globals – This is required so we can use apm variable in other files as part of global sails object module.exports.globals = { _: require('@sailshq/lodash'), async: false, models: true, sails: true, apm : apm, logger: logger }; Copy Also add middleware in http.js file present in config folder. Which allows to instrument our code module.exports.http = { middleware: { order: [ 'elasticAPM' ], elasticAPM: (function () { return function (err, req, res, next) { apm.middleware.connect(); if (typeof err !== 'undefined') apm.captureError(err); return next(); }; })() }}; Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in Kubernetes deployment file. https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/ If deploying with helm provide above variables in values.yaml and use them in deployment file of charts. https://phoenixnap.com/kb/helm-environment-variables Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/RefappNodeSail "},{"title":"Docker","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#docker-1","content":""},{"title":"Node.JS Express","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#nodejs-express-2","content":"Install nodejs dependencies and save it in package.json using RUN npm install --save elastic-apm-node@^3.20.0 RUN npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries: \"elastic-apm-node\": \"^3.20.0\" \"sf-apm-lib\": \"^1.0.2\" Copy And run ‘npm install’ to install dependencies Add initilization code at start of the file in app.js Get Snappyflow trace config using const Snappyflow = require('sf-apm-lib'); var sfObj = new Snappyflow(); // Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml. // Add below part to manually configure the initialization let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; sfObj.init(profileKey, projectName, appName); // Manual override let sfTraceConfig = sfObj.getTraceConfig(); Copy Initialize apm object using var apm; try { apm = require('elastic-apm-node').start({ serviceName: '<SERVICE_NAME>', // Specify your service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'] }) } catch (e) { console.log(e); } Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in docker-compose.yml or docker stack deployment file or at command line when using docker run command for deployment. Eg: Docker-compose and stack: https://docs.docker.com/compose/environment-variables/ Docker run cli command: docker run -d -t -i -e SF_PROJECT_NAME='<Project name>' \\ -e SF_APP_NAME='<SF_APP_NAME>' \\ -e SF_PROFILE_KEY='<snappyflow profile key>' \\ --name <container_name> <dockerhub_id/image_name> Copy Once your server is up and running you can check trace in Snappyflow Server. // Project related info For viewing trace in Snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-express "},{"title":"Node.JS Sails","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#nodejs-sails-2","content":"Install nodejs dependencies and save it in package.json using RUN npm install --save elastic-apm-node@^3.20.0 RUN npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries: \"elastic-apm-node\": \"^3.20.0\" \"sf-apm-lib\": \"^1.0.2\" Copy And run ‘npm install’ to install dependencies Add initilization code at start of the file in globals.js present in config folder. Get Snappyflow trace config using const Snappyflow = require('sf-apm-lib'); var sfObj = new Snappyflow(); // Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml. // Add below part to manually configure the initialization let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; sfObj.init(profileKey, projectName, appName); // Manual override let sfTraceConfig = sfObj.getTraceConfig(); Copy Initialize apm object using var apm; try { apm = require('elastic-apm-node').start({ serviceName: '<SERVICE_NAME>', // Specify your service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'] }) } catch (e) { console.log(e); } Copy Attach apm object to globals – This is required so we can use apm variable in other files as part of global sails object. module.exports.globals = { _: require('@sailshq/lodash'), async: false, models: true, sails: true, apm : apm, logger: logger }; Copy Also add middleware in http.js file present in config folder. Which allows to instrument our code. module.exports.http = { middleware: { order: [ 'elasticAPM' ], elasticAPM: (function () { return function (err, req, res, next) { apm.middleware.connect(); if (typeof err !== 'undefined') apm.captureError(err); return next(); }; })() } }; Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in docker-compose.yml or docker stack deployment file or at command line when using docker run command for deployment. Eg: Docker-compose and stack: https://docs.docker.com/compose/environment-variables/ Docker run cli command: docker run -d -t -i -e SF_PROJECT_NAME='<SF_PROJECT_NAME>' \\ -e SF_APP_NAME='<SF_APP_NAME>' \\ -e SF_PROFILE_KEY='<snappyflow profile key>' \\ --name <container_name> <dockerhub_id/image_name> Copy Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/RefappNodeSail "},{"title":"ECS","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#ecs-1","content":""},{"title":"Node.JS Express","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#nodejs-express-3","content":"Install nodejs dependencies and save it in package.json using npm install --save elastic-apm-node@^3.20.0 npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries \"elastic-apm-node\": \"^3.20.0\" \"sf-apm-lib\": \"^1.0.2\" Copy And run ‘npm install’ to install dependencies Add initilization code at start of the file in app.js Get Snappyflow trace config using: const Snappyflow = require('sf-apm-lib'); var sfObj = new Snappyflow(); // Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml. // Add below part to manually configure the initialization let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; sfObj.init(profileKey, projectName, appName); // Manual override let sfTraceConfig = sfObj.getTraceConfig(); Copy Initialize apm object using var apm; try { apm = require('elastic-apm-node').start({ serviceName: '<SERVICE_NAME>', // Specify your service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'] }) } catch (e) { console.log(e); } Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in add container section of task definitions. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in Snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to: View dashboard -> Click on Tracing on left side bar -> Click on view transaction -> Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-express "},{"title":"Node.JS Sails","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#nodejs-sails-3","content":"Install nodejs dependencies and save it in package.json using npm install --save elastic-apm-node@^3.20.0 npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries \"elastic-apm-node\": \"^3.20.0\" \"sf-apm-lib\": \"^1.0.2\" Copy And run ‘npm install’ to install dependencies Add initilization code at start of the file in globals.js present in config folder. Get Snappyflow trace config using const Snappyflow = require('sf-apm-lib'); var sfObj = new Snappyflow(); // Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml. // Add below part to manually configure the initialization let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; sfObj.init(profileKey, projectName, appName); // Manual override let sfTraceConfig = sfObj.getTraceConfig(); Copy Initialize apm object using var apm; try { apm = require('elastic-apm-node').start({ serviceName: '<SERVICE_NAME>', // Specify your service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'] }) } catch (e) { console.log(e); } Copy Attach apm object to globals – This is required so we can use apm variable in other files as part of global sails object module.exports.globals = { _: require('@sailshq/lodash'), async: false, models: true, sails: true, apm : apm, logger: logger }; Copy Also add middleware in http.js file present in config folder. Which allows to instrument our code module.exports.http = { middleware: { order: [ 'elasticAPM' ], elasticAPM: (function () { return function (err, req, res, next) { apm.middleware.connect(); if (typeof err !== 'undefined') apm.captureError(err); return next(); }; })() } }; Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in add container section of task definitions. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/RefappNodeSail "},{"title":"AWS Lambda","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#aws-lambda-1","content":"Install dependency libraries in the node_modules directory using the npm install command npm install sf-apm-lib@^1.0.2 npm install elastic-apm-node@^3.20.0 Copy Ref: https://docs.aws.amazon.com/lambda/latest/dg/nodejs-package.html Instrument lambda function to enable tracing Add code outside lambda handler method to get tracing config and create trace client // SnappyFlow Tracing config const Snappyflow = require('sf-apm-lib'); let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; var sfObj = new Snappyflow(); sfObj.init(profileKey, projectName, appName); var apm; try { var sfTraceConfig = sfObj.getTraceConfig(); apm = require('elastic-apm-node').start({ serviceName: '<SERVICE_NAME_CHANGEME>', serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'], captureBody: 'all' }) } catch (e) { console.log(e) } Copy Add custom instrumentation inside lambda handler method Ref: https://www.elastic.co/guide/en/apm/agent/nodejs/current/custom-transactions.html https://www.elastic.co/guide/en/apm/agent/nodejs/current/custom-spans.html // Create custom transaction var trans = apm.startTransaction('lambda handler', 'lambda'); //Create custom span is needed var span = apm.startSpan('parse json'); // your CODE here // End of span if (span) span.end() //Some more code part of the transaction or add more spans here. Don’t RETURN/EXIT //end custom transaction trans.result = 'success'; trans.end(); // RETURN code Copy Deploy the lambda app. Follow README to test sample app Reference app: https://github.com/upendrasahu/aws-lambda-nodejs-tracing-sample Configure Lambda function before trigger/invoke. Add the environment variable SF_PROFILE_KEY and set the value to your profile key copied from SnappyFlow. Add environment variables SF_APP_NAME and SF_PROJECT_NAME with appropriate values. Create this Project and Application in SnappyFlow if not already present. At this point you can trigger lambda function and get tracing data in SnappyFlow. "},{"title":"Tracing Python Applications","type":0,"sectionRef":"#","url":"docs/Tracing/python","content":"","keywords":""},{"title":"Instances","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#instances-1","content":""},{"title":"Django","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#django","content":"Add sf-elastic-apm==6.3.4sf-apm-lib==0.1.1 Copy entries in requirements.txt file and install these in your project environment or Install through CLI using pip install sf-elastic-apm==6.3.4 pip install sf-apm-lib==0.1.1 Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variable. Add following entries in settings.py Add import statement from sf_apm_lib.snappyflow import Snappyflow Copy Add following entry in INSTALLED_APPS 'elasticapm.contrib.django' Copy Add following entry in MIDDLEWARE 'elasticapm.contrib.django.middleware.TracingMiddleware' Copy Add this entry for instrumenting Django app try: sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = os.getenv('SF_PROJECT_NAME') SF_APP_NAME = os.getenv('SF_APP_NAME') profile_key = os.getenv('SF_PROFILE_KEY') sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() ELASTIC_APM={ 'SERVICE_NAME': \"<Service name>\" , # Specify your service name for tracing 'SERVER_URL': SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), 'GLOBAL_LABELS': SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), 'VERIFY_SERVER_CERT': SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT'), 'SPAN_FRAMES_MIN_DURATION': SFTRACE_CONFIG.get('SFTRACE_SPAN_FRAMES_MIN_DURATION'), 'STACK_TRACE_LIMIT': SFTRACE_CONFIG.get('SFTRACE_STACK_TRACE_LIMIT'), 'CAPTURE_SPAN_STACK_TRACES': SFTRACE_CONFIG.get('SFTRACE_CAPTURE_SPAN_STACK_TRACES'), 'DJANGO_TRANSACTION_NAME_FROM_ROUTE': True, 'CENTRAL_CONFIG': False, 'DEBUG': True } except Exception as error: print(\"Error while fetching snappyflow tracing configurations\", error) Copy Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-django "},{"title":"Flask","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#flask","content":"Add sf-elastic-apm[flask]==6.3.4sf-apm-lib==0.1.1 Copy entries in requirements.txt file and install these in your project environment or Install through CLI using pip install sf-elastic-apm[flask]==6.3.4 pip install sf-apm-lib==0.1.1 Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variable. Add following entries in app.py Add imports statement from elasticapm.contrib.flask import ElasticAPM from sf_apm_lib.snappyflow import Snappyflow Copy Get trace config sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = os.getenv('SF_PROJECT_NAME') SF_APP_NAME = os.getenv('SF_APP_NAME') profile_key = os.getenv('SF_PROFILE_KEY') sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() Copy Initialize elastic apm and instrument it to flask app app.config['ELASTIC_APM'] = { 'SERVICE_NAME': '<SERVICE_NAME>', # Specify your service name for tracing 'SERVER_URL': SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), 'GLOBAL_LABELS': SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), 'VERIFY_SERVER_CERT': SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT'), 'SPAN_FRAMES_MIN_DURATION': SFTRACE_CONFIG.get('SFTRACE_SPAN_FRAMES_MIN_DURATION'), 'STACK_TRACE_LIMIT': SFTRACE_CONFIG.get('SFTRACE_STACK_TRACE_LIMIT'), 'CAPTURE_SPAN_STACK_TRACES': SFTRACE_CONFIG.get('SFTRACE_CAPTURE_SPAN_STACK_TRACES'), 'DEBUG': True } apm = ElasticAPM(app) Copy Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab "},{"title":"Script","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#script","content":"Install following requirements pip install sf-elastic-apm==6.3.4 pip install sf-apm-lib==0.1.1 Copy Add following code at start of script file to setup elastic apm client import elasticapm from sf_apm_lib.snappyflow import Snappyflow sf = Snappyflow() # Initialize Snappyflow. By default intialization will pick profileKey, projectName and appName from sfagent config.yaml. # Add below part to manually configure the initialization SF_PROJECT_NAME = '<Snappyflow Project Name>' SF_APP_NAME = '<Snappyflow App Name>' profile_key = '<Snappyflow Profile Key>' sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration trace_config = sf.get_trace_config() # Returns trace config client = elasticapm.Client( service_name=\"<Service name> \",# Specify service name for tracing server_url=trace_config['SFTRACE_SERVER_URL'], verify_cert=trace_config['SFTRACE_VERIFY_SERVER_CERT'], global_labels=trace_config['SFTRACE_GLOBAL_LABELS'] ) elasticapm.instrument() # Only call this once, as early as possible. Copy Once instrumentation is completed we can create custom transaction and span Example def main(): sess = requests.Session() for url in [ 'https://www.elastic.co', 'https://benchmarks.elastic.co' ]: resp = sess.get(url) time.sleep(1) client.begin_transaction(transaction_type=\"script\") main() # Record an exception try: 1/0 except ZeroDivisionError: ident = client.capture_exception() print (\"Exception caught; reference is %s\" % ident) client.end_transaction(name=__name__, result=\"success\") Copy Refer link to know more: https://www.elastic.co/guide/en/apm/agent/python/master/instrumenting-custom-code.html Now run you script and test your trace in snappyflow server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -> Click on Tracing on left side bar -> Click on view transaction -> Go to real time tab Refer complete script: https://github.com/snappyflow/tracing-reference-apps/blob/master/refapp-django/python_script_trace.py "},{"title":"Celery","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#celery","content":"Install following requirements (Following example is based on redis broker) pip install sf-elastic-apm==6.3.4 pip install redis pip install sf-apm-lib==0.1.1 Copy Add following code at start of the file where celery app is initialized to setup elastic apm client from sf_apm_lib.snappyflow import Snappyflow from elasticapm import Client, instrument from elasticapm.contrib.celery import register_exception_tracking, register_instrumentation instrument() try: sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = '<SF_PROJECT_NAME>' # Replace with appropriate Snappyflow project name SF_APP_NAME = '<SF_APP_NAME>' # Replace with appropriate Snappyflow app name profile_key = '<SF_PROFILE_KEY>' # Replace Snappyflow Profile key sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() apm_client = Client( service_name= '<Service_Name>', # Specify service name for tracing server_url= SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), global_labels= SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), verify_server_cert= SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT') ) register_exception_tracking(apm_client) register_instrumentation(apm_client) except Exception as error: print(\"Error while fetching snappyflow tracing configurations\", error) Copy Once instrumentation is done and celery worker is running we can see trace for each celery task in Snappyflow server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -> Click on Tracing on left side bar -> Click on view transaction -> Go to real time tab Refer complete code: https://github.com/snappyflow/tracing-reference-apps/blob/master/ref-celery/tasks.py "},{"title":"Kubernetes","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#kubernetes-1","content":""},{"title":"Django","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#django-1","content":"Add sf-elastic-apm==6.3.4sf-apm-lib==0.1.1 Copy entries in requirements.txt file and install these in your project environment or Install through CLI using pip install sf-elastic-apm==6.3.4 pip install sf-apm-lib==0.1.1 Copy Add following entries in settings.py Add import statement from sf_apm_lib.snappyflow import Snappyflow Copy Add following entry in INSTALLED_APPS 'elasticapm.contrib.django' Copy Add following entry in MIDDLEWARE 'elasticapm.contrib.django.middleware.TracingMiddleware' Copy Add this entry for instrumenting Django app try: sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = os.getenv('SF_PROJECT_NAME') SF_APP_NAME = os.getenv('SF_APP_NAME') profile_key = os.getenv('SF_PROFILE_KEY') sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() ELASTIC_APM={ 'SERVICE_NAME': \"<Service name>\" , # Specify your service name for tracing 'SERVER_URL': SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), 'GLOBAL_LABELS': SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), 'VERIFY_SERVER_CERT': SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT'), 'SPAN_FRAMES_MIN_DURATION': SFTRACE_CONFIG.get('SFTRACE_SPAN_FRAMES_MIN_DURATION'), 'STACK_TRACE_LIMIT': SFTRACE_CONFIG.get('SFTRACE_STACK_TRACE_LIMIT'), 'CAPTURE_SPAN_STACK_TRACES': SFTRACE_CONFIG.get('SFTRACE_CAPTURE_SPAN_STACK_TRACES'), 'DJANGO_TRANSACTION_NAME_FROM_ROUTE': True, 'CENTRAL_CONFIG': False, 'DEBUG': True } except Exception as error: print(\"Error while fetching snappyflow tracing configurations\", error) Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in Kubernetes deployment file. https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/ If deploying with helm provide above variables in values.yaml and use them in deployment file of charts. https://phoenixnap.com/kb/helm-environment-variables Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-django "},{"title":"Flask","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#flask-1","content":"Add sf-elastic-apm[flask]==6.3.4sf-apm-lib==0.1.1 Copy entries in requirements.txt file and install these in your project environment or Install through CLI using RUN pip install sf-elastic-apm[flask]==6.3.4 RUN pip install sf-apm-lib==0.1.1 Copy Add following entries in app.py Add imports statement from elasticapm.contrib.flask import ElasticAPM from sf_apm_lib.snappyflow import Snappyflow Copy Get trace config sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = os.getenv('SF_PROJECT_NAME') SF_APP_NAME = os.getenv('SF_APP_NAME') profile_key = os.getenv('SF_PROFILE_KEY') sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() Copy Initialize elastic apm and instrument it to flask app app.config['ELASTIC_APM'] = { 'SERVICE_NAME': '<SERVICE_NAME>', # Specify your service name for tracing 'SERVER_URL': SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), 'GLOBAL_LABELS': SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), 'VERIFY_SERVER_CERT': SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT'), 'SPAN_FRAMES_MIN_DURATION': SFTRACE_CONFIG.get('SFTRACE_SPAN_FRAMES_MIN_DURATION'), 'STACK_TRACE_LIMIT': SFTRACE_CONFIG.get('SFTRACE_STACK_TRACE_LIMIT'), 'CAPTURE_SPAN_STACK_TRACES': SFTRACE_CONFIG.get('SFTRACE_CAPTURE_SPAN_STACK_TRACES'), 'DEBUG': True } apm = ElasticAPM(app) Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in Kubernetes deployment file. https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/ If deploying with helm provide above variables in values.yaml and use them in deployment file of charts. https://phoenixnap.com/kb/helm-environment-variables Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab "},{"title":"Celery","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#celery-1","content":"Install following requirements (Following example is based on redis broker) pip install sf-elastic-apm==6.3.4 pip install redis pip install sf-apm-lib==0.1.1 Copy Add following code at start of the file where celery app is initialized to setup elastic apm client from sf_apm_lib.snappyflow import Snappyflow from elasticapm import Client, instrument from elasticapm.contrib.celery import register_exception_tracking, register_instrumentation instrument() try: sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = '<SF_PROJECT_NAME>' # Replace with appropriate Snappyflow project name SF_APP_NAME = '<SF_APP_NAME>' # Replace with appropriate Snappyflow app name profile_key = '<SF_PROFILE_KEY>' # Replace Snappyflow Profile key sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() apm_client = Client(service_name= '<Service_Name>', # Specify service name for tracing server_url= SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), global_labels= SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), verify_server_cert= SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT') ) register_exception_tracking(apm_client) register_instrumentation(apm_client) except Exception as error: print(\"Error while fetching snappyflow tracing configurations\", error) Copy Once instrumentation is done and celery worker is running we can see trace for each celery task in Snappyflow server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -> Click on Tracing on left side bar -> Click on view transaction -> Go to real time tab Refer complete code: https://github.com/snappyflow/tracing-reference-apps/blob/master/ref-celery/tasks.py "},{"title":"Docker","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#docker-1","content":""},{"title":"Django","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#django-2","content":"Add sf-elastic-apm==6.3.4sf-apm-lib==0.1.1 Copy entries in requirements.txt file and install these in your project environment or Install through CLI using RUN pip install sf-elastic-apm==6.3.4 RUN pip install sf-apm-lib==0.1.1 Copy Add following entries in settings.py Add import statement from sf_apm_lib.snappyflow import Snappyflow Copy Add following entry in INSTALLED_APPS 'elasticapm.contrib.django' Copy Add following entry in MIDDLEWARE 'elasticapm.contrib.django.middleware.TracingMiddleware' Copy Add this entry for instrumenting Django app try: sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = os.getenv('SF_PROJECT_NAME') SF_APP_NAME = os.getenv('SF_APP_NAME') profile_key = os.getenv('SF_PROFILE_KEY') sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() ELASTIC_APM={ 'SERVICE_NAME': \"<Service name>\" , # Specify your service name for tracing 'SERVER_URL': SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), 'GLOBAL_LABELS': SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), 'VERIFY_SERVER_CERT': SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT'), 'SPAN_FRAMES_MIN_DURATION': SFTRACE_CONFIG.get('SFTRACE_SPAN_FRAMES_MIN_DURATION'), 'STACK_TRACE_LIMIT': SFTRACE_CONFIG.get('SFTRACE_STACK_TRACE_LIMIT'), 'CAPTURE_SPAN_STACK_TRACES': SFTRACE_CONFIG.get('SFTRACE_CAPTURE_SPAN_STACK_TRACES'), 'DJANGO_TRANSACTION_NAME_FROM_ROUTE': True, 'CENTRAL_CONFIG': False, 'DEBUG': True } except Exception as error: print(\"Error while fetching snappyflow tracing configurations\", error) Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in docker-compose.yml or docker stack deployment file or at command line when using docker run command for deployment. Eg: Docker-compose and stack: https://docs.docker.com/compose/environment-variables/ Docker RUN: docker run -d -t -i -e SF_PROJECT_NAME='' \\ -e SF_APP_NAME='' \\ -e SF_PROFILE_KEY='' \\ -p 80:80 \\ --link redis:redis \\ --name <container_name> <dockerhub_id/image_name> Copy Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-django "},{"title":"Flask","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#flask-2","content":"Add sf-elastic-apm[flask]==6.3.4sf-apm-lib==0.1.1 Copy entries in requirements.txt file and install these in your project environment or Install through CLI using RUN pip install sf-elastic-apm[flask]==6.3.4 RUN pip install sf-apm-lib==0.1.1 Copy Add following entries in app.py Add imports statement from elasticapm.contrib.flask import ElasticAPM from sf_apm_lib.snappyflow import Snappyflow Copy Get trace config sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = os.getenv('SF_PROJECT_NAME') SF_APP_NAME = os.getenv('SF_APP_NAME') profile_key = os.getenv('SF_PROFILE_KEY') sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() Copy Initialize elastic apm and instrument it to flask app app.config['ELASTIC_APM'] = { 'SERVICE_NAME': '<SERVICE_NAME>', # Specify your service name for tracing 'SERVER_URL': SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), 'GLOBAL_LABELS': SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), 'VERIFY_SERVER_CERT': SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT'), 'SPAN_FRAMES_MIN_DURATION': SFTRACE_CONFIG.get('SFTRACE_SPAN_FRAMES_MIN_DURATION'), 'STACK_TRACE_LIMIT': SFTRACE_CONFIG.get('SFTRACE_STACK_TRACE_LIMIT'), 'CAPTURE_SPAN_STACK_TRACES': SFTRACE_CONFIG.get('SFTRACE_CAPTURE_SPAN_STACK_TRACES'), 'DEBUG': True } apm = ElasticAPM(app) Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in docker-compose.yml or docker stack deployment file or at command line when using docker run command for deployment. Eg: Docker-compose and stack: https://docs.docker.com/compose/environment-variables/ Docker run cli command: docker run -d -t -i -e SF_PROJECT_NAME='<SF_PROJECT_NAME>' \\ -e SF_APP_NAME='<SF_APP_NAME>' \\ -e SF_PROFILE_KEY='<snappyflow profile key>' \\ --name <container_name> <dockerhub_id/image_name> Copy Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab "},{"title":"Celery","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#celery-2","content":"Install following requirements (Following example is based on redis broker) pip install sf-elastic-apm==6.3.4 pip install redis pip install sf-apm-lib==0.1.1 Copy Add following code at start of the file where celery app is initialized to setup elastic apm client from sf_apm_lib.snappyflow import Snappyflow from elasticapm import Client, instrument from elasticapm.contrib.celery import register_exception_tracking, register_instrumentation instrument() try: sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = '<SF_PROJECT_NAME>' # Replace with appropriate Snappyflow project name SF_APP_NAME = '<SF_APP_NAME>' # Replace with appropriate Snappyflow app name profile_key = '<SF_PROFILE_KEY>' # Replace Snappyflow Profile key sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() apm_client = Client( service_name= '<Service_Name>', # Specify service name for tracing server_url= SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), global_labels= SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), verify_server_cert= SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT') ) register_exception_tracking(apm_client) register_instrumentation(apm_client) except Exception as error: print(\"Error while fetching snappyflow tracing configurations\", error) Copy Once instrumentation is done and celery worker is running we can see trace for each celery task in Snappyflow server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -> Click on Tracing on left side bar -> Click on view transaction -> Go to real time tab Refer complete code: https://github.com/snappyflow/tracing-reference-apps/blob/master/ref-celery/tasks.py "},{"title":"ECS","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#ecs-1","content":""},{"title":"Django","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#django-3","content":"Add sf-elastic-apm==6.3.4sf-apm-lib==0.1.1 Copy entries in requirements.txt file and install these in your project environment or Install through CLI using pip install sf-elastic-apm==6.3.4 pip install sf-apm-lib==0.1.1 Copy Add following entries in settings.py Add import statement from sf_apm_lib.snappyflow import Snappyflow Copy Add following entry in INSTALLED_APPS 'elasticapm.contrib.django' Copy Add following entry in MIDDLEWARE: 'elasticapm.contrib.django.middleware.TracingMiddleware' Copy Add this entry for instrumenting Django app try: sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = os.getenv('SF_PROJECT_NAME') SF_APP_NAME = os.getenv('SF_APP_NAME') profile_key = os.getenv('SF_PROFILE_KEY') sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() ELASTIC_APM={ 'SERVICE_NAME': \"<Service name>\" , # Specify your service name for tracing 'SERVER_URL': SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), 'GLOBAL_LABELS': SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), 'VERIFY_SERVER_CERT': SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT'), 'SPAN_FRAMES_MIN_DURATION': SFTRACE_CONFIG.get('SFTRACE_SPAN_FRAMES_MIN_DURATION'), 'STACK_TRACE_LIMIT': SFTRACE_CONFIG.get('SFTRACE_STACK_TRACE_LIMIT'), 'CAPTURE_SPAN_STACK_TRACES': SFTRACE_CONFIG.get('SFTRACE_CAPTURE_SPAN_STACK_TRACES'), 'DJANGO_TRANSACTION_NAME_FROM_ROUTE': True, 'CENTRAL_CONFIG': False, 'DEBUG': True } except Exception as error: print(\"Error while fetching snappyflow tracing configurations\", error) Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in add container section of task definitions. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-django "},{"title":"Flask","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#flask-3","content":"Add sf-elastic-apm[flask]==6.3.4sf-apm-lib==0.1.1 Copy entries in requirements.txt file and install these in your project environment or Install through CLI using RUN pip install sf-elastic-apm[flask]==6.3.4 RUN pip install sf-apm-lib==0.1.1 Copy Add following entries in app.py Add imports statement from elasticapm.contrib.flask import ElasticAPM from sf_apm_lib.snappyflow import Snappyflow Copy Get trace config sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = os.getenv('SF_PROJECT_NAME') SF_APP_NAME = os.getenv('SF_APP_NAME') profile_key = os.getenv('SF_PROFILE_KEY') sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() Copy Initialize elastic apm and instrument it to flask app app.config['ELASTIC_APM'] = { 'SERVICE_NAME': '<SERVICE_NAME>', # Specify your service name for tracing 'SERVER_URL': SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), 'GLOBAL_LABELS': SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), 'VERIFY_SERVER_CERT': SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT'), 'SPAN_FRAMES_MIN_DURATION': SFTRACE_CONFIG.get('SFTRACE_SPAN_FRAMES_MIN_DURATION'), 'STACK_TRACE_LIMIT': SFTRACE_CONFIG.get('SFTRACE_STACK_TRACE_LIMIT'), 'CAPTURE_SPAN_STACK_TRACES': SFTRACE_CONFIG.get('SFTRACE_CAPTURE_SPAN_STACK_TRACES'), 'DEBUG': True } apm = ElasticAPM(app) Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in add container section of task definitions. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab "},{"title":"Celery","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#celery-3","content":"Install following requirements (Following example is based on redis broker) pip install sf-elastic-apm==6.3.4 pip install redis pip install sf-apm-lib==0.1.1 Copy Add following code at start of the file where celery app is initialized to setup elastic apm client from sf_apm_lib.snappyflow import Snappyflow from elasticapm import Client, instrument from elasticapm.contrib.celery import register_exception_tracking, register_instrumentation instrument() try: sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = '<SF_PROJECT_NAME>' # Replace with appropriate Snappyflow project name SF_APP_NAME = '<SF_APP_NAME>' # Replace with appropriate Snappyflow app name profile_key = '<SF_PROFILE_KEY>' # Replace Snappyflow Profile key sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() apm_client = Client( service_name= '<Service_Name>', # Specify service name for tracing server_url= SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), global_labels= SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), verify_server_cert= SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT') ) register_exception_tracking(apm_client) register_instrumentation(apm_client) except Exception as error: print(\"Error while fetching snappyflow tracing configurations\", error) Copy Once instrumentation is done and celery worker is running we can see trace for each celery task in Snappyflow server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -> Click on Tracing on left side bar -> Click on view transaction -> Go to real time tab Refer complete code: https://github.com/snappyflow/tracing-reference-apps/blob/master/ref-celery/tasks.py "},{"title":"AWS Lambda","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#aws-lambda-1","content":""},{"title":"Script","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#script-1","content":"Add these python libraries in requirements.txt file. Follow the AWS lambda doc on adding runtime dependency to lambda function. sf-apm-lib==0.1.1 sf-elastic-apm==6.3.4 Copy Ref: https://docs.aws.amazon.com/lambda/latest/dg/python-package-create.html#python-package-create-with-dependency Instrument lambda function to enable tracing. Import Libraries import elasticapm from sf_apm_lib.snappyflow import Snappyflow Copy Add code to get SnappyFlow Trace config, outside lambda handler method. sf = Snappyflow()SF_PROJECT_NAME = os.environ['SF_PROJECT_NAME'] SF_APP_NAME = os.environ['SF_APP_NAME'] profile_key = os.environ['SF_PROFILE_KEY'] sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) trace_config = snappyflow.get_trace_config() Copy Add custom instrumentation in lambda handler function def lambda_handler(event, context): client = elasticapm.Client(service_name=\"<SERVICE_NAME_CHANGEME>\", server_url=trace_config['SFTRACE_SERVER_URL'], verify_cert=trace_config['SFTRACE_VERIFY_SERVER_CERT'], global_labels=trace_config['SFTRACE_GLOBAL_LABELS'] ) elasticapm.instrument() client.begin_transaction(transaction_type=\"script\") # DO SOME WORK. No return statements. client.end_transaction(name=__name__, result=\"success\") # RETURN STATEMENT e.g. return response Copy Deploy the Lambda function. Follow README to test sample app Sample code for reference: https://github.com/upendrasahu/aws-lambda-python-tracing-sample Configure Lambda function before trigger/invoke. Add the environment variable SF_PROFILE_KEY and set the value to your profile key copied from SnappyFlow. Add environment variables SF_APP_NAME and SF_PROJECT_NAME with appropriate values. "}]