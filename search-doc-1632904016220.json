[{"title":"First Blog Post","type":0,"sectionRef":"#","url":"blog/first-blog-post","content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet","keywords":""},{"title":"MDX Blog Post","type":0,"sectionRef":"#","url":"blog/mdx-blog-post","content":"Blog posts support Docusaurus Markdown features, such as MDX. tip Use the power of React to create interactive blog posts. <button onClick={() => alert('button clicked!')}>Click me!</button> Copy Click me!","keywords":""},{"title":"Long Blog Post","type":0,"sectionRef":"#","url":"blog/long-blog-post","content":"This is the summary of a very long blog post, Use a <!-- truncate --> comment to limit blog post size in the list view. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet","keywords":""},{"title":"Getting Started","type":0,"sectionRef":"#","url":"docs/Alerts_notifications/getting_started","content":"","keywords":""},{"title":"Coming Soon...","type":1,"pageTitle":"Getting Started","url":"docs/Alerts_notifications/getting_started#coming-soon","content":""},{"title":"Welcome","type":0,"sectionRef":"#","url":"blog/welcome","content":"Docusaurus blogging features are powered by the blog plugin. Simply add Markdown files (or folders) to the blog directory. Regular blog authors can be added to authors.yml. The blog post date can be extracted from filenames, such as: 2019-05-30-welcome.md2019-05-30-welcome/index.md A blog post folder can be convenient to co-locate blog post images: The blog supports tags as well! And if you don't want a blog: just delete this directory, and use blog: false in your Docusaurus config.","keywords":""},{"title":"SLO","type":0,"sectionRef":"#","url":"docs/Alerts_notifications/slo","content":"","keywords":""},{"title":"Coming Soon...","type":1,"pageTitle":"SLO","url":"docs/Alerts_notifications/slo#coming-soon","content":""},{"title":"dashboard_management","type":0,"sectionRef":"#","url":"docs/Dashboards/dashboard_management","content":"dashboard_management","keywords":""},{"title":"Alert Management","type":0,"sectionRef":"#","url":"docs/Alerts_notifications/alert_management","content":"","keywords":""},{"title":"Coming Soon...","type":1,"pageTitle":"Alert Management","url":"docs/Alerts_notifications/alert_management#coming-soon","content":""},{"title":"Getting Started","type":0,"sectionRef":"#","url":"docs/Dashboards/getting_started","content":"","keywords":""},{"title":"Coming Soon...","type":1,"pageTitle":"Getting Started","url":"docs/Dashboards/getting_started#coming-soon","content":""},{"title":"Overview","type":0,"sectionRef":"#","url":"docs/Integrations/java/overview","content":"Overview JAVA monitoring on SnappyFlow is available for the following platforms Instances Kubernetes","keywords":""},{"title":"Overview","type":0,"sectionRef":"#","url":"docs/Integrations/kubernetes/overview","content":"Overview SnappyFlow provides various approaches to monitor Kubernetes applications. Choose one to continue sfPod# sfPod is a collector that is installed on Kubernetes and runs as a DaemonSet on each worker node. Host, Pod & Container metricsResources such as deployments, Daemon Sets etc.Kubernetes core services metricsCluster logsMonitor Prometheus exporters running on any of the application pods sfKubeAgent# sfKubeAgent is sfAgent packaged as a container and run as a sidecar within a Kubernetes application pod. It can be configured to collect both application metrics and logs similar to the way sfAgent does. Prometheus Integration# Centralized Logging#","keywords":""},{"title":"Monitoring JAVA on Instances","type":0,"sectionRef":"#","url":"docs/Integrations/java/java_instance","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Monitoring JAVA on Instances","url":"docs/Integrations/java/java_instance#overview","content":"JVM on instances is monitored using sfAgent configured with jvm plugin. The plugin monitors JVM metrics, jvm arguments used to start Java process and deadlock metrics. JVM plugin internally uses the following utilities to collect metrics: Jstats for JVM metrics Jolokia will be started by plugin to collect deadlock metrics if monitor Deadlocks parameter is set in configuration file  "},{"title":"Pre-requisites","type":1,"pageTitle":"Monitoring JAVA on Instances","url":"docs/Integrations/java/java_instance#pre-requisites","content":"Jcmd has to be installed in the instance "},{"title":"Configuration","type":1,"pageTitle":"Monitoring JAVA on Instances","url":"docs/Integrations/java/java_instance#configuration","content":"sfAgent section provides steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory - name: jvm enabled: true interval: 60 config: process: * #process that needs to be monitored heapInterval: 3600 # polling interval to collect jvm arguments passed to the process monitorDeadlocks: false #enable/disable deadlock monitoring deadLockMonitoringInterval: 300 #polling interval for deadlock monitoring. Copy "},{"title":"Viewing data and dashboards","type":1,"pageTitle":"Monitoring JAVA on Instances","url":"docs/Integrations/java/java_instance#viewing-data-and-dashboards","content":"Data generated by this plugin can be viewed in “browse data” page inside the respective application under plugin=jvm and documentType=jvm Dashboard for this data can be instantiated by importing dashboard template “JVM” to the application dashboard. "},{"title":"Kubernetes Monitoring with sfPod","type":0,"sectionRef":"#","url":"docs/Integrations/kubernetes/kubernetes_monitoring_with_sfPod","content":"","keywords":""},{"title":"sfPod overview","type":1,"pageTitle":"Kubernetes Monitoring with sfPod","url":"docs/Integrations/kubernetes/kubernetes_monitoring_with_sfPod#sfpod-overview","content":"sfPod is a collector that is installed on Kubernetes and runs as a DaemonSet on each worker node. It monitors the following elements of a Kubernetes environment: Kubernetes metrics Cluster metrics Host metrics Pod metrics Container metrics Events Kubernetes services health– Kubelet, Kube-Proxy, API Server, Controller Manager, Core DNS Kubernetes cluster logs Poll Prometheus Exporters running on application pods Pod Application Logs "},{"title":"sfPod installation","type":1,"pageTitle":"Kubernetes Monitoring with sfPod","url":"docs/Integrations/kubernetes/kubernetes_monitoring_with_sfPod#sfpod-installation","content":"Below is short video explaining the sfPOD installation steps  "},{"title":"Step 1: Create a Cloud profile","type":1,"pageTitle":"Kubernetes Monitoring with sfPod","url":"docs/Integrations/kubernetes/kubernetes_monitoring_with_sfPod#step-1-create-a-cloud-profile","content":"Create a cloud profile in SnappyFlow (or use an existing profile) and copy the profile key to use it while installing the sfPod in your cluster. "},{"title":"Step 2: Add Snappyflow helm chart","type":1,"pageTitle":"Kubernetes Monitoring with sfPod","url":"docs/Integrations/kubernetes/kubernetes_monitoring_with_sfPod#step-2-add-snappyflow-helm-chart","content":"Login to any node that has network connectivity to Kubernetes master node and run the following commands helm repo add snappyflow https://snappyflow.github.io/helm-charts helm repo list helm repo update Copy note <my-cluster-name> Replace with any name, Cluster is discovered by this name on the Snappyflow. <profile key> Replace with key copied from SnappyFlow. "},{"title":"Restricted sfPod Configuration","type":1,"pageTitle":"Kubernetes Monitoring with sfPod","url":"docs/Integrations/kubernetes/kubernetes_monitoring_with_sfPod#restricted-sfpod-configuration","content":"By default, sfPod is installed in a full configuration mode where it monitors all the elements. For a restricted configuration i.e. monitor only cluster logs or cluster metrics, user can use the flags outlined below: --set config.cluster_monitoring=true/false If true monitoring of cluster metrics and cluster logs is enabled. If this field is made false, cluster monitoring is switched off and only Prometheus polling and Centralized Application Log Monitoring are enabled --set config.node_agent.drop_cluster_logs=true => If true, monitoring of Kubernetes cluster logs is disabled. --set config.<doc_type>= false sfPod organizes data collected by plugin/documentType. Example of some of the document types that are collected by sfPod include – pod, node, container, cluster_stats etc. User can disable collection of a documentType using this configuration. The detailed list of document types can be reviewed in Browse Data page of a Kubernetes cluster --set config.app_view By default sfPod sends pod and container metrics of tagged pods (I,e pods that have projectName and appName tags) to both Cluster Index and Project Index leading to duplication of metric data. This feature is enabled to enhance correlation of Application and Infra data. This feature can be switched-off if the flag= true. "},{"title":"Prometheus Exporter","type":0,"sectionRef":"#","url":"docs/Integrations/kubernetes/prometheus_exporter","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Prometheus Exporter","url":"docs/Integrations/kubernetes/prometheus_exporter#overview","content":"sfPod scans all pods in the namespaces that it has access to for specific labels snappyflow/projectname and snappyflow/appname If a pod is tagged with SnappyFlow labels, sfPod then looks for standard Prometheus annotations Label\tValueprometheus.io/scrape\tIf true, the pod is considered for Prometheus scraping, else it is excluded. prometheus.io/port\tThis label defines a list of ports that sfPod will scan. sfPod will also apply the appropriate parser. If this label is empty, sfPod scans all exposed container ports. Default value is empty. prometheus.io/path\tDefine the path as /metrics. Empty by default. If sfPod finds data on these ports, the data is scanned, parsed and sent to SnappyFlow  "},{"title":"Monitoring pods using Prometheus exporter","type":1,"pageTitle":"Prometheus Exporter","url":"docs/Integrations/kubernetes/prometheus_exporter#monitoring-pods-using-prometheus-exporter","content":""},{"title":"Pre-requisites","type":1,"pageTitle":"Prometheus Exporter","url":"docs/Integrations/kubernetes/prometheus_exporter#pre-requisites","content":"Ensure sfPod is running and has access privileges to namespace in which application pod is running Ensure sfPod has access to ports exposing Prometheus exporters  "},{"title":"Enable access to Prometheus exporter","type":1,"pageTitle":"Prometheus Exporter","url":"docs/Integrations/kubernetes/prometheus_exporter#enable-access-to-prometheus-exporter","content":"Add Prometheus exporter container as a sidecar in the application pod. Pls see example below for Prometheus exporter pod. sfPod needs to access the Prometheus exporter on the exported port, which should be exposed in pod’s service PostgreSQL Statefulset YAML PostgreSQL Service YAML After setup of Prometheus exporter container, please verify connectivity using: curl service_ip: 9187 curl service_ip: 5432 Copy "},{"title":"Tag applications with Labels","type":1,"pageTitle":"Prometheus Exporter","url":"docs/Integrations/kubernetes/prometheus_exporter#tag-applications-with-labels","content":"note Applying labels are key to monitoring in SnappyFLow. Endpoints are organized in a hierarchy as per the labels defined. Add labels snappyflow/projectName and snappyflow/appName if the application pods are already running, use the following kubectl commands to tag your application pods with the appropriate tags: kubectl label pods <pod_name> snappyflow/projectname=<project_name> --namespace<appnamespace>kubectl label pods <pod_name> snappyflow/appname=<app_name> --namespace<appnamespace> Copy To automatically apply the right labels for the new pods which get created due to various reasons such as upgrades, restarts etc, apply labels to pod templates. If you are using helm chart, a best practice is to define labels in values.yaml and use the label parameters in pod template section of Deployment, StatefulSet, DaemonSet or other Kubernetes controller. "},{"title":"List of Prometheus exporters supported by sfPod","type":1,"pageTitle":"Prometheus Exporter","url":"docs/Integrations/kubernetes/prometheus_exporter#list-of-prometheus-exporters-supported-by-sfpod","content":"Plugins\tExporter Links\tservice_discovery_regex\tDocker imageapache\tExporter Link\t[\"apache_accesses_total\",\"apache_+\"]\tDocker image elasticsearch\tExporter Link\t[\"elasticsearch_+\"]\tDocker image haproxy\tExporter Link\t[\"haproxy_+\"]\tDocker image jmx\tExporter Link\t[\"jmx_exporter_build_info\",\"jmx_+\",\"java_lang_+\"]\tDocker image kafka-connect-j9\tExporter Link\t[\"kafka_connect+\",\"java_lang_+\",\"java_lang_memorymanager_valid_j9+\"]\tDocker image kafka-connect\tExporter Link\t[\"kafka_connect+\",\"java_lang_+\",\"java_lang_garbagecollector_collectiontime_g1_young_generation\"]\tDocker image kafka-jmx\tExporter Link\t[\"kafka_server_+\",\"kafka_network_+\",\"java_lang_+\"]\tDocker image kafka-rest-j9\tExporter Link\t[\"kafka_rest+\",\"java_lang_+\",\"java_lang_memorymanager_valid_j9+\"]\tDocker image kafka-rest\tExporter Link\t[\"kafka_rest+\",\"java_lang_+\",\"java_lang_garbagecollector_collectiontime_g1_young_generation\"]\tDocker image kafka\tExporter Link\t[\"kafka_topic_+\"]\tDocker image linux\tExporter Link\t[\"node_cpu_+\",\"node_disk_+\",\"node_procs_+\"]\tDocker image mongod\tExporter Link\t[\"mongodb_+\"]\tDocker image mysql\tExporter Link\t[\"mysql_global_+\",\"mysql_version_+\"]\tDocker image nginx\tExporter Link\t[\"nginx_+\"]\tDocker image nodejs\tNo exporter. Using code instrumentation\t[\"nodejs_+\"] postgres\tExporter Link\t\"pg_stat_+\"\tDocker image zookeeper-jmx\tExporter Link\t[\"zookeeper_+\",\"java_lang_\"]\tDocker image "},{"title":"Overview","type":0,"sectionRef":"#","url":"docs/Integrations/nginx/overview","content":"Overview NGINX monitoring on SnappyFlow is available for the following platforms Instances Kubernetes","keywords":""},{"title":"sfKubeAgent Installation","type":0,"sectionRef":"#","url":"docs/Integrations/kubernetes/sfkubeagent_installation","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"sfKubeAgent Installation","url":"docs/Integrations/kubernetes/sfkubeagent_installation#overview","content":"sfKubeAgent is sfAgent packaged as a container and run as a sidecar within a Kubernetes application pod. It can be configured to collect both application metrics and logs similar to the way sfAgent does. "},{"title":"Integrating sfKubeAgent to application pods","type":1,"pageTitle":"sfKubeAgent Installation","url":"docs/Integrations/kubernetes/sfkubeagent_installation#integrating-sfkubeagent-to-application-pods","content":"Instantiate sfKubeAgent docker image in the pod Mount sfKubeAgent config map to the container. Config.yaml file used here is similar to the one used for sfAgent. Configurations for specific applications or log types can be found in Integrations section Pass parameters projectName and appName through container’s yaml file. These are mandatory tags and SnappyFlow uses these tags to organize the end-points in a project/ application hierarchy Mount log paths that need to be monitored to sfKubeAgent container in the correct path "},{"title":"Example:","type":1,"pageTitle":"sfKubeAgent Installation","url":"docs/Integrations/kubernetes/sfkubeagent_installation#example","content":"Below is an example of sfKubeAgent yaml that monitors JVM and Syslog in an application pod. "},{"title":"Pod description YAML","type":1,"pageTitle":"sfKubeAgent Installation","url":"docs/Integrations/kubernetes/sfkubeagent_installation#pod-description-yaml","content":"kind: PodapiVersion: v1metadata: name: jvm-pod labels: snappyflow/appname: test snappyflow/projectname: test-new-1spec: containers: - name: java-container image: ruchira27/jolokia:latest ports: - name: jolokiaport containerPort: 8778 # Snappyflow's sfkubeagent container - name: java-sfagent image: snappyflowml/sfagent:latest imagePullPolicy: Always command: - /app/sfagent - -enable-console-log env: - name: APP_NAME value: test - name: PROJECT_NAME value: test-new-1 volumeMounts: - name: configmap-jmx mountPath: /opt/sfagent/config.yaml subPath: config.yaml - name: varlog mountPath: /var/log volumes: - name: configmap-jmx configMap: name: jmx-configmap - name: varlog hostPath: path: /var/log Copy "},{"title":"Config Map","type":1,"pageTitle":"sfKubeAgent Installation","url":"docs/Integrations/kubernetes/sfkubeagent_installation#config-map","content":"apiVersion: v1kind: ConfigMapmetadata: name: jmx-configmapdata: config.yaml: |- key: Hc0cioeml0Sv7b7MbC+N56DKjygUlcvtP3wLtoUQitk3hw3/SevFv5loicDL9cCJDz3fImeLCuR1MrM/un4z+G2gELVeapNVCh96RhqSDvrV4MV9jMiuGi8RCa8MEj6KzAsvxnBPotbYKiM+11cm0xWOZ7K5G0C6J6T+SLX2/xk9us3BN2MhnBCH1N3xGhlDrNAy7j+KLSKsroiZcDw87iFjSaUzt0ADhCEwEJV3JBLZc2xpSM+n1hm3e4HHnVhaXcOi3Fcb9qD280Ya15t7eTsJywHyhKPcNKXpqF0OGVolLEUDc2vwklHGHIZXHF9hY/+/anS9+VSfhVpBNKVsDb+hDCLJbB8uBivJ9idRcnMvGkhir4kAUcsryCgvpay0ghqKZkjQ7zuhzKYW4/szHoXv+8g/Gn+nnxu3yFAa4aTOq6/AMNCA49S9EmU9Tn2yr+dUhiheWhKWFCTc8jd7vowehcPstNW1t8+SMfERkTqSKo1I/PSG0MGm3vrAa2yfU2GwnsyJnROSF/ylSY5JjTBlmfp7ZozKO8XPc7q+vaMwKEQzcDSqpSE26gOVMxrkYD2ksE/BQPbO2X1YTwlOqHSbr9Z0E5XOJXBSmgT7it7BgBCNro0/YcpALdoyEsJr4FBzM0K4ZwZNpnbDrbs0UIKLISaSGkYGAGBtuEXrusQ= metrics: plugins: - name: jvmjolokia enabled: true interval: 300 config: ip: 127.0.0.1 protocol: http port: 8778 context: jolokia monitorDeadlocks: false deadLockMonitoringInterval: 300 logging: plugins: - name: linux-syslog enabled: true config: log_level: - error - warning - info log_path: /var/log/auth.log,/var/log/messages,/var/log/secure Copy "},{"title":"Monitoring Nginx on Instances","type":0,"sectionRef":"#","url":"docs/Integrations/nginx/nginx_instance","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Monitoring Nginx on Instances","url":"docs/Integrations/nginx/nginx_instance#overview","content":"Nginx monitoring involves monitoring of the following elements: Nginx Access Logs Nginx Error Logs Nginx Server Health "},{"title":"Pre-requisites","type":1,"pageTitle":"Monitoring Nginx on Instances","url":"docs/Integrations/nginx/nginx_instance#pre-requisites","content":"Ensure Nginx access logs are in format expected by sfAgent parser. Edit nginx conf file /etc/nginx/nginx.conf and set log format as follows: '$remote_addr $remote_user [$time_local] ' '\"$request\" $status $body_bytes_sent ' '\"$http_referer\" \"$http_user_agent\" ua=\"$upstream_addr\" ' 'rt=$request_time uct=$upstream_connect_time uht=$upstream_header_time urt=$upstream_response_time'; Copy Sample: log_format snappyflow '$remote_addr $remote_user [$time_local] ' '\"$request\" $status $body_bytes_sent ' '\"$http_referer\" \"$http_user_agent\" ua=\"$upstream_addr\" ' 'rt=$request_time uct=$upstream_connect_time uht=$upstream_header_time urt=$upstream_response_time'; access_log /var/log/nginx/access.log snappyflow buffer=16k flush=5s; Copy After configuring log format, the expected log entry would be: 172.31.72.81 - [01/Jul/2020:03:36:04 +0000] \"POST /owners/6/edit HTTP/1.1\" 504 167 \"-\" \"Apache-HttpClient/4.5.7 (Java/1.8.0_252)\" ua=\"-\" rt=60.004 uct=- uht=- urt=60.004 Copy Description of log fields is as follows: log_format snappyflow '$remote_addr:$remote_port $remote_user .... 'rt=$request_time uct=$upstream_connect_time uht=$upstream_header_time urt=$upstream_response_time rs=$request_length'; Copy Enable Nginx status module: This is required to monitor Nginx server health Open source Nginx exposes several basic metrics about server activity on a simple status page, provided that you have HTTP Stub Status Module enabled. To check if the module is already enabled, run: nginx -V 2>&1 | grep -o with-http_stub_status_module Copy The status module is enabled if you see with-http_stub_status_module as output in the terminal. In order to enable mod_status , you will need to enable the status module. You can use the --with-http_stub_status_module configuration parameter when building Nginx from source: ./configure \\ … \\ --with-http_stub_status_module make sudo make install Copy After verifying the module is enabled, you will also need to modify your Nginx configuration to set up a locally accessible URL (e.g., /stats) for the status page: server { location /stats { stub_status; access_log off; allow 127.0.0.1; deny all; } } Copy note The server blocks of Nginx config are usually found not in the master configuration file (e.g., /etc/nginx/nginx.conf) but in supplemental configuration files that are referenced by the master config. To find the relevant configuration files, first locate the master config by running: nginx -t Open the master configuration file listed, and look for lines that begin with “include” near the end of the http block, e.g.: include /etc/nginx/conf.d/*.conf; In one of the referenced config files you should find the main server block, which you can modify as above to configure Nginx metrics reporting. After changing any configurations, reload the configs by executing: nginx -s reload Now you can view the status page to see your metrics:http://127.0.0.1/stats "},{"title":"Configuration","type":1,"pageTitle":"Monitoring Nginx on Instances","url":"docs/Integrations/nginx/nginx_instance#configuration","content":"sfAgent section provides steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: <profile key> generate_name: true tags: Name: <unique instance name or will be generated from IP> appName: <add application name> projectName: <add project name> metrics: plugins: - name: nginx enabled: true interval: 300 config: port: 80 secure: false location: ‘stats’ logging: plugins: - name: nginx-access enabled: true config: geo_info: true log_path: /var/log/nginx/access.log, /var/log/nginx/access_log ua_parser: true - name: nginx-error enabled: true config: log_level: - emerg - alert - error log_path: /var/log/nginx/error.log, /var/log/nginx/error_log Copy "},{"title":"Viewing data and dashboards","type":1,"pageTitle":"Monitoring Nginx on Instances","url":"docs/Integrations/nginx/nginx_instance#viewing-data-and-dashboards","content":"Data generated by plugin can be viewed in “browse data” page inside the respective application under plugin=jvm and documentType=jvmDashboard for this data can be instantiated by Importing dashboard template JVM to the application dashboard. "},{"title":"SnappyFlow Integrations","type":0,"sectionRef":"#","url":"docs/Integrations/overview","content":"SnappyFlow Integrations SnappyFlow support a wide range of build in integrations to help you get started quickly. MySQL MySQL Infrastructure platform is built for enterprises Postgres Postgres Infrastructure platform is built for enterprises Java Java Infrastructure platform is built for enterprises Oracle Oracle Cloud Infrastructure platform is built for enterprises MongoDB MongoDB Infrastructure platform is built for enterprises Microsoft SQL Server Microsoft Infrastructure platform is built for enterprises Cassandra Cassandra Infrastructure platform is built for enterprises Kafka Kafka Infrastructure platform is built for enterprises Ldap Ldap Infrastructure platform is built for enterprises Kubernetes Clusters Kubernetes clusters Infrastructure platform is built for enterprises MYSQL MYSQL Infrastructure platform is built for enterprises Okta Okta Infrastructure platform is built for enterprises Nginx Nginx Infrastructure platform is built for enterprises Nginx Nginx Infrastructure platform is built for enterprises NodeJS NodeJS Infrastructure platform is built for enterprises OneLogin OneLogin Infrastructure platform is built for enterprises Oozie Oozie Infrastructure platform is built for enterprises Opsgenie Opsgenie Infrastructure platform is built for enterprises Pagerduty Pagerduty Infrastructure platform is built for enterprises Postgres Postgres Infrastructure platform is built for enterprises Prometheus Prometheus Infrastructure platform is built for enterprises Public cloud elbs Public cloud elbs Infrastructure platform is built for enterprises Python Python Infrastructure platform is built for enterprises Redis Redis Infrastructure platform is built for enterprises Spark Spark Infrastructure platform is built for enterprises Stack Stack Infrastructure platform is built for enterprises Teams Teams Infrastructure platform is built for enterprises Saml Saml Infrastructure platform is built for enterprises vCenter vCenter Infrastructure platform is built for enterprises Windows VMs Windows VMs Infrastructure platform is built for enterprises Zenduty Zenduty Infrastructure platform is built for enterprises AWS ECS AWS ECS Infrastructure platform is built for enterprises Teams Teams Infrastructure platform is built for enterprises AWS elastic beanstalk AWS elastic beanstalk Infrastructure platform is built for enterprises C# C# Infrastructure platform is built for enterprises Custom Metrics Custom Metrics Infrastructure platform is built for enterprises Elastic Search Elastic Search Infrastructure platform is built for enterprises Generic ETL Workflows Generic ETL Workflows Infrastructure platform is built for enterprises Generic Webhooks Generic Webhooks Infrastructure platform is built for enterprises Golang Golang Infrastructure platform is built for enterprises Google Authentication Google Authentication Infrastructure platform is built for enterprises HA Proxy HA Proxy Infrastructure platform is built for enterprises IIS Server IIS Server Infrastructure platform is built for enterprises Jaeger Jaeger for Opentracing Infrastructure platform is built for enterprises","keywords":""},{"title":"Monitoring JAVA on Kubernetes","type":0,"sectionRef":"#","url":"docs/Integrations/java/java_kubernetes","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Monitoring JAVA on Kubernetes","url":"docs/Integrations/java/java_kubernetes#overview","content":"Java applications running in Kubernetes can be monitored in SnappyFlow using two approaches: sfKubeAgent as sidecar container. Prometheus exporter  "},{"title":"Java monitoring with sfKubeAgent","type":1,"pageTitle":"Monitoring JAVA on Kubernetes","url":"docs/Integrations/java/java_kubernetes#java-monitoring-with-sfkubeagent","content":"In this option, the Java application should be run with Jolokia agent and sfKubeAgent running as a sidecar container and fetches metrics via Jolokia port. Refer to sfKubeAgent Overview "},{"title":"Prerequisites","type":1,"pageTitle":"Monitoring JAVA on Kubernetes","url":"docs/Integrations/java/java_kubernetes#prerequisites","content":"Copy Jolokia JAR into docker image Run the java application with Jolokia JAR in docker image: -javaagent:/<path_jolokia_jar>/jolokia-jvm-<version>-agent.jar Copy "},{"title":"Configurations","type":1,"pageTitle":"Monitoring JAVA on Kubernetes","url":"docs/Integrations/java/java_kubernetes#configurations","content":"Run sfKubeAgent with JVMJolokia plugin, which is specified using the config map shown below: apiVersion: v1 kind: ConfigMap metadata: name: jvm-configmap data: config.yaml: |- key: <profile_key> metrics: plugins: name: jvmjolokia enabled: true interval: 300 config: ip: 127.0.0.1 protocol: http port: <userDefinedJolokiaPort> context: jolokia monitorDeadlocks: false deadLockMonitoringInterval: 300 Copy The example illustrates instantiating sfKubeAgent with jvm-configmap. sfAKubeAgent talks to the Java application via userDefinedJolokiaPort (this example used 8778) kind: Pod apiVersion: v1 metadata: name: my-first-pod-1 labels: snappyflow/appname: <app_name> snappyflow/projectname: <project_name> spec:containers: name: java-container image: <docker_id>/<docker_image>:<tag> ports: name: jolokiaport containerPort: <userDefinedJolokiaPort> Snappyflow's sfkubeagent container name: java-sfagent image: snappyflowml/sfagent:latest imagePullPolicy: Always command: /app/sfagent -enable-console-log env: name: APP_NAME value: <app_name> name: PROJECT_NAME value: <project_name> volumeMounts: name: configmap-jvm mountPath: /opt/sfagent/config.yaml subPath: config.yaml volumes: name: configmap-jvm configMap: name: jvm-configmap Copy "},{"title":"Viewing data and dashboards","type":1,"pageTitle":"Monitoring JAVA on Kubernetes","url":"docs/Integrations/java/java_kubernetes#viewing-data-and-dashboards","content":"Data generated by plugin can be viewed in browse data page inside the respective application under plugin=jvm_jolokia and documentType=jvm Dashboard for this data can be instantiated by Importing dashboard template JVM to the application dashboard  "},{"title":"Troubleshooting","type":1,"pageTitle":"Monitoring JAVA on Kubernetes","url":"docs/Integrations/java/java_kubernetes#troubleshooting","content":"Check if the Jolokia port is accessible From inside the application container, run a curl command to the userDefinedJolokiaPort. curl http://localhost:<userDefinedJolokiaPort> Copy Check the logs in sfKubeAgent container for any errors "},{"title":"JVM Monitoring with Prometheus exporter","type":1,"pageTitle":"Monitoring JAVA on Kubernetes","url":"docs/Integrations/java/java_kubernetes#jvm-monitoring-with-prometheus-exporter","content":"Refer to Prometheus Exporter Overview. Prometheus exporter is deployed as a sidecar container in the application pod and connects to the JMX target exposed by the application to scrape the metrics. sfPod polls Prometheus exporter to scrape the metrics. "},{"title":"Pre-requisites","type":1,"pageTitle":"Monitoring JAVA on Kubernetes","url":"docs/Integrations/java/java_kubernetes#pre-requisites","content":"sfPod can access Prometheus exporter at Service IP: userDefinedPrometheusPort "},{"title":"Configurations","type":1,"pageTitle":"Monitoring JAVA on Kubernetes","url":"docs/Integrations/java/java_kubernetes#configurations-1","content":"Run Java application with JMX options: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port= <userDefinedJMXPort> -Dcom.sun.management.jmxremote.authenticate=false - Dcom.sun.management.jmxremote.ssl=false Copy Start the Prometheus exporter with java -jar jmx_prometheus_httpserver.jar <userDefinedPrometheusPort> <exporterConfigFile> Copy Configurations are passed using config map: apiVersion: v1 kind: ConfigMap metadata: labels: snappyflow/appname: <app_name> snappyflow/projectname: <project_name> data: jmx-config.yaml: | --- jmxUrl: service:jmx:rmi:///jndi/rmi://127.0.0.1:<userDefinedJMXPort>/jmxrmi ssl: false rules: - pattern: '.*' Copy Prometheus exporter interfaces to JMX via userDefinedJMXPort. Example below uses 555S as the port. Prometheus exporter exposes userDefinedPrometheusPort for scraping. Example uses 5556 as the port Pod definition YAML that illustrates the configuration for Java application and exporter kind: Pod apiVersion: v1 metadata: name: my-first-pod labels: snappyflow/appname: <app_name> snappyflow/projectname: <project_name> spec: containers: name: app-container image: <docker_id>/<docker_image>:<tag> command: sh -c -x java -jar -Dcom.sun.management.jmxremote - Dcom.sun.management.jmxremote.port=<userDefinedJMXPort> - Dcom.sun.management.jmxremote.authenticate=false - Dcom.sun.management.jmxremote.ssl=false <application_jar> name: \"exporter-container\" image: \"bitnami/jmx-exporter:latest\" imagePullPolicy: command: sh -c -x java -jar jmx_prometheus_httpserver.jar <userDefinedPrometheusPort> /tmp/jmx-config.yaml ports: name: exporter-port containerPort: <userDefinedPrometheusPort> volumeMounts: name: configmap-jmx mountPath: /tmp volumes: name: configmap-jmx configMap: name: jmx-configmap Copy "},{"title":"Viewing data and dashboards","type":1,"pageTitle":"Monitoring JAVA on Kubernetes","url":"docs/Integrations/java/java_kubernetes#viewing-data-and-dashboards-1","content":"Data generated by plugin can be viewed in browse data page inside the respective application under plugin=kube-prom-jmx and documentType=jmxStatsDashboard for this data can be instantiated by Importing dashboard template JVM to the application dashboard.  "},{"title":"Troubleshooting","type":1,"pageTitle":"Monitoring JAVA on Kubernetes","url":"docs/Integrations/java/java_kubernetes#troubleshooting-1","content":"Check if the JMX port is accessible .From inside the application container, run a curl command to the userDefinedJMXPort. curl http://localhost:<userDefinedJMXPort> Copy Check if metrics are getting scraped. From inside the exporter container, run a curl command to the userDefinedPrometheusPort curl http://localhost:<userDefinedPrometheusPort>/metrics Copy "},{"title":"Centralized Logging of Application Pod Logs","type":0,"sectionRef":"#","url":"docs/Integrations/kubernetes/centralized_logging_of_application_pod_logs","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Centralized Logging of Application Pod Logs","url":"docs/Integrations/kubernetes/centralized_logging_of_application_pod_logs#overview","content":"SnappyFlow can collect & parse application logs from pods in 2 ways: Collect logs locally by running sfKubeAgent as a sidecar container inside application pod Collect logs centrally through sfPod, which is explained in this page  "},{"title":"Procedure for Centralized Logging","type":1,"pageTitle":"Centralized Logging of Application Pod Logs","url":"docs/Integrations/kubernetes/centralized_logging_of_application_pod_logs#procedure-for-centralized-logging","content":"User runs a busybox sidecar container in the application pod with log files mounted to the container. Busybox tails & streams the application logs to stdout Add SnappyFlow labels: snappyflow/projectName and snappyflow/appName: These are mandatory labels for SnappyFlow monitoring. sfPod collects logs only from pods that have these labels and collected logs are organized under projectName/appName hierarchy in SnappyFlow snappyflow/component: This label is used to signal to sfPod on which parser to apply to parse the logs. List of standard parsers packaged with sfPod. If no label is present, sfPod will apply SnappyFlow’s generic parser which collects the whole log line as a message. sfPod runs as daemon-set in all the Kubernetes data nodes. It picks up logs from stdout of tagged pods, parses the logs based on component tag and ships parsed logs to SnappyFlow under projectName/appName hierarchy. "},{"title":"How to tag application Pods with project and application name labels","type":1,"pageTitle":"Centralized Logging of Application Pod Logs","url":"docs/Integrations/kubernetes/centralized_logging_of_application_pod_logs#how-to-tag-application-pods-with-project-and-application-name-labels","content":""},{"title":"Running Pods","type":1,"pageTitle":"Centralized Logging of Application Pod Logs","url":"docs/Integrations/kubernetes/centralized_logging_of_application_pod_logs#running-pods","content":"Use the following kubectl commands to tag your application pods with the appropriate tags: kubectl label pods <pod_name> snappyflow/projectname=<project_name> --namespace <appnamespace>kubectl label pods <pod_name> snappyflow/appname=<app_name> --namespace <appnamespace> Copy "},{"title":"Automatically apply labels to new Pods","type":1,"pageTitle":"Centralized Logging of Application Pod Logs","url":"docs/Integrations/kubernetes/centralized_logging_of_application_pod_logs#automatically-apply-labels-to-new-pods","content":"To automatically apply right labels for new pods which get created due to various reasons such as upgrade, restarts etc. apply labels to pod templates. If you are using helm chart, best practice is to define labels in values.yaml and use these labels parameter in pod template section of Deployment, StatefulSet, Daemonset or other Kubernetes controller. Below is one example values.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: sfapm-ui labels: app: sfapm role: uispec: replicas: 1 selector: matchLabels: app: sfapm role: ui template: metadata: labels: app: sfapm role: ui snappyflow/appname: demo-application snappyflow/projectname: demo-project Copy "},{"title":"Example","type":1,"pageTitle":"Centralized Logging of Application Pod Logs","url":"docs/Integrations/kubernetes/centralized_logging_of_application_pod_logs#example","content":""},{"title":"Centralized logging for nginx-access logs","type":1,"pageTitle":"Centralized Logging of Application Pod Logs","url":"docs/Integrations/kubernetes/centralized_logging_of_application_pod_logs#centralized-logging-for-nginx-access-logs","content":"Configure Nginx to drop logs in the required format in /var/log/nginx folder using config map Add busy box container to tail logs from access logs and stream to stdout Signal to sfPod to use “nginx” parser using label “component” Nginx Pod YAML kind: Pod apiVersion: v1 metadata: name: my-first-pod labels: snappyflow/appname: <app_name> snappyflow/projectname: <project_name> snappyflow/component: nginx spec: containers: - name: nginx-container image: nginx:latest imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP volumeMounts: - name: varlog mountPath: /var/log/nginx - name: nginx-config mountPath: /etc/nginx/nginx.conf subPath: nginx.conf - name: busybox-container image: busybox command: [\"/bin/sh\", \"-c\"] args: [\"tail -n+1 -f /var/log/nginx/access1.log\"] volumeMounts: - name: varlog mountPath: /var/log/nginx volumes: - name: nginx-config configMap: name: nginx-config - name: varlog emptyDir: {} Copy Config map for Nginx configuration apiVersion: v1 kind: ConfigMap metadata: name: nginx-configmap labels: snappyflow/appname: <app_name> snappyflow/projectname: <project_name> data: nginx.conf: | worker_processes 5; events { worker_connections 4096; } http { default_type application/octet-stream; log_format upstream_time '$remote_addr:$remote_port $remote_user [$time_local] ' '\"$request\" $status $body_bytes_sent ' '\"$http_referer\" \"$http_user_agent\" \"$http_referer\" ' 'rt=$request_time uct=$upstream_connect_time uht=$upstream_header_time urt=$upstream_response_time'; server { listen 80; error_log /var/log/nginx/error1.log; access_log /var/log/nginx/access1.log upstream_time; location /nginx_status { stub_status; } } } Copy "},{"title":"Archival","type":0,"sectionRef":"#","url":"docs/Log_management/archival","content":"","keywords":""},{"title":"Coming Soon!","type":1,"pageTitle":"Archival","url":"docs/Log_management/archival#coming-soon","content":""},{"title":"Monitoring Nginx on Kubernetes","type":0,"sectionRef":"#","url":"docs/Integrations/nginx/nginx_kubernetes","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Monitoring Nginx on Kubernetes","url":"docs/Integrations/nginx/nginx_kubernetes#overview","content":"Nginx monitoring involves monitoring of the following elements: Nginx Access Logs Nginx Error Logs Nginx Server Health "},{"title":"Pre-reading","type":1,"pageTitle":"Monitoring Nginx on Kubernetes","url":"docs/Integrations/nginx/nginx_kubernetes#pre-reading","content":"Refer to the links below for generic approach to monitoring application metrics and logs in Kubernetes environment sfKubeAgent Prometheus Exporter Centralized Log Monitoring  Refer to Nginx monitoring on instances for sfAgent configurations "},{"title":"Configuration","type":1,"pageTitle":"Monitoring Nginx on Kubernetes","url":"docs/Integrations/nginx/nginx_kubernetes#configuration","content":""},{"title":"Configure Nginx server to enable monitoring","type":1,"pageTitle":"Monitoring Nginx on Kubernetes","url":"docs/Integrations/nginx/nginx_kubernetes#configure-nginx-server-to-enable-monitoring","content":"Configure format of access logs so that it can be parsed by SnappyFlow Enable Nginx status module to monitor Nginx server health These configurations can be achieved with the below ConfigMap: apiVersion: v1 kind: ConfigMap metadata: name: nginx-configmap labels: snappyflow/appname: <app_name> snappyflow/projectname: <project_name> data: nginx.conf: | worker_processes 5; events { worker_connections 4096; } http { default_type application/octet-stream; log_format upstream_time '$remote_addr:$remote_port $remote_user [$time_local] ' '\"$request\" $status $body_bytes_sent ' '\"$http_referer\" \"$http_user_agent\" \"$http_referer\" ' 'rt=$request_time uct=$upstream_connect_time uht=$upstream_header_time urt=$upstream_response_time'; server { listen 80; error_log /var/log/nginx/error1.log; access_log /var/log/nginx/access1.log upstream_time; location /nginx_status { stub_status; } } } Copy "},{"title":"sfKubeAgent","type":1,"pageTitle":"Monitoring Nginx on Kubernetes","url":"docs/Integrations/nginx/nginx_kubernetes#sfkubeagent","content":"sfKubeAgent is deployed as sidecar container in the NGINX pod and can be used to monitor Nginx server health as well as Access Logs & Error Logs. Below YAML files provide example for setting up NGINX monitoring with sfKubeAgent. sfKubeAgent ConfigMap (sfAgent-config.yaml) apiVersion: v1 kind: ConfigMap metadata: name: sfagent-configmap labels: snappyflow/appname: <app_name> snappyflow/projectname: <project_name> data: config.yaml: |+ --- key: \"<profile_key>\" metrics: plugins: - name: kube-sfagent-nginx enabled: true interval: 300 config: location: nginx_status port: 80 secure: false logging: plugins: - name: nginx-access enabled: true config: log_path: \"/var/log/nginx/access1.log\" - name: nginx-error enabled: true config: log_path: \"/var/log/nginx/error1.log\" Copy Pod description YAML running NGINX and sfKubeAgent apiVersion: v1 metadata: name: my-first-pod labels: snappyflow/appname: <app_name> snappyflow/projectname: <project_name> spec: containers: - name: nginx-container image: nginx:latest imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP volumeMounts: - name: varlog mountPath: /var/log/nginx - name: nginx-config mountPath: /etc/nginx/nginx.conf subPath: nginx.conf # Snappyflow's sfkubeagent container - name: nginx-sfagent image: snappyflowml/sfagent:latest imagePullPolicy: Always command: - /app/sfagent - -enable-console-log env: - name: APP_NAME value: <app_name> - name: PROJECT_NAME value: <project_name> volumeMounts: - name: nginx-sfagent-config mountPath: /opt/sfagent/config.yaml subPath: config.yaml - name: varlog mountPath: /var/log/nginx volumes: - name: nginx-sfagent-config configMap: name: sfagent-configmap - name: nginx-config configMap: name: nginx-configmap - name: varlog emptyDir: {} Copy "},{"title":"Centralized logging","type":1,"pageTitle":"Monitoring Nginx on Kubernetes","url":"docs/Integrations/nginx/nginx_kubernetes#centralized-logging","content":"Log monitoring (both access and error logs) can be implemented through Centralized Logging approach as well which does not require sfKubeAgent. Centralized logging however requires running a busybox container as a sidecar container to stream logs to container’s stdout. Add the label - snappyflow/component: nginx, which signals to apply Nginx to container’s stdout. "},{"title":"Viewing data and dashboards","type":1,"pageTitle":"Monitoring Nginx on Kubernetes","url":"docs/Integrations/nginx/nginx_kubernetes#viewing-data-and-dashboards","content":"Data generated by plugin can be viewed in browse data page inside the respective application under plugin=jvm and documentType=jvm Dashboard for this data can be instantiated by Importing dashboard template JVM to the application dashboard. "},{"title":"Log Signatures","type":0,"sectionRef":"#","url":"docs/Log_management/log_signatures","content":"","keywords":""},{"title":"Coming Soon!","type":1,"pageTitle":"Log Signatures","url":"docs/Log_management/log_signatures#coming-soon","content":""},{"title":"Analyzing ETL Jobs with SnappyFlow","type":0,"sectionRef":"#","url":"docs/Log_management/etl_jobs","content":"","keywords":""},{"title":"Step 1: Drop logs from ETL Jobs","type":1,"pageTitle":"Analyzing ETL Jobs with SnappyFlow","url":"docs/Log_management/etl_jobs#step-1-drop-logs-from-etl-jobs","content":"SnappyFlow allows for a job to have up to a 3-level hierarchy- Job, Stage, Task. Logs in JSON format have to be dropped whenever a job/stage/task is started, completed or terminated. This log can be parsed using SnappyFlow’s ETL parser. Log format for a Job: { \"jobName\": <Job-name>, \"jobId\": <Unique JobId>, \"time\": <Time in epoch milliseconds format> \"type\": \"job\", \"status\": <status: started, success, failed, aborted> } Copy Log format for a Stage:  { \"jobName\": <Job-name>, \"jobId\": <Unique JobId>, \"stageId\": <stageId>, \"stageName\": <stageName> \"time\": <Time in epoch milliseconds format> \"type\": \"stage\", \"status\": <status can be started, success, failed, aborted> } Copy Log format for a Task: { \"jobName\": <Job-name>, \"jobId\": <Unique JobId>, “stageId”: <staged>, “stageName”: <stageName> \"time\": <Time in epoch milliseconds format> \"type\": \"task\", \"status\": <status can be started, success, failed, aborted>} Copy "},{"title":"Step 2: Forward logs to SnappyFlow","type":1,"pageTitle":"Analyzing ETL Jobs with SnappyFlow","url":"docs/Log_management/etl_jobs#step-2-forward-logs-to-snappyflow","content":"Add the following log parser to logging section of sfAgent’s config.yaml: logging: plugins: - name: etlRaw enabled: true config: log_path: <log file path> Copy Restart sfAgent with the new configuration. service sfagent restart Copy Check if documents have been received in SnappyFlow. You will find 3 documents under metrics with plugin name as “etlRaw” and documentType as “job”, “stage” and “task” depending on your hierarchy. "},{"title":"Step 3: Generate an access URL for use by summarization module","type":1,"pageTitle":"Analyzing ETL Jobs with SnappyFlow","url":"docs/Log_management/etl_jobs#step-3-generate-an-access-url-for-use-by-summarization-module","content":"Logs shipped to SnappyFlow are in a raw form and they cannot be directly used for reporting and analysis. Therefore user has to export this raw data to a summarization script that transforms the data and sends it back to SnappyFlow into a new document. Import a ETL template into your dashboard. Go to “Scratchpad” pane Click on ‘Export API Endpoint’ option in the component and create component URL for all 3 components for interval, say Last 5 mins.  Click on the ‘API Endpoints’ option for the project to view the API List. Copy the URL’s for the 3 components and the Authentication token. These need to be provided in Step 4  "},{"title":"Step 4: Run summarization script as a cronjob","type":1,"pageTitle":"Analyzing ETL Jobs with SnappyFlow","url":"docs/Log_management/etl_jobs#step-4-run-summarization-script-as-a-cronjob","content":"Install the pip utility from the below link. Refer to the link for Installation and Usage instructions. ​ sfapmetl · PyPI The python script takes a config file path as input Set values for key, appName, projectName, Name. Provide the component Url’s for Job, stage and Task and authKey (from Step 3) The data will be available in the dashboard under the plugin ‘etlReport’ and documentType - job, stage and task. "},{"title":"Step 5: Review ETL Dashboards","type":1,"pageTitle":"Analyzing ETL Jobs with SnappyFlow","url":"docs/Log_management/etl_jobs#step-5-review-etl-dashboards","content":"You will now see the summarized data in dashboard under etlReport for job, stage and tasks. Select a particular job and choose a timeline to see job duration trends over the selected time period. Clicking on a particular job id provides a drilled down view of stages and tasks within that job.  "},{"title":"SnappyFlow GO Profiler","type":0,"sectionRef":"#","url":"docs/New_pages/go_profiler","content":"","keywords":""},{"title":"getting started","type":1,"pageTitle":"SnappyFlow GO Profiler","url":"docs/New_pages/go_profiler#getting-started","content":"pre-requisite install and configure snappyflow agent on vm or as a sidecar in the container, as it is required to send data to snappyflow-apm run below command to download or update sf-go-profiler package in your current project. go get -u -v github.com/snappyflow/sf-go-profiler/profiler Copy simple example import \"github.com/snappyflow/sf-go-profiler/profiler\" main(){ profile := profiler.NewProfilerConfig(\"server\") profile.Start() defer profile.Stop() // rest of the application code} Copy profiling can conditionally enabled when required using golang flags import ( \"github.com/snappyflow/sf-go-profiler/profiler\" \"flag\") main(){ enableprofile := flag.Bool(\"profile\",false,\"enable profiler\") if *enableprofile { profile := profiler.NewProfilerConfig(\"server\") // below line disables collection of go runtime metrics // profile.DisableRuntimeMetrics() // below line disables profiling // profile.DisableProfiles() profile.Start() defer profile.Stop() } // rest of the application code} Copy runtime metrics can be disable by calling DisableRuntimeMetrics() similarly profiling can be disabled by calling DisableProfiles() on profile config object.  profile := profiler.NewProfilerConfig(\"server\") // below line disables collection of go runtime metrics profile.DisableRuntimeMetrics() // below line disables profiling profile.DisableProfiles() profile.Start() defer profile.Stop() Copy enable other supported profiles as required  // enable block profile and set given block profile rate profile.EnableBlockProfile(100) // enable mutex profile and set given mutex profile fraction profile.EnableMutexProfile(1000) // enable goroutine profile profile.EnableGoRoutineProfile() // enable threadcreate profile profile.EnableThreadCreateProfile() Copy since only heap and cpu profiles are enabled by default, all supported profiles can be enabled by call to function EnableAllProfiles(), this sets block profile rate to DefaultBlockProfileRate and mutex profile fraction to DefaultMutexProfileFraction  // enable all supported profiles profile.EnableAllProfiles() Copy "},{"title":"Postgres on Instances","type":0,"sectionRef":"#","url":"docs/New_pages/postgres_instances","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Postgres on Instances","url":"docs/New_pages/postgres_instances#overview","content":"PostgreSQL on instances is monitored using sfAgent configured with postgres plugin "},{"title":"Metrics plugin","type":1,"pageTitle":"Postgres on Instances","url":"docs/New_pages/postgres_instances#metrics-plugin","content":"Collects metric data organized in following documentTypes in metrics index: serverDetails databaseDetails tableDetails IndexDetails queryDetails  "},{"title":"Logger plugin","type":1,"pageTitle":"Postgres on Instances","url":"docs/New_pages/postgres_instances#logger-plugin","content":"Collects general logs and slow query logs. General logs are sent to log index under documentType: postgres-general and slow queries logs are parsed and data is sent metrics index in documentType: postgres-slowquery "},{"title":"Pre-requisites","type":1,"pageTitle":"Postgres on Instances","url":"docs/New_pages/postgres_instances#pre-requisites","content":""},{"title":"Enable PostgreSQL general logs","type":1,"pageTitle":"Postgres on Instances","url":"docs/New_pages/postgres_instances#enable-postgresql-general-logs","content":"Logging needs to be configured in the postgresql.conf file. This file can be located by executing the command shown below: postgres=# show config_file; config_file ---------------------------------- /data/pgsql/data/postgresql.conf (1 row) Copy In postgresql.conf file, uncomment and configure the variables shown below:  log_min_messages = warning # set level as appropriate log_line_prefix = '< %m > ' Copy "},{"title":"Enable Slow Query Logs","type":1,"pageTitle":"Postgres on Instances","url":"docs/New_pages/postgres_instances#enable-slow-query-logs","content":"Configuring log_min_duration_statement = 200 will log any query which takes more than 200ms to execute which. Set the value to appropriate value "},{"title":"Set access permissions","type":1,"pageTitle":"Postgres on Instances","url":"docs/New_pages/postgres_instances#set-access-permissions","content":"Username used for DB access should have appropriate permissions grant SELECT ON pg_stat_database to <username>; grant pg_monitor to <username>; Copy note root user has these permissions by default "},{"title":"Configuration","type":1,"pageTitle":"Postgres on Instances","url":"docs/New_pages/postgres_instances#configuration","content":"Refer to sfAgent section for steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: <profile_key> tags: Name: <name> appName: <app_name> projectName: <project_name> metrics: plugins: - name: postgres enabled: true interval: 60 config: documentsTypes: - databaseDetails - indexDetails - queryDetails - serverDetails - tableDetails host: 127.0.0.1 password: <password> port: 5432 user: <username> logging: plugins: - name: postgres-general enabled: true config: log_level: - error - warning - info - log log_path: /var/log/postgresql/postgresql-10-main.log - name: postgres-slowquery enabled: true config: log_path: /var/log/postgresql/postgresql-10-main.log Copy "},{"title":"Viewing data and dashboards","type":1,"pageTitle":"Postgres on Instances","url":"docs/New_pages/postgres_instances#viewing-data-and-dashboards","content":"Data generated by plugin can be viewed in browse data page inside the respective application under plugin=postgres and documentType= serverDetails, databaseDetails, tableDetails, IndexDetails, queryDetails, postgres-slowquery Dashboard for this data can be instantiated by Importing dashboard template PostgreSQL to the application dashboard "},{"title":"MySQL on Instances","type":0,"sectionRef":"#","url":"docs/New_pages/mysql_instances","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"MySQL on Instances","url":"docs/New_pages/mysql_instances#overview","content":"MySQL on instances is monitored using sfAgent configured with MySQL plugin  "},{"title":"Metrics plugin","type":1,"pageTitle":"MySQL on Instances","url":"docs/New_pages/mysql_instances#metrics-plugin","content":"Collects metric data organized in following documentType under metrics index:  serverDetails databaseDetails tableDetails  "},{"title":"Logger plugin","type":1,"pageTitle":"MySQL on Instances","url":"docs/New_pages/mysql_instances#logger-plugin","content":"collects general logs and slow query logs. General logs are sent to log index whereas slow queries are sent to metrics index under documentType:mysqlSlowQueryLogs  "},{"title":"Pre-requisites ","type":1,"pageTitle":"MySQL on Instances","url":"docs/New_pages/mysql_instances#pre-requisites","content":""},{"title":"Enable MySQL configurations","type":1,"pageTitle":"MySQL on Instances","url":"docs/New_pages/mysql_instances#enablemysqlconfigurations","content":"Logging needs to be configured in the mysql.conf.d/mysqld.cnf file. In the configuration file uncomment and configure the variables shown below:  show_compatibility_56 = On #neeeded for metrics log-error=/var/log/mysqld.log pid-file=/var/run/mysqld/mysqld.pid general_log_file=/var/log/mysql/mysql.log general_log=1 Copy This file can be located by executing the command as shown below:  mysqld --verbose --help | grep -A 1 \"Default options\" Copy E.g. output is /etc/my.cnf /etc/mysql/my.cnf ~/my.cnf. User needs to check each of the files for the configuration Alternatively, login to mysql with root user and execute below commands  SET GLOBAL general_log = 'ON'; SET GLOBAL general_log_file= '/path/filename'; Copy "},{"title":"Enable Slow Query Logs  ","type":1,"pageTitle":"MySQL on Instances","url":"docs/New_pages/mysql_instances#enable-slow-query-logs","content":"In mysqld.cnf file, uncomment and configure the variables shown below:  slow_query_log= 1 slow_query_log_file=/var/log/mysql/mysql-slow.log Copy Or, login to mysql with root user and execute below commands  SET GLOBAL slow_query_log = 'ON'; SET GLOBAL long_query_time = 100; SET GLOBAL slow_query_log_file = '/path/filename'; Copy note By Default /var/log/mysql directory is not present in centos, so we must create and provide ownership of that directory as mysql chown -R mysql:mysql /var/log/mysql Copy "},{"title":"Set access permissions","type":1,"pageTitle":"MySQL on Instances","url":"docs/New_pages/mysql_instances#set-access-permissions","content":"Username used for DB access should have appropriate permissions  grant select on information_schema.* to 'username' identified by 'password'; grant select on performance_schema.* to 'username' identified by 'password'; Copy note Root user has these permissions by default  "},{"title":"Configuration ","type":1,"pageTitle":"MySQL on Instances","url":"docs/New_pages/mysql_instances#configuration","content":"Refer to sfAgent section for steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory  metrics: plugins: - name: mysql enabled: true interval: 60 config: documentsTypes: - databaseDetails - serverDetails - tableDetails host: 127.0.0.1 password: USERad@123$ port: 3306 user: root logging: plugins: - name: mysql-error enabled: true config: log_level: - error - warning - note log_path: /var/log/mysql/error.log, /var/log/mysql/mysql-error.log, /var/log/mysqld.err, /var/log/mysqld.log - name: mysql-general enabled: true config: log_path: /var/log/mysql/mysql.log , /var/log/mysql.log, /var/log/mysqld.log, /var/lib/mysql/ip-*.log - name: mysql-slowquery enabled: true config: log_path: /var/lib/mysql/ip-*slow.log, /var/log/mysql/mysql-slow.log Copy Viewing data and dashboards  Data generated by plugin can be viewed in browse data page inside the respective application under plugin=mysql and documentType= serverDetails Dashboard for this data can be instantiated by Importing dashboard template MySQL to the application dashboard "},{"title":"Monitoring Windows Instances","type":0,"sectionRef":"#","url":"docs/New_pages/sfagent_windows","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Monitoring Windows Instances","url":"docs/New_pages/sfagent_windows#overview","content":"Monitoring of Windows based application requires installation of a lightweight agent, sfAgent on Windows. sfAgent provides following features: Monitoring of various services based on specified configurationsLog parsing and collectionTrace Java, Python and Golang applications (check out sfTracing for details) "},{"title":"Supported Platforms","type":1,"pageTitle":"Monitoring Windows Instances","url":"docs/New_pages/sfagent_windows#supported-platforms","content":"Windows Server 2012Windows Server 2016Windows Server 2019 "},{"title":"Install sfAgent on Windows","type":1,"pageTitle":"Monitoring Windows Instances","url":"docs/New_pages/sfagent_windows#install-sfagent-on-windows","content":"Download the sfAgent executable from the link below Dowload sfAgent Run sfAgent.exe executable with Administrator privileges and complete the installation "},{"title":"Configure sfAgent on Windows","type":1,"pageTitle":"Monitoring Windows Instances","url":"docs/New_pages/sfagent_windows#configure-sfagent-on-windows","content":"Navigate to sfAgent installed location (C:\\Program Files (x86)\\sfAgent)Open file sample.yamlAdd Key and edit configuration for metrics and loggerSave it and rename sample.yaml as config.yaml "},{"title":"Prerequisites","type":1,"pageTitle":"Monitoring Windows Instances","url":"docs/New_pages/sfagent_windows#prerequisites","content":"Powershell.exe must be available in %PATH environment variableFor winjvm plugin, java should be installed and java path should be set in %PATH environment variable "},{"title":"Run sfAgent service","type":1,"pageTitle":"Monitoring Windows Instances","url":"docs/New_pages/sfagent_windows#run-sfagent-service","content":"Open task manager and service tabSearch for service “sfAgent” and right click on it and click start to start serviceTo stop right click on running service and click stop "},{"title":"Standard Plugins and Log Parsers","type":1,"pageTitle":"Monitoring Windows Instances","url":"docs/New_pages/sfagent_windows#standard-plugins-and-log-parsers","content":"sfAgent for Windows includes plugins and log parsers for a number of standard applications and operating system utilities. Category\tServicesWindows[Windows Server 2012 and above]\tCPU and RAM static and dynamic parameters WindowsWinPSUtil Web Tier\tIIS Server (Server Monitoring, Access & Error Logs) App Tier\tWinJVMApache Tomcat Database andDataflowElements\tMySQLMS-SQL "},{"title":"Monitoring Linux Instances","type":0,"sectionRef":"#","url":"docs/New_pages/sfagent_linux","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Monitoring Linux Instances","url":"docs/New_pages/sfagent_linux#overview","content":"Monitoring of applications running on VM or bare-metal requires installation of a lightweight sfAgent. sfAgent provides following features: Discovery of servicesAuto-recommendation of monitoring configuration based on discovered servicesMonitoring of various services based on specified configurations Log parsing and collectionOrchestration of tracing (check out sfTracing for details) "},{"title":"Supported Platforms","type":1,"pageTitle":"Monitoring Linux Instances","url":"docs/New_pages/sfagent_linux#supported-platforms","content":"ubuntu 18 lts ubuntu 16 lts centos 7 RHEL 7 "},{"title":"Instal sfAgent on Linux","type":1,"pageTitle":"Monitoring Linux Instances","url":"docs/New_pages/sfagent_linux#instal-sfagent-on-linux","content":"Run the following commands to install sfAgent on VMs: wget https://raw.githubusercontent.com/snappyflow/apm-agent/master/install.sh -O install.shchmod +x install.shsudo ./install.sh Copy note sfAgent executes commands such docas iostat or jcmd to fetch metrics. In order to specific path to sfAgent use -p or --include-paths Example: '\"./install.sh -p /opt/jdk1.8.0_211/bin/\"' To install sfAgent on multiple end-points using Ansible playbook, refer the following script at https://github.com/snappyflow/apm-agent "},{"title":"Pre-requisites","type":1,"pageTitle":"Monitoring Linux Instances","url":"docs/New_pages/sfagent_linux#pre-requisites","content":"sfAgent requires certain pre-requisites for monitoring. Common pre-requisites are mentioned below. Further, all pre-requisites and configurations needed for monitoring a specific application are mentioned under Integrations section. For Linux OS monitoring, install iostat sudo apt-get install sysstat Copy or sudo yum install sysstat Copy For JVM monitoring, install java headless service for jcmd & jmap command sudo apt-get install –y openjdk-12-jdk-headless Copy or sudo yum -y install java-1.8.0-openjdk-devel-1.8.0* Copy "},{"title":"Configure sfAgent on Linux","type":1,"pageTitle":"Monitoring Linux Instances","url":"docs/New_pages/sfagent_linux#configure-sfagent-on-linux","content":"sfAgent is configured through its config.yaml file. There are sections for metrics and logs where appropriate plugins with their configurations have to added to these sections. Below is an example: key: <add profile key here> generate_name: true tags: Name: <add name tag> appName: <add application name tag> projectName: <add project name tag> metrics: plugins: - name: linux enabled: true interval: 30 logging: plugins: - name: linux-syslog enabled: true config: log_level: - error - warning - info log_path: /var/log/auth.log,/var/log/messages,/var/log/secure - name: nginx-access enabled: true config: geo_info: true log_path: /var/log/nginx/access.log, /var/log/nginx/access_log ua_parser: true - name: nginx-error enabled: true config: log_level: - emerg - alert - error log_path: /var/log/nginx/error.log, /var/log/nginx/error_log Copy sfAgent can be either configured or manually. In an automatic configuration step, sfAgent discovers services running in a VM and automatically generates a default configuration for monitoring the discovered services. User can further modify the configurations as needed. Detailed configuration for a specific application types are present in Integrations section. Follow the steps below for automatic discovery & configuration  Run following commands to discover services and generate config: sudo su cd /opt/sfagent ./sfagent -generate-config cp config-generated.yaml config.yaml Copy Add the profile key and SnappyFlow tags in the configuration file. Copy profile key from SnappyFlow and update \"key:\" Set values for \"Name:\", \"appName:\", \"projectName:\" under \"tags:\" section Verify configuration and restart sfAgent ./sfagent -check-config service sfagent restart Copy "},{"title":"Upgrade sfAgent on Linux","type":1,"pageTitle":"Monitoring Linux Instances","url":"docs/New_pages/sfagent_linux#upgrade-sfagent-on-linux","content":"Run following commands to upgrade sfAgent: wget https://raw.githubusercontent.com/snappyflow/apm-agent/master/install.sh -O install.sh chmod +x install.sh sudo ./install.sh --upgrade Copy "},{"title":"sfPoller Setup","type":0,"sectionRef":"#","url":"docs/New_pages/sfpoller_setup","content":"sfPoller Setup Coming Soon!","keywords":""},{"title":"getting_start","type":0,"sectionRef":"#","url":"docs/Quick_Start/getting_start","content":"","keywords":""},{"title":"On this page","type":1,"pageTitle":"getting_start","url":"docs/Quick_Start/getting_start##","content":"Setup SaaS AccountSetup On-Prem SaaSImportant terminologies and conceptssfAgentsfPollersfPodsfKubeAgentProfile KeyTagging ApproachLet's Start Monitoring  SnappyFlow is offered in two modes - SaaS and On-Prem Saas Monitored end-points send data securely to SnappyFlow SaaS. SnappyFlow is deployed within user’s cloud account and all data is retained within the account. Features are identical to SaaS. "},{"title":"Setup SaaS Account","type":1,"pageTitle":"getting_start","url":"docs/Quick_Start/getting_start#setup-saas-account","content":"Go to www.snappyflow.io Register for a free trial. A demo account will be created with a pre-configured sample application Request an upgrade to Full Trial by clicking on the link provided in the top bar. You will get an email stating “your trial environment is ready” once SnappyFlow team approves your trial request.  "},{"title":"Setup On-Prem SaaS","type":1,"pageTitle":"getting_start","url":"docs/Quick_Start/getting_start#setup-on-prem-saas","content":"On-Prem SaaS can be deployed within your public cloud accounts using scripts provided by SnappyFlow team Please reach out to support@snappyflow.io. A support engineer will understand your data ingest rates and provide an appropriately sized solution "},{"title":"Important terminologies and concepts","type":1,"pageTitle":"getting_start","url":"docs/Quick_Start/getting_start#important-terminologies-and-concepts","content":"sfAgent sfPoller sfPod sfKubeAgent Profile Key Tagging Approach "},{"title":"sfAgent","type":1,"pageTitle":"getting_start","url":"docs/Quick_Start/getting_start#sfagent","content":"It is a lightweight agent installed on VMs to collect metrics, logs and tracing data. Installation procedures For sfAgent on Linux For sfAgent on Windows "},{"title":"sfPoller","type":1,"pageTitle":"getting_start","url":"docs/Quick_Start/getting_start#sfpoller","content":"Poller appliance is installed within user’s cloud account and can be used to Monitor cloud services such as RDS, ELB, Lamba, ECS, Azure App Service etc. Monitor Databases Perform Synthetic Monitoring of APIs using postman like collections Stream logs from applications to sfPoller, apply parsing rules and forward logs to SnappyFlow.  Procedure for sfPoller setup "},{"title":"sfPod","type":1,"pageTitle":"getting_start","url":"docs/Quick_Start/getting_start#sfpod","content":"Daemon set installed on Kubernetes cluster and monitors the following elements: Host, Pod & Container metrics Resources such as deployments, Daemon Sets etc. Kubernetes core services metrics Cluster logs Monitor Prometheus exporters running on any of the application pods  Procedure for sfPod setup "},{"title":"sfKubeAgent","type":1,"pageTitle":"getting_start","url":"docs/Quick_Start/getting_start#sfkubeagent","content":"sfAgent equivalent and installed as a side-car container within a Kubernetes pod and can be configured to monitor metrics and logs of other containers running on pods. Procedure for setting up sfKubeAgent "},{"title":"Profile Key","type":1,"pageTitle":"getting_start","url":"docs/Quick_Start/getting_start#profile-key","content":"Every user account has a unique system generated profile key. Data sent by collectors to SnappyFlow need to have the correct profile key and tags to be allowed into SnappyFlow. This key has to be copied by the user and pasted into the configuration file of sfAgent or within sfPoller’s UI "},{"title":"Tagging Approach","type":1,"pageTitle":"getting_start","url":"docs/Quick_Start/getting_start#tagging-approach","content":"SnappyFlow mandates that all end-points should be assigned two tags - _tag_projectName and _tag_appName. These tags have to be added to configuration files of sfAgent or within sfPoller’s UI. Pls see the video that explains how end-points should be organized hierarchically in SnappyFlow and how tags should be assigned  "},{"title":"Let's Start Monitoring","type":1,"pageTitle":"getting_start","url":"docs/Quick_Start/getting_start#lets-start-monitoring","content":"Try out one of the simple exercises to familiarize yourself with the product Monitor a Linux instance# Monitor a Kubernetes Cluster# Monitor a Windows instance# Trace an application# "},{"title":"Go","type":0,"sectionRef":"#","url":"docs/Tracing/go","content":"Go Coming Soon!","keywords":""},{"title":"Log Onboarding","type":0,"sectionRef":"#","url":"docs/Log_management/log_overview","content":"","keywords":""},{"title":"Document format in SnappyFlow","type":1,"pageTitle":"Log Onboarding","url":"docs/Log_management/log_overview#document-format-in-snappyflow","content":"SnappyFlow stores all information in JSON format to the datastore. A sample log document is shown below: \"node\": \"ip-172-31-14-187\",\"_plugin\": \"linux-syslog\",\"ident\": \"sshd\",\"_tag_Name\": \"demo-presto-worker-0\",\"level\": \"info\",\"@timestamp\": \"2020-10-15T18:35:13.000000000Z\",\"time\": \"1602786913000\",\"pid\": \"14153\",\"_documentType\": \"syslog\",\"host\": \"ip-172-31-14-187\",\"_tag_uuid\": \"0aa46894f321\",\"_tag_projectName\": \"presto\",\"file\": \"/var/log/auth.log\",\"signatureKey\": \"8276318930445510094\",\"_tag_appName\": \"presto\",\"message\": \"Invalid user ofandino from 152.32.180.15 port 56712\" Copy "},{"title":"Types of search","type":1,"pageTitle":"Log Onboarding","url":"docs/Log_management/log_overview#types-of-search","content":"Searching across all fields and valuesRange queries for numeric dataWildcard searchRegular expression searchLogical operations like AND, OR, NOT to build complex searches SnappyFlow datastore, processes fields which contain string values differently. The string is stored as a list of tokens. Each token is a unique word in the string. For example if the field “message” has a value “user:admin CMD=rm –rf temp PWD=/home/admin PATH=var.log.secure”. Copy This string is converted as a list of tokens as follows: “user:admin CMD rm rf temp PWD home admin PATH var.log.secure”. Copy Note that in the above tokenization, character “:” and character “.” Are treated differently. They are considered as alpha-numeric, for the purpose of tokenization and are retained, if they are preceded and succeeded by alpha-numeric characters between the “:”. A search of string user\\:admin will be successful in the above document. Note that “:” was escaped using “\\” in the search string, as “:” is considered a reserved character. See below for more on reserved characters. Also note, that a search string temp home will also match the above string, as the words temp and home are present in the above string, even though they do not appear in consecutive positions. A phrase search “temp home” (the search string is encapsulated between “), will match only if temp and home appear together in the string and are in the same order. "},{"title":"SnappyFlow Query language operator support","type":1,"pageTitle":"Log Onboarding","url":"docs/Log_management/log_overview#snappyflow-query-language-operator-support","content":"Operator\tDescription\tExample\tExplanation:\tSearch for a value within a field\tlevel:info\tGet all documents where field “level” has value “info” &&\tAND operation\tinfo && ident:sshd\tGet all documents where value “info” is present in any of the fields AND “ident” field has value “sshd” ||\tOR operation\tlevel:warn || level:error\tGet all documents where “level” field has value “warn” or “level” field has value “error” \"\"\tPhrase searches\tmessage: \"Invalid\tGet all documents where “message” field has a phrase “Invalid user”. Note: searches are case insensitive. “Invalid user” will match only if token “Invalid” and token “user” are present in the string in the same order. >\tGreater than\tpid:>14153\tGet all documents where field “pid” has values greater than 14153 <\tLesser than\tpid:<14153\tGet all documents where field “pid” has values less than 14153 >=\tGreater than or equal\tpid:>=14153\tGet all documents where field “pid” has values greater than or equal to 14153 <=\tLesser than or equal\tpid:<14153\tGet all documents where field “pid” has values less than or equal to 14153 ()\tGrouping\t(pid:(>14000 && <=15000) || level:error) && ident:sshd\tGet all documents where field “pid” is in the range 14000 – 14999 OR field “level” has value “error”. From the above search get only those documents where field “ident” has value “sshd” -\tNOT operation\tlevel:-(info || warn)\tGet all documents where field “level” does not contain value “info” or “warn” ?\tSingle character wildcard\t_plugin: sys???\tGet all documents where the field plugin has a word sys followed by 3 characters. *\tZero or more characters wildcard\tmessage: var\tGet all documents where “message” field contains a string var preceded by any characters and succeeded by any characters. For example in the message “user:admin CMD=rm –rf temp PWD=/home/admin PATH=var.log.secure”, var matches var.log.secure //\tPattern searches\tmessage: /[0-9]+.[0-9]+.[0-9]+.[0-9]+/\tGet all documents which contain an IP address pattern. In the sample log document with the message field containing \"Invalid user ofandino from 152.32.180.15 port 56712\" , the regex pattern will match 152.32.180.15. \\ Escape sequence\tmessage: sudo\\:linux\tSome of the special characters need to be escaped if they are part of a search string. Special characters to be escaped are: & | \" = : ( ) [ ] - ? * / \\ exists:\tField name search\texists:pid\tGet all logs Note: Field names are case sensitive i.e. latency: 20 and LATENCY: 20 will give different results. Field values are case insensitive i.e. name: KEVIN and name: kevin will give the same results. Applying range queries i.e. key: >=200 etc. to text fields give unpredictable results. Make sure to apply such queries on numeric fields only. Range queries cannot be used without specifying the field name i.e. >=20 is not a valid query. Wildcards cannot be used in phrase searches i.e. \"*error\" or \"er??r\" is not allowed. Using a wildcard at the beginning of a word e.g. *ing is particularly heavy, because all terms in the index need to be examined, just in case they match. Regex patterns must be enclosed in forward slashes. Any string present between a pair of forward slashes will be treated as a Java regex pattern. Search Regex does not support all regex meta-characters. For details, https://www.elastic.co/guide/en/elasticsearch/reference/current/regexp-syntax.htmlPatterns are anchored by default i.e. they must match an entire Elasticsearch token. "},{"title":"Examples","type":1,"pageTitle":"Log Onboarding","url":"docs/Log_management/log_overview#examples","content":""},{"title":"Basic Search","type":1,"pageTitle":"Log Onboarding","url":"docs/Log_management/log_overview#basic-search","content":"Datastore has following documents { \"pid\": 3245, \"upstream_response_time\": 10, \"URL\": \"https://www.elastic.co/guide/en/elasticsearch/reference\"}{\"pid\": 2445, \"upstream_response_time\": 4, \"URL\": \"https://www.elastic.co/guide/en/machine-learning\" }{\"pid\": 3246, \"upstream_response_time\": 2, \"URL\": \"https://docker-hub/pricing\"}{\"message\": \"docker image built\", \"pid\": 1000} Copy Search Query & Logic\tResult\tResults explainedpid: 3?4?\tMatches documents 1, 3.\tGet all documents with pid field value matching the pattern 3?4? (? matches any character) upstream_response_time:>5 && elasticsearch\tMatches document 1\tGet all documents where field upstream_response_time key has a value greater than 5 AND the string elasticsearch is present in any of the fields. elastic && machine-learning\tNo documents are matched.\tGet all documents where strings elastic AND machine-learning are present in any of the fields. Though string elastic is present in documents 1, 2; it does not appear as a standalone term. This is because, special character “.” is handled differently in tokenization and is tokenized as www.elastic.co. If the search query is modified as www.elastic.co && machine-learning, document 2 will match the search. Alternatively, search elasstic && machine-learning, will also return the same result https docker hub pricing\tMatches documents 3 and 4.\tGet all documents which contain the words https OR docker OR hub or pricing in any order. Matches documents 3 and 4. Document 3 has all the terms and Document 4 has the term docker. If the intent is to search for a document with all the terms in the same order, then the search should be modified to “https docker hub pricing”. Note the phrase is enclosed in double quotes. This search will match only document 3. Also note the words http docker hub pricing are connected with special characters in document 3. But the search is on the tokenized version of the document and hence all special characters are removed. "},{"title":"Logical Operations and wild card usage","type":1,"pageTitle":"Log Onboarding","url":"docs/Log_management/log_overview#logical-operations-and-wild-card-usage","content":"Datastore contains following documents {\"message\": \"Disconnected from 118.24.197.243 port 35662 [preauth]\"}{\"message\": \"Unregistered Authentication Agent for unix-session:7 (system bus name :1.89, object path /org/freedesktop/PolicyKit1/AuthenticationAgent, locale en_IN) (disconnected from bus)\"}{\"responseCode\": \"400\", \"responseMessage\": Null}{\"message\": \"request received from IP1 and redirected to IP2\", \"responseCode\": \"200\"}{\"message\": \"ValueError(…)\"}{\"message\": \"ArithmeticException(…)\"} Copy Examples Search Query:\"disconnected from\" Get all documents that contain the terms disconnected and from. The terms should appear together in the same order in the document. Results and explanation:Matches documents 1 and 2. Notice that in document 1, the word disconnected appears as Disconnected. Since search is always case-insensitive, document 1 is also matched. Search Query:message: (disconnected && from && port) Get all documents that contain the words disconnected and from and port Results and explanation:Matches document 1 Note: words need not appear together and they may appear in any order. Search Query:message: (disconnect* port) Get all documents that contain word starting with disconnect or a word port. Results and explanation:Matches documents 1 and 2. This is interpreted asmessage: (disconnect || port) disconnect matches all terms which start with the word disconnect and have zero or more characters after it i.e. disconnecting, disconnected and disconnect Search Query:message: (disconnected && -port) Get all documents that has term disconnected and does not have the term port Results and explanation:Matches document 2 -(responseCode: 400 || message: (exception || error)) 2 This is a complete negation of the above search i.e. NOT operator is applied to above search Search Query:responseCode: 400 || message: (exception || error) Results and explanation:Matches 3, 5 and 6 exception matches any word that contains the string exception and similarly error. The term ArithmeticException(...) matches exception and ValueError(...) matches error Search Query:-(responseCode: 400 || message: (exception || error)) This search is a total negation of the previous search. Results and explanation:Matches document 4 "},{"title":"Regex Patterns","type":1,"pageTitle":"Log Onboarding","url":"docs/Log_management/log_overview#regex-patterns","content":"Datastore contains following documents {\"message\": \"No identification string for 118.24.197.243\"}{\"message\": \"No identification string for 119:25.200.255\"}{\"message\": \"Received bad request from 119:25.200.255\"}{\"message\": \"pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=203.195.182.3\"}{\"message\": \"Authentication failure for user admin\"} Copy Examples Search Query: message: /[0-9]+.[0-9]+.[0-9]+.[0-9]+/ Copy Get all documents where message field contains an IP address pattern.Results and explanation: Matches documents 1,2,3,4 Search Query: (message: /119.25.[0-9]+.[0-9]+/) Copy Get all documents where message field contains an IP Address pattern with a network address 119.25 Results and explanation: Matches document 2 and 3auth* && failure &&-/[0-9]+.[0-9]+.[0-9]+.[0-9]+/ 5Any key’s value needs to consist of auth*, failure but not an IP i.e. [0-9]+.[0-9]+.[0-9]+.[0- 9]+ Copy Search Query: auth* && failure && -/[0-9]+.[0-9]+.[0-9]+.[0-9]+/ Copy Get all documents where an IP address pattern is NOT present in any of the fields and contains a word starting with auth in any of the fields AND contains the word failure in any of the fields. Results and explanation: Matches documents 5 "},{"title":"C#","type":0,"sectionRef":"#","url":"docs/Tracing/csharp","content":"C#","keywords":""},{"title":"sfAgent Installation in Linux","type":0,"sectionRef":"#","url":"docs/sfAgent_Linux/sfAgent_installation_in_Linux","content":"","keywords":""},{"title":"Supported Platforms","type":1,"pageTitle":"sfAgent Installation in Linux","url":"docs/sfAgent_Linux/sfAgent_installation_in_Linux#supported-platforms","content":"ubuntu 18 lts ubuntu 16 lts centos 7 RHEL 7 "},{"title":"Installation","type":1,"pageTitle":"sfAgent Installation in Linux","url":"docs/sfAgent_Linux/sfAgent_installation_in_Linux#installation","content":"Run the following commands to install sfAgent on VMs: wget https://raw.githubusercontent.com/snappyflow/apm-agent/master/install.sh -O install.shchmod +x install.shsudo ./install.sh Copy note sfAgent executes commands such docas iostat or jcmd to fetch metrics. In order to specific path to sfAgent use -p or --include-paths Example ./install.sh -p /opt/jdk1.8.0_211/bin/ Copy To install sfAgent on multiple end-points using Ansible playbook, refer the following script at https://github.com/snappyflow/apm-agent "},{"title":"Pre-requisites","type":1,"pageTitle":"sfAgent Installation in Linux","url":"docs/sfAgent_Linux/sfAgent_installation_in_Linux#pre-requisites","content":"sfAgent requires certain pre-requisites for monitoring. Common pre-requisites are mentioned below. Further, all pre-requisites and configurations needed for monitoring a specific application are mentioned under Integrations section. For Linux OS monitoring, install iostat sudo apt-get install sysstat Copy or sudo yum install sysstat Copy For JVM monitoring, install java headless service for jcmd & jmap command sudo apt-get install –y openjdk-12-jdk-headless Copy or sudo yum -y install java-1.8.0-openjdk-devel-1.8.0* Copy "},{"title":"Configure sfAgent","type":1,"pageTitle":"sfAgent Installation in Linux","url":"docs/sfAgent_Linux/sfAgent_installation_in_Linux#configure-sfagent","content":"sfAgent is configured through its config.yaml file. There are sections for metrics and logs where appropriate plugins with their configurations have to added to these sections. Below is an example: key: <add profile key here> generate_name: true tags: Name: <add name tag> appName: <add application name tag> projectName: <add project name tag> metrics: plugins: - name: linux enabled: true interval: 30 logging: plugins: - name: linux-syslog enabled: true config: log_level: - error - warning - info log_path: /var/log/auth.log,/var/log/messages,/var/log/secure - name: nginx-access enabled: true config: geo_info: true log_path: /var/log/nginx/access.log, /var/log/nginx/access_log ua_parser: true - name: nginx-error enabled: true config: log_level: - emerg - alert - error log_path: /var/log/nginx/error.log, /var/log/nginx/error_log Copy sfAgent can be either configured or manually. In an automatic configuration step, sfAgent discovers services running in a VM and automatically generates a default configuration for monitoring the discovered services. User can further modify the configurations as needed. Detailed configuration for a specific application types are present in Integrations section. Follow the steps below for automatic discovery & configuration  Run following commands to discover services and generate config: sudo su cd /opt/sfagent ./sfagent -generate-config cp config-generated.yaml config.yaml Copy Add the profile key and SnappyFlow tags in the configuration file. Copy profile key from SnappyFlow and update key: Set values for Name:, appName:, projectName: under tags: section Verify configuration and restart sfAgent ./sfagent -check-config service sfagent restart Copy "},{"title":"Upgrade sfAgent","type":1,"pageTitle":"sfAgent Installation in Linux","url":"docs/sfAgent_Linux/sfAgent_installation_in_Linux#upgrade-sfagent","content":"Run following commands to upgrade sfAgent: wget https://raw.githubusercontent.com/snappyflow/apm-agent/master/install.sh -O install.sh chmod +x install.sh sudo ./install.sh --upgrade Copy "},{"title":"Tracing Java Applications","type":0,"sectionRef":"#","url":"docs/Tracing/java","content":"","keywords":""},{"title":"Available Platforms","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#available-platforms","content":" "},{"title":"Instance","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#instance","content":"Install sfAgent which automatically installs sfTrace agent as well. Link the application with sfTrace Java Agent "},{"title":"Command Line","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#command-line","content":"Use the following arguments while starting your application using java –jar command, in IDE, Maven or Gradle script: java -javaagent:/opt/sfagent/sftrace/java/sftrace-java-agent.jar -Dsftrace.service_name=<my-service> -jar <application jar> Copy Note: If service_name is not provided, an auto discovered service name will be added. Service_name is used to identify and filter the traces related to an application and should be named appropriately to distinctly identify it. Service name must only contain characters from the ASCII alphabet, numbers, dashes, underscores and spaces. Additional features available for Spring Boot Applications# By default, transaction names of unsupported Servlet API-based frameworks are in the form of $method unknown route. To modify this and to report the transactions names in the form of $method $path, use the following in javaagent configuration. This option is applicable only for spring-boot based applications. -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true Copy Normalizing Transaction URLs# If your URLs contain path parameters like /user/$userId, it can lead to an explosion of transaction types. This can be avoided by using URL groups. For example, if the application supports urls like: /owners, /owners/<owner_id>, /owners/<owner_id>/edit, /owners/<owner_id>/pets, Copy then url groups would be configured as: url_groups=/owners/*,/owner/*/edit,/owners/*/pets Copy Example of running java application via command line using these parameters# java -javaagent:/opt/sfagent/sftrace/java/sftrace-java-agent.jar -Dsftrace.service_name=my-service -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true -Delastic.apm.url_groups=/owners/*,/owner/*/edit,/owners/*/pets -jar <application jar> Copy "},{"title":"Apache Tomcat","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#apache-tomcat","content":"Add the agent configuration in setenv.sh. If this file is not present, create the file in below folder <tomcat installation path>/bin Copy Refer to tomcat_setenv.sh for tracing specific configuration that needs to be copied to setenv.sh file. Make the file executable using chmod +x bin/setenv.sh and start the server Add the agent configuration in setenv.sh. If this file is not present, create the file in below folder <tomcat installation path>/bin Copy Refer to tomcat_setenv.sh for tracing specific configuration that needs to be copied to setenv.sh file. Make the file executable using chmod +x bin/setenv.sh and start the server Additional features available for Spring Boot Applications# By default, transaction names of unsupported Servlet API-based frameworks are in the form of $method unknown route. To modify this and to report the transactions names in the form of $method $path, use the following in javaagent configuration. This option is applicable only for spring-boot based applications. -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true Copy Normalizing Transaction URLs# If your URLs contain path parameters like /user/$userId, it can lead to an explosion of transaction types. This can be avoided by using URL groups. For example, if the application supports urls like: /owners, /owners/<owner_id>, /owners/<owner_id>/edit, /owners/<owner_id>/pets, Copy then url groups would be configured as: url_groups=/owners/*,/owner/*/edit,/owners/*/pets Copy Example of running java application via command line using these parameters# java -javaagent:/opt/sfagent/sftrace/java/sftrace-java-agent.jar -Dsftrace.service_name=my-service -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true -Delastic.apm.url_groups=/owners/*,/owner/*/edit,/owners/*/pets -jar <application jar> Copy "},{"title":"JBOSS EAP","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#jboss-eap","content":"Standalone Mode# Add the agent configuration in standalone.conf file and start the server Refer to JBOSS_standalone.conf for tracing specific configuration. Copy from section with “SFTRACE-CONFIG” in comments Domain Mode# Add the agent configuration in domain.xml and start the server Refer to JBOSS_domain.xml for tracing specific configuration. Copy from section with “SFTRACE-CONFIG” in comments After updating the configuration, restart the application. Additional features available for Spring Boot Applications# By default, transaction names of unsupported Servlet API-based frameworks are in the form of $method unknown route. To modify this and to report the transactions names in the form of $method $path, use the following in javaagent configuration. This option is applicable only for spring-boot based applications. -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true Copy Normalizing Transaction URLs# If your URLs contain path parameters like /user/$userId, it can lead to an explosion of transaction types. This can be avoided by using URL groups. For example, if the application supports urls like: /owners, /owners/<owner_id>, /owners/<owner_id>/edit, /owners/<owner_id>/pets, Copy then url groups would be configured as: url_groups=/owners/*,/owner/*/edit,/owners/*/pets Copy Example of running java application via command line using these parameters# java -javaagent:/opt/sfagent/sftrace/java/sftrace-java-agent.jar -Dsftrace.service_name=my-service -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true -Delastic.apm.url_groups=/owners/*,/owner/*/edit,/owners/*/pets -jar <application jar> Copy "},{"title":"Docker","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#docker","content":"Refer to java_Dockerfile. Look at sections with SFTRACE-CONFIG description. Installation steps are provided. copy the trace agent to the container and start the container by attaching the agent to the application. Additionally, user has to add SnappyFlow configurations for profile_key, projectName, appName to the docker file Once updated, build and start the container. "},{"title":"Kubernetes","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#kubernetes","content":"sfTrace is run as an initContainer in the application pod. User can deploy this either using a manifest yaml or a Helm chart. "},{"title":"Example of Manifest yaml","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#example-of-manifest-yaml","content":"java_k8s_standalone_deployment.yaml  "},{"title":"Example of a Helm chart","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#example-of-a-helm-chart","content":"Update values.yaml: Refer to java_k8s_with_helm_chart_values.yaml to configure agent specific properties. Look at sections with SFTRACE-CONFIG description Update deployment.yaml: Refer to java_k8s_with_helm_chart_deployment.yaml to copy trace agent to the container and start the container by attaching the agent. Look at sections with SFTRACE-CONFIG description "},{"title":"ECS","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#ecs","content":""},{"title":"Create the Task definition","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#create-the-task-definition","content":"Open Amazon ECS, in navigation pane, choose task definition and click on Create New Task Definition and select the launch type as EC2 or Fargate, click on Next step. Give the Task definition Name Task Role, choose an IAM role that provides permissions for containers in your task to make calls to AWS APIs on your behalf and Network Mode Click on Add containers. Give a Container name, and give the Image of your Java Application. Set Memory limit and port mappings as per your task requirements. In the environment section, for Entry Point give sh , -c For Command paste the following lines mkdir /sfagent && wget -O /sfagent/sftrace-agent.tar.gzhttps://github.com/snappyflow/apm-agent/releases/download/latest/sftrace-agent.tar.gz && cd /sfagent && tar -xvzf sftrace-agent.tar.gz && java -javaagent:/sfagent/sftrace/java/sftrace-java-agent.jar -jar <your_jar_name> Copy Note:Some EC2 task definitions may be running on host containers that don’t recoginise the wget command in such case, add below lines in the above command, apt update && apt -y upgrade. Add the following Environment Variables:- SFTRACE_PROJECT_NAME <project_name>SFTRACE_APP_NAME <app_name>SFTRACE_SERVICE_NAME <service_name>SFTRACE_PROFILE_KEY <profile_key> Copy The below environment variables are only applicable for springmvc and optional. ELASTIC_APM_DISABLE_INSTRUMENTATIONS spring-mvcELASTIC_APM_USE_PATH_AS_TRANSACTION_NAME \"true\" Copy "},{"title":"Create the Cluster","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#create-the-cluster","content":"In the Navigation pane, select Clusters and click on Create ClusterSelect the template as per your requirementGive a Cluster name and give instance, networking Configurations IAM role as per your task requirements "},{"title":"Create the Service","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java#create-the-service","content":"Click on the Cluster Name you created in the step2Click on Create , Select the Launch type matching to your task definition. Select the Task Definition Name and Version in the Drop down matching to the task definition you created in step 1Give a Service Name and select other requirements as per your task compatibilityClick on next step and start your service "},{"title":"Tracing Java Applications","type":0,"sectionRef":"#","url":"docs/Tracing/java_v2","content":"","keywords":""},{"title":"Available Platforms","type":1,"pageTitle":"Tracing Java Applications","url":"docs/Tracing/java_v2#available-platforms","content":"InstanceKubernetesDockerECSAWS Lambda Tracing JAVA applications on instances is a 2 step process involving installation of SnappyFlow agent on the instance and configuring the app to allow metrics to be collected. Install agentConfigure application for tracing Configuring the app allows the collection of metrics by the SnappyFlow trace agent. There are multiple ways to configure JAVA apps depending on the type of JAVA application. Choose the type below or follow the command line instructions for generic JAVA applications. Command Line# Use the following arguments while starting your application using java –jar command, in IDE, Maven or Gradle script: java -javaagent:/opt/sfagent/sftrace/java/sftrace-java-agent.jar -Dsftrace.service_name=<my-service> -jar <application jar> Copy Note: If service_name is not provided, an auto discovered service name will be added. Service_name is used to identify and filter the traces related to an application and should be named appropriately to distinctly identify it. Service name must only contain characters from the ASCII alphabet, numbers, dashes, underscores and spaces. Additional features available for Spring Boot Applications# By default, transaction names of unsupported Servlet API-based frameworks are in the form of $method unknown route. To modify this and to report the transactions names in the form of $method $path, use the following in javaagent configuration. This option is applicable only for spring-boot based applications. -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true Copy Normalizing Transaction URLs# If your URLs contain path parameters like /user/$userId, it can lead to an explosion of transaction types. This can be avoided by using URL groups. For example, if the application supports urls like: /owners, /owners/<owner_id>, /owners/<owner_id>/edit, /owners/<owner_id>/pets, Copy then url groups would be configured as: url_groups=/owners/*,/owner/*/edit,/owners/*/pets Copy Apache Tomcat# Add the agent configuration in setenv.sh. If this file is not present, create the file in below folder <tomcat installation path>/bin Copy Refer to tomcat_setenv.sh for tracing specific configuration that needs to be copied to setenv.sh file. Make the file executable using chmod +x bin/setenv.sh and start the server Additional features available for Spring Boot Applications# By default, transaction names of unsupported Servlet API-based frameworks are in the form of $method unknown route. To modify this and to report the transactions names in the form of $method $path, use the following in javaagent configuration. This option is applicable only for spring-boot based applications. -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true Copy Normalizing Transaction URLs# If your URLs contain path parameters like /user/$userId, it can lead to an explosion of transaction types. This can be avoided by using URL groups. For example, if the application supports urls like: /owners, /owners/<owner_id>, /owners/<owner_id>/edit, /owners/<owner_id>/pets, Copy then url groups would be configured as: url_groups=/owners/*,/owner/*/edit,/owners/*/pets Copy Add the agent configuration in setenv.sh. If this file is not present, create the file in below folder <tomcat installation path>/bin Copy Refer to tomcat_setenv.sh for tracing specific configuration that needs to be copied to setenv.sh file. Make the file executable using chmod +x bin/setenv.sh and start the server JBOSS/EAP# Standalone Mode# Add the agent configuration in standalone.conf file and start the server Refer to JBOSS_standalone.conf for tracing specific configuration. Copy from section with “SFTRACE-CONFIG” in comments Domain Mode# Add the agent configuration in domain.xml and start the server Refer to JBOSS_domain.xml for tracing specific configuration. Copy from section with “SFTRACE-CONFIG” in comments After updating the configuration, restart the application. Additional features available for Spring Boot Applications# By default, transaction names of unsupported Servlet API-based frameworks are in the form of $method unknown route. To modify this and to report the transactions names in the form of $method $path, use the following in javaagent configuration. This option is applicable only for spring-boot based applications. -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true Copy Normalizing Transaction URLs# If your URLs contain path parameters like /user/$userId, it can lead to an explosion of transaction types. This can be avoided by using URL groups. For example, if the application supports urls like: /owners, /owners/<owner_id>, /owners/<owner_id>/edit, /owners/<owner_id>/pets, Copy then url groups would be configured as: url_groups=/owners/*,/owner/*/edit,/owners/*/pets Copy Example of running java application via command line using these parameters# java -javaagent:/opt/sfagent/sftrace/java/sftrace-java-agent.jar -Dsftrace.service_name=my-service -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true -Delastic.apm.url_groups=/owners/*,/owner/*/edit,/owners/*/pets -jar <application jar> Copy "},{"title":"Tracing in SnappyFlow","type":0,"sectionRef":"#","url":"docs/Tracing/overview","content":"","keywords":""},{"title":"Select your language","type":1,"pageTitle":"Tracing in SnappyFlow","url":"docs/Tracing/overview#select-your-language","content":" "},{"title":"Tracing Ruby Applications","type":0,"sectionRef":"#","url":"docs/Tracing/ruby","content":"","keywords":""},{"title":"Instances","type":1,"pageTitle":"Tracing Ruby Applications","url":"docs/Tracing/ruby#instances","content":"sfTrace Ruby Agent automatically instruments applications, based on web frameworks Ruby on Rails and other Rack-compatible applications. It uses the hooks and signals provided by these frameworks to trace the application. Installation steps# Install sfAgent (if not already installed)Confirm that /opt/sfagent/config.yaml is configured with correct profile key and tags. Trace setup for Ruby on Rails Applications# Install sftrace agent by either adding the gem to Gemfile gem 'sftrace-agent' and then execute the command bundle install or install the agent yourself using the command gem install sftrace-agent.Add the agent configuration file in application’s config folder. Refer to application.rb for tracing specific configuration. Search for SFTRACE-CONFIG in sample application.rb  "},{"title":"Kubernetes","type":1,"pageTitle":"Tracing Ruby Applications","url":"docs/Tracing/ruby#kubernetes","content":"Follow the steps below to enable tracing in Ruby on Rails applications running as a Kubernetes pod Follow the first 2 steps in Trace setup for Ruby on Rails Applications to update the application with agent specific configuration. sfTrace ruby agent configuration can be set to the application running in Kubernetes pod. This can be done in 2 ways: Option 1: manifest deployment# Refer to ruby_k8s_manifest_deployment.yaml to copy trace agent configuration to the application container and start the container with trace agent configurations. Search for SFTRACE-CONFIG in sample deployment yaml file Once updated, deploy the pod. Option 2: Deploy using helm chart# Step 1: Update values.yaml Refer to k8s_with_helm_chart_values.yaml to configure agent specific properties. Search for SFTRACE-CONFIG in sample values.yaml file Step 2: Update deployment.yaml Refer to ruby_k8s_with_helm_chart_deployment.yaml to copy trace agent to the application container and start the container using the trace agent configurations. Search for SFTRACE-CONFIG in sample deployment yaml file Once updated, deploy the pod. "},{"title":"Feature Extraction","type":0,"sectionRef":"#","url":"docs/Log_management/feature_extraction","content":"","keywords":""},{"title":"Examples","type":1,"pageTitle":"Feature Extraction","url":"docs/Log_management/feature_extraction#examples","content":"Example 1 EV(message, /\\d+\\.\\d+\\.\\d+\\.\\d+/, string, ip_addr) Copy Extract IP address from field message. {\"message\": \"DHCPACK from 172.31.32.1 (xid=0x381e913e)\",“ip_addr”: “172.31.32.1”}{\"message\": \"DHCPREQUEST on eth0 to 172.31.32.1 port 67 (xid=0x381e913e)\", “ip_addr”:“172.31.32.1”}{\"message\": \"Received disconnect from 167.114.226.137 port 47545:11: [preauth]\", “ip_addr”:“167.114.226.137”}{\"message\": \"Disconnected from 167.71.217.175 port 46180 [preauth]\",“ip_addr”: “167.71.217.175”}{\"message\": \"Received disconnect from 51.91.159.46 port 33914:11: [preauth]\",“ip_addr”:“51.91.159.46”} Copy Example 2 EV(log_msg, /\\d+(.\\d+)*(?=ms)/, float, delay) Copy Extract delay values from the field “log_msg”. Delay value is identified by the pattern (a) one digit string, immediately followed by string “ms” OR (b) two digit strings, each separated by “.” and second digit string is immediately followed by string “ms”. Save extracted value in field named delay. {\" log_msg\": \"Received request from 10.11.100.29 and re-directed to 33.229.79.17 in 10.34ms\", “delay”: 10.34}{\"log_msg\": \"Latency time is 5ms\",“delay”: 5}{\"log_msg\": \"Process 2131 completed in 50s\"} – *nothing is extracted from this message* Copy Example 3 (\"received request from\" && \"re-directed\") with EV(log_msg, /\\d+.\\d+.\\d+.\\d+/, string, -,rd_ip_addr) Extract IP addresses from log_msg field. Skip the first extraction and save the second extraction to field named rd_ip_addr. {\"log_msg\": \"Received request from 10.11.100.29 and re-directed to 33.229.79.17 in 10.34ms\",“rd_ip_addr”: \"33.229.79.17”} Copy "},{"title":"Extract Group","type":1,"pageTitle":"Feature Extraction","url":"docs/Log_management/feature_extraction#extract-group","content":"Extract Group (EG) construct allow users to specify a pattern with multiple groups and extract the value of each group into a separate field. In Extract Value a single pattern contained a single group, but this single group could match multiple values in the field. This concept will be better understood through examples. EG construct uses, following parameters: Parameter\tUsefield\tfield name to which the extraction is applied, in this case message field of the log pattern\tREGEX pattern which identifies the extraction groups. Pattern is enclosed in a pair of “/”. In the REGEX pattern the group to be extracted is enclosed in parenthesis “()”. name:type\ta comma separated list of name:type tuples which specify name of the extracted group and its type. Type can be int, float or string. If no type is specified, it is assumed to be string. Example 1 EG(message, /(\\d+.\\d+.\\d+.\\d+) - \\[(.*)\\] \"(\\w+) ([^\\s]+) ([^\\s\"]+)\" (\\d+) (\\d+) \"-\" \"(.*)\" \"-\"/, host, httpd_timestamp, method, path, header, code:int, size:int, agent) Copy The example illustrates extracting information from an nginx access log, which contains the host sending the request, time at which the request was received, HTTP method, request Path, header version, response code, size and the agent making request. The response code, and size are integer values, whereas the other extractions are string type values. Patterns corresponding to these groups are highlighted in the above EG construct. Note: the group REGEX patterns are enclosed in “()”, strings matching each of those patterns are extracted and placed in the field name provided in the same order they appear. {\"message\": \"172.31.31.45 - [06/May/2020:11:44:41] \\\"GET /owners/2 HTTP/1.1\\\" 200 4964\\\"-\\\" \\\"Apache-HttpClient/4.5.7 (Java/1.8.0_242)\\\" \\\"-\\\" rt=14.717 uct=0.000 uht=14.716urt=14.716\",\"host\": “172.31.31.45”,\"httpd_timestamp\": \"06/May/2020:11:44:41\",\"method\": \"GET\",\"path\": \"/owners/2\",\"header\": \"HTTP/1.1\",\"code\": 302,\"size\": 4964,\"agent\": \"Apache-HttpClient/4.5.7 (Java/1.8.0_242)\"}{\"message\": \"172.31.81.81 - [06/May/2020:11:44:41] \\\"POST /owners/new HTTP/1.1\\\" 20124 \\\"-\\\" \\\"Mozilla/5.0 (Windows NT 10.0; WOW64)\\\" \\\"-\\\" rt=1.088 uct=0.000 uht=1.088urt=1.088\",\"host\": “172.31.81.81”, \"httpd_timestamp\": \"06/May/2020:11:44:41\",\"method\": \"POST\",\"path\": \"/owners/new\",\"header\": \"HTTP/1.1\",\"code\": 201,\"size\": 24,\"agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64)\"} Copy Example 2 EG(message, /TTY=\\w+ ; PWD=([^\\s]+) ; USER=(\\w+) ; COMMAND=(.*)$/, path, username, cmd) Copy {\"message\": \"root : TTY=unknown ; PWD=/home/centos ; USER=root ; COMMAND=/bin/rm –rfjmeter.log\",\"pwd\": \"/home/centos\",\"username\": \"root\",\"cmd\": \"/bin/rm -rf jmeter.log\"}{\"message\": \"centos : TTY=unknown ; PWD=/home/kevin ; USER=adam ; COMMAND=/bin/rm -rf jmeter.log \",\"pwd\": \"/home/kevin\",\"username\": \"adam\",\"cmd\": \"/bin/rm -rf jmeter.log\"} Copy "},{"title":"Extract Pair","type":1,"pageTitle":"Feature Extraction","url":"docs/Log_management/feature_extraction#extract-pair","content":"Applications often use logs to dump their internal statistics for debugging purposes. Most often, such logs will contain expressions like: <name1>:<value1>, <name2>:<value2>, ….. Copy OR <name1>=<value1>; <name2>=<value2>;…. Copy In such logs, the field-value pairs can be identified with a value separator and a pair delimiter. In the first example value separator is “:” and pair delimiter is “,”; whereas in the second example, value separator is “=” and pair delimiter is “;”. General syntax used in Extract Pairs is: EP(field, /pair_delimiter/, /value_separator/, convert=[..], include=[..], exclude=[..]) Parameter\tUsefield\tfield name to which the extraction is applied, example: *message* field of the log pair_delimiter\tREGEX pattern which identifies the field-value pair delimiter. Pattern is enclosed in a pair of “/”. value_separator\tREGEX pattern which identifies the separator between field and value. Pattern is enclosed in a pair of “/”. convert=[]\ta list of field:type tuples, which specify how to convert the field values. If nothing is specified, all fields are considered have a string type value. Note: the field names are the names extracted from the log message itself. Example is convert=(field1:int,field2:float). This is an optional field, all values are converted to string type, if not specified. include=[]\tif only selected fields from the extraction are to be added to the log document, list those field names. Note: include and exclude lists can not both be present at the same time. An optional field and can be omitted. exclude=[]\tif selected fields from the extraction are not to be added to the log document, list those field names. Note: include and exclude lists can not both be present at the same time. An optional field and can be omitted. Example 1 EP(message, /,/, /=/, convert=[price:int]) Copy From the field message, extract field-value pairs where pair_delimiter is “,” and value separator is “=”. Convert the value for field “price” to integer. {\"message\": \"name=Kevin,user_id=3212,order_id=234,price=240.56\",\"name\": \"Kevin\", \"user_id\": \"3212\", \"order_id\": \"234\", \"price\": 241}{\"message\": \"name=larry,user_id=1111,order_id=100,price=123\",\"name\": \"larry\",\"user_id\": \"1111\",\"order_id\": \"100\",\"price\": 123}{\"message\": \"process completed in 20s, and details=HEXA3413\",\"and details\": \"HEXA3413\"} Copy Note: in the above example Field “price”, if present is converted to Integer. In the 3 rd message field, “price” is not presentIn the 3 rd message, notice that the field name is taken as “and details”. This is because the pair delimiter was specified as “,” and anything between “,” (pair delimiter) and “=” (value separator) is taken as a field name Example 2 EP(message, /,|(.*,\\s+and)/, /=/, exclude=[name], convert=[price:float, order_id]) Copy From the field message, extract field-value pairs delimited by text matching the pattern /,|(.*,\\s+and)/ and has a value separator “=”. Note that the pair delimiter has a REGEX pattern which defines multiple delimiters. Each delimiter is separated by “|”. Options specified here are:“,” – comma OR “.*,\\s+and’ - a string with any number of characters(.*), followed by a “,” followed by any number of white space characters(\\s+) and followed by the string “and”. Copy Following log messages will illustrate the use of this extraction {\"message\": \"name=Kevin,user_id=3212,order_id=234,price=240.56\",\"user_id\": \"3212\",\"order_id\": 234,\"price\": 240.56}{\"message\": \"name=larry,user_id=1111,order_id=100,price=123\",\"user_id\": \"1111\", \"order_id\": 100,\"price\": 123}{\"message\": \"process completed in 20s, and details=HEXA3413\",\"details\": \"HEXA3413\"} Copy In the 1 st and 2 nd example messages, pair delimiter used was “,”.In the 3 rd example message, the pair delimiter is “process completed in 20s, and”Also note, in messages 1 and 2, the field “name” was extracted, but was not added to the output document, because of the “exclude” option. Example 3 message: (user_id && price) with EP(message, /,/, /=/, include=[price]) Copy {\"message\": \"name=Kevin,user_id=3212,order_id=234,price=240.56\",\"price\": “240.56”}{\"message\": \"name=larry,user_id=1111,order_id=100,price=123\",\"price\": “123”}{\"message\": \"process completed in 20s, and details=HEXA3413\"} Copy Since include option had the field “price”, only those documents where field “price” was found were updated. 3 rd document though matched the delimiter and separator, did not have the field “price”. Example 4 EP(message, /(.*PID.*?(?=\\w+=))|(\\})|(\\{)|(\\s(?=\\w+=))/, /=/, exclude=[ startTime, endTime], convert=[count, minimum: float, mean: float, maximum: float]) Copy Another example using a complex delimiter, with exclude option and convert options. Multiple pair delimiters are used. {\"message\": \"StatisticsLogTask - PID1 - context=Execution Fill {subContext=Order Update section=Top Level startTime=2019-12-17 23:59 endTime=2019-12-20 00:00} count=3minimum=1.0 mean=5.0 maximum=21.0\",\"context\": \"Execution Fill\", \"subContext\": \"Order Update\",\"section\": \"Top Level\", \"count\": 3,\"minimum\": 1,\"maximum\": 21,\"mean\": 5}{\"message\": \"StatisticsLogTask - PID2 - context=Execution Fill {subContext=Order Placed section=Mid Level startTime=2019-12-17 23:59 endTime=2019-12-20 00:00 count=6minimum=0.813 mean=7.9 maximum=13.0}\",\"context\": \"Execution Fill\", \"subContext\": \"Order Placed\",\"section\": \"Mid Level\", \"count\": 6,\"minimum\": 0.813, \"maximum\": 13, \"mean\": 7.9} Copy In the extraction, multiple pair delimiters were specified /(.*PID.*?(?=\\w+=))|(\\})|(\\{)|(\\s(?=\\w+=))/ Copy StatisticsLogTask - PID1 - context=Execution Fill (subContext=Order Update section=Top Level startTime=2019-12-17 23:59 endTime=2019-12-20 00:00} count=3 minimum=1.0 mean=5.0 maximum=21.0 StatisticsLogTask - PID2 - context=Execution Fill (subContext=Order Place section=Mid Level startTime=2019-12-17 23:59 endTime=2019-12-20 00:00} count=6 minimum=0.813 mean=7.3 maximum=13.0 Value pairs identified by the pair delimiter pattern is shown in the above picture. Though startTime and endTime are extracted, but they are excluded. "},{"title":"Tools and Tips","type":1,"pageTitle":"Feature Extraction","url":"docs/Log_management/feature_extraction#tools-and-tips","content":"Java REGEX testing tools can be used to validate the REGEX patterns used for extractions. A web tool like https://www.freeformatter.com/java-regex-tester.html can be used for this. Pattern used in extract values (EV) should match the sub-strings you intend to extract. For example, to extract the timestamps from the following message, a pattern like below can be used /\\d+-\\d+- \\d+\\s+\\d+:\\d+/ Copy Extract Value Pattern \\d+-\\d+-\\d+\\s+\\d+:\\d+ Copy String: StatisticsLogTask - PID1 - context=Execution Fill {subContext=Order Update section=Top Level startTime=2019-12-17 23:59 endTime=2019-12-20 00:00} count=3 minimum=1.0 mean=5.0 maximum=21.0 Copy StatisticsLogTask - PID2 - Context = Execution Fill (subContext=Order Place section=Mid Level startTime=2019-12-17 23:59 endTime=2019-12-20 00:00 count=6 minimum=0.813 mean=7.9 maximum=13.0 Copy Match #\tGroup index\tStart index\tEnd index\tGroup content1\t0\t103\t119\t2019-12-17 23:59 2\t0\t128\t144\t2019-12-20 00:00 Extract Group allows to apply a REGEX to the field and extract only the groups identified (i.e. patterns enclosed in parentheses). As an example the following group REGEX when applied to the string below will result in the following groups. Extract group pattern (\\d+\\.\\d+\\.\\d+\\.\\d+) - - \\[[^\\]]* \\+\\d+\\] \\\\\\\\\\\\\\\"POST \\/topics\\/(\\w+)-(\\w+) HTTP/1.0\\\\\\\\\\\\\\\"(\\d+) (\\d+) (\\d+) Copy String: 10.233.117.0 - - [26/Oct/2020:22:30:00 +0000] \\\\\\\"POST /topics/metric-grqqwwi7 HTTP/1.0\\\\\\\"200 12602 12 (io.confluent.rest-utils.requests) Copy Copy the pattern and string to the java-regex-tester. Observe the resultant matches and groups. In Extract Group, the values extracted in non-zero group index are used. 10.233.177.0 - - [26/Oct/2020:22:30:00 +0000] \\\\\\\"Post /topica/metric-grqqwwi7 HTTP/1.0\\\\\\\" 200 12602 12(io.confluent.reat-utile.requests) Copy Match #\tGroup index\tStart index\tEnd index\tGroup content1\t0\t0\t106\t10.233.117.0--[26/Oct/2000:22:30:00 +0000] \\\\\"POST /topics/metric-grqqwwi7 HTTP/1.0\\\\* 200 1260212 1\t1\t0\t12\t10.233.117.0 1\t2\t63\t69\tmetric 1\t3\t70\t78\tgrqqwwi7 1\t4\t92\t95\t200 1\t5\t96\t101\t12602 1\t6\t103\t106\t12 To extract value pairs, pair-delimiter should match the boundaries and all other symbols which are needed to split the string into an array of pairs. Each pair is again split, based on value- separator pattern SnappyFlow provides following built-in regex patterns, these built-in patterns can be used in place of a REGEX required in EV, EP or EG feature extraction constructs. Built-in pattern names are encapsulated in “$” $hostname$ - hostname string, e.g.apmmanager.snappyflow.io$url$ - URL, e.g. https://apmmanager.snappyflow.io$file_path$ - UNIX file path, e.g. /usr/share/nginx/html/theme-chrome.js$float$ - floating point number, e.g.31.45$integer$ - integer number, e.g. 19345$alphanumeric$ - alpha-numeric characters, e.g. admin1$alphanumericspecial$ - alpha-numeric with hyphen and underscore, e.g. date_time$string$ - a string encapsulated in ‘ (single quote) or “ (double quote)$date$ - date string, e.g. 02-04-2020$datetime$ - date with time string, e.g. 02-04-20 21:41:59$time$ - time string, e.g. 21:41:59$ipv4addr$ - IPv4 Address, e.g. 172.31.22.98 Copy Example extraction with built-in regex patterns message: \"*72281 open() \"/usr/share/nginx/html/theme-chrome.js\" failed (2: No such file or directory), client: 172.31.22.98, server: _, request: \"GET /theme-chrome.js HTTP/1.1\", host: \"apmmanager.snappyflow.io\", referrer: “https://apmmanager.snappyflow.io/” Extractionmessage:\"No such file or directory\" with EG(message,/($integer$) open\\(\\) \\\"($file_path$)\\\" failed \\(2: No such file or directory\\), client: ($ipv4addr$), server: ([^,]*), request: \\\"([^\\\"]*)\\\", host: \\\"($hostname$)\\\", referrer: \\\"($url$)\\\"/,m_size:int, m_file, m_client, m_server, m_request, m_host, m_referrer) The extraction operation above will extract the fields m_size: 72281, m_file: /usr/share/nginx/html/theme-chrome.js, m_client: 172.31.22.98, m_server: _, m_request: GET /theme-chrome.js HTTP/1.1, m_host: apmmanager.snappyflow.io and m_referrer: https://apmmanager.snappyflow.io/ Copy "},{"title":"Postgres on Kubernetes","type":0,"sectionRef":"#","url":"docs/New_pages/postgres_kubernetes","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Postgres on Kubernetes","url":"docs/New_pages/postgres_kubernetes#overview","content":"PostgreSQL running in Kubernetes can be monitored in SnappyFlow using two approaches: sfKubeAgent as sidecar containerPrometheus exporter  "},{"title":"PostgreSQL monitoring with sfKubeAgent","type":1,"pageTitle":"Postgres on Kubernetes","url":"docs/New_pages/postgres_kubernetes#postgresql-monitoring-with-sfkubeagent","content":"sfKubeAgent is run as a sidecar with the configMap shown below. The config map instantiates plugins for metrics, general logs and slow queries. apiVersion: v1 kind: ConfigMap metadata: name: postgres-configmap data: config.yaml: |- key: <profile_key> metrics: plugins: - name: postgres enabled: true interval: 60 config: documentsTypes: #user can enable all or only needed documents - databaseDetails - indexDetails8 - queryDetails - serverDetails - tableDetails host: 127.0.0.1 user: <userName> password: <password> port: 5432 logging: plugins: - name: postgres-general enabled: true config: log_level: - error - warning - info - log log_path: /var/log/postgres/*.log - name: postgres-slowquery enabled: true config: log_path: /var/log/postgres/*.log Copy The example of PostgreSQL pod with Postgres and sfKubeAgent containers is shown below: kind: Pod apiVersion: v1 metadata: name: postgres-pod labels: snappyflow/appname: <app_name> snappyflow/projectname: <project_name> spec: containers: - name: postgres-container securityContext: {} image: \"postgres:9.6\" args: [\"-c\", \"log_statement=all\", \"-c\", \"log_min_messages=warning\", \"-c\", \"log_min_duration_statement=200\", \"-c\",\"log_directory=/var/log/postgres\",\"-c\",\"log_line_prefix=< %m > \",\"-c\",\"log_filename=postgresql-%Y-%m-%d_%H%M%S.log\",\"-c\",\"log_truncate_on_rotation=off\",\"-c\",\"log_rotation_age=1d\",\"-c\",\"logging_collector=on\"] imagePullPolicy: IfNotPresent ports: - name: tcp containerPort: 5432 protocol: TCP env: - name: POSTGRES_PASSWORD value: <password> - name: POSTGRES_USER value: <userName> volumeMounts: - name: varlog mountPath: /var/log/postgres # Snappyflow's sfkubeagent container - name: sfagent-container image: snappyflowml/sfagent:latest imagePullPolicy: Always command: - /app/sfagent - -enable-console-log env: - name: APP_NAME value: <app_name> - name: PROJECT_NAME value: <project_name> volumeMounts: - name: configmap-postgres mountPath: /opt/sfagent/config.yaml subPath: config.yaml - name: varlog mountPath: /var/log/postgres volumes: - name: configmap-postgres configMap: name: postgres-configmap - name: varlog emptyDir: {} Copy "},{"title":"Viewing data and dashboards","type":1,"pageTitle":"Postgres on Kubernetes","url":"docs/New_pages/postgres_kubernetes#viewing-data-and-dashboards","content":"Data generated by plugin can be viewed in “browse data” page inside the respective application under plugin=postgres and documentType= serverDetails, databaseDetails, tableDetails, IndexDetails Dashboard for this data can be instantiated by Importing dashboard template PostgreSQL to the application dashboard  "},{"title":"PostgreSQL monitoring with Prometheus","type":1,"pageTitle":"Postgres on Kubernetes","url":"docs/New_pages/postgres_kubernetes#postgresql-monitoring-with-prometheus","content":"Refer to Prometheus Exporter overview to understand how SnappyFlow monitors using Prometheus exporters. "},{"title":"Pre-requisites","type":1,"pageTitle":"Postgres on Kubernetes","url":"docs/New_pages/postgres_kubernetes#pre-requisites","content":"Prometheus exporter is deployed as a side-car in the application container and the exporter port is accessible to sfPod  "},{"title":"Configurations","type":1,"pageTitle":"Postgres on Kubernetes","url":"docs/New_pages/postgres_kubernetes#configurations","content":"kind: Pod apiVersion: v1 metadata: name: postgres-pod labels: snappyflow/appname: <app_name> snappyflow/projectname: <project_name> snappyflow/component: postgresql spec: containers: - name: postgres-exporter image: bitnami/postgres-exporter ports: - name: pg-exporter containerPort: 9187 command: [\"/bin/sh\", \"-c\"] args: ['DATA_SOURCE_NAME=\"postgresql://<user_name>:<password>@localhost:5432/<dbname>?sslmode=disable\" /opt/bitnami/postgres-exporter/bin/postgres_exporter'] - name: postgres-container securityContext: {} image: \"postgres:9.6\" args: [\"-c\", \"log_statement=all\", \"-c\", \"log_min_messages=warning\", \"-c\", \"log_min_duration_statement=200\", \"-c\",\"log_line_prefix=< %m > \"] imagePullPolicy: IfNotPresent ports: - name: tcp containerPort: 5432 protocol: TCP env: - name: POSTGRES_PASSWORD value: <password> - name: POSTGRES_USER value: <user_name> - name: POSTGRES_DB value: <dbname> Copy "},{"title":"Viewing data and dashboards","type":1,"pageTitle":"Postgres on Kubernetes","url":"docs/New_pages/postgres_kubernetes#viewing-data-and-dashboards-1","content":"Data generated by plugin can be viewed in browse data page inside the respective application under plugin=kube-prom-postgres and documentType= psql Dashboard for this data can be instantiated by Importing dashboard template PostgreSQL_Prom to the application dashboard  "},{"title":"PostgreSQL Pod Centralized Logging","type":1,"pageTitle":"Postgres on Kubernetes","url":"docs/New_pages/postgres_kubernetes#postgresql-pod-centralized-logging","content":"Pls refer to Centralized Logging Overview to understand how SnappyFlow implements centralized logging Centralized logging approach requires the application pod to stream logs to stdout, which is achieved by running a busy box container as shown below. kind: PodapiVersion: v1metadata: name: postgres-pod labels: snappyflow/appname: <app_name> snappyflow/projectname: <project_name> snappyflow/component: postgresqlspec: containers: - name: postgres-exporter image: bitnami/postgres-exporter ports: - name: pg-exporter containerPort: 9187 command: - /bin/sh - '-c' args: - >- DATA_SOURCE_NAME=\"postgresql://<user_name>:<password>@localhost:5432/<dbname>?sslmode=disable\" /opt/bitnami/postgres-exporter/bin/postgres_exporter - name: postgres-container securityContext: {} image: 'postgres:9.6' args: - '-c' - log_statement=all - '-c' - log_min_messages=warning - '-c' - log_min_duration_statement=200 - '-c' - 'log_line_prefix=< %m > ' - '-c' - log_directory=/var/log/postgres - '-c' - log_filename=postgresql.log - '-c' - logging_collector=on imagePullPolicy: IfNotPresent ports: - name: tcp containerPort: 5432 protocol: TCP env: - name: POSTGRES_PASSWORD value: <password> - name: POSTGRES_USER value: <user_name> - name: POSTGRES_DB value: <dbname> volumeMounts: - name: postgres-log mountPath: /var/log/postgres - name: postgres-general image: busybox command: - /bin/sh - '-c' args: - tail -n+1 -f /var/log/postgres/*.log volumeMounts: - name: postgres-log mountPath: /var/log/postgres volumes: - name: postgres-log emptyDir: {} Copy "},{"title":"Tracing node.js applications","type":0,"sectionRef":"#","url":"docs/Tracing/nodejs","content":"","keywords":""},{"title":"Choose your platform","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#choose-your-platform","content":""},{"title":"Instance","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#instance","content":""},{"title":"Node.JS Express","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#nodejs-express","content":"Install nodejs dependencies and save it in package.json using npm install --save elastic-apm-node@^3.20.0npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries \"elastic-apm-node\": \"^3.20.0\"\"sf-apm-lib\": \"^1.0.2\" Copy And run ‘npm install’ to install dependencies Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variable in .env file and load it using require('dotenv').config() and access it in code using process.env.<ENV_VAR> Add initilization code at start of the file Get Snappyflow trace config using const Snappyflow = require('sf-apm-lib');var sfObj = new Snappyflow(); // Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml. // Add below part to manually configure the initialization let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; sfObj.init(profileKey, projectName, appName); // Manual override let sfTraceConfig = sfObj.getTraceConfig(); Copy Initialize apm object using var apm; try { apm = require('elastic-apm-node').start({ serviceName: '<SERVICE_NAME>', // Specify your service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'] }) } catch (e) { console.log(e); } Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in add container section of task definitions. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in Snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to: View dashboard -> Click on Tracing on left side bar -> Click on view transaction -> Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-express "},{"title":"Node.JS Script","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#nodejs-script","content":"Install nodejs dependencies and save it in package.json using npm install --save elastic-apm-node@^3.20.0 npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries: \"elastic-apm-node\": \"^3.20.0\" \"sf-apm-lib\": \"^1.0.2\" Copy And run ‘npm install’ to install dependencies Add initilization code at start of the file Get Snappyflow trace config using const Snappyflow = require('sf-apm-lib'); let projectName = <SF_PROJECT_NAME>; //replace with appropriate project name let appName = <SF_APP_NAME>; //replace with appropriate application name let profileKey = <SF_PROFILE_KEY>; //replace with key copied from SF profile var sfObj = new Snappyflow(); sfObj.init(profileKey, projectName, appName); let sfTraceConfig = sfObj.getTraceConfig(); Copy Initialize apm object using var apm;try { apm = require('elastic-apm-node').start({ serviceName: '<SERVICE_NAME>', // Specify service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'] }) } catch (e) { console.log(e); } Copy Create a custom transaction and span within transaction using following code var trans = apm.startTransaction('json transaction', 'reference-app'); var span = apm.startSpan('parse json'); try { JSON.parse('{\"app\": \"test\"}') } catch (e) { apm.captureError(e); // Capture the error using apm.captureError(e) method.} // when we've processed, stop the custom span if (span) span.end() trans.result = err ? 'error' : 'success'; // end the transaction trans.end(); Copy For more info refer https://www.elastic.co/guide/en/apm/agent/nodejs/current/custom-transactions.html https://www.elastic.co/guide/en/apm/agent/nodejs/current/custom-spans.html Run you script using node <file_name.js> you should see trace data in Snappyflow server. For viewing trace in Snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to: View dashboard -> Click on Tracing on left side bar -> Click on view transaction -> Go to real time tab Refer sample script file at: https://github.com/snappyflow/tracing-reference-apps/blob/master/refapp- express/node_trace_script.js "},{"title":"Node.JS Sails","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#nodejs-sails","content":"Install nodejs dependencies and save it in package.json using npm install --save elastic-apm-node@^3.20.0 npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries \"elastic-apm-node\": \"^3.20.0\" \"sf-apm-lib\": \"^1.0.2\" Copy And run ‘npm install’ to install dependencies Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variable in .env file and load it using require('dotenv').config() and access it in code using process.env.<ENV_VAR> Add initilization code at start of the file in globals.js present in config folder. Get Snappyflow trace config using: const Snappyflow = require('sf-apm-lib'); var sfObj = new Snappyflow(); // Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml. // Add below part to manually configure the initialization let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; sfObj.init(profileKey, projectName, appName); // Manual override let sfTraceConfig = sfObj.getTraceConfig(); Copy Initialize apm object using: var apm; try { apm = require('elastic-apm-node').start({ serviceName: '<SERVICE_NAME>', // Specify your service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'] }) } catch (e) { console.log(e); } Copy Attach apm object to globals – This is required so we can use apm variable in other files as part of global sails object. module.exports.globals = { _: require('@sailshq/lodash'), async: false, models: true, sails: true, apm : apm, logger: logger }; Copy Also add middleware in http.js file present in config folder. Which allows to instrument our code. module.exports.http = { middleware: { order: [ 'elasticAPM' ], elasticAPM: (function () { return function (err, req, res, next) { apm.middleware.connect(); if (typeof err !== 'undefined') apm.captureError(err); return next(); }; })() } }; Copy Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/RefappNodeSail "},{"title":"Kubernetes","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#kubernetes","content":""},{"title":"Node.JS Express","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#nodejs-express-1","content":"Install nodejs dependencies and save it in package.json using npm install --save elastic-apm-node@^3.20.0 npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries: \"elastic-apm-node\": \"^3.20.0\" \"sf-apm-lib\": \"^1.0.2\" Copy And run ‘npm install’ to install dependencies Add initilization code at start of the file in app.js Get Snappyflow trace config using const Snappyflow = require('sf-apm-lib'); var sfObj = new Snappyflow(); // Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml. // Add below part to manually configure the initialization let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; sfObj.init(profileKey, projectName, appName); // Manual override let sfTraceConfig = sfObj.getTraceConfig(); Copy Initialize apm object using var apm; try { apm = require('elastic-apm-node').start({ serviceName: '<SERVICE_NAME>', // Specify your service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'] }) } catch (e) { console.log(e); } Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in Kubernetes deployment file. https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/ Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in Snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to: View dashboard -> Click on Tracing on left side bar -> Click on view transaction -> Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-express "},{"title":"Node.JS Sails","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#nodejs-sails-1","content":"Install nodejs dependencies and save it in package.json using npm install --save elastic-apm-node@^3.20.0 npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries: \"elastic-apm-node\": \"^3.20.0\" \"sf-apm-lib\": \"^1.0.2\" Copy And run ‘npm install’ to install dependencies Add initilization code at start of the file in globals.js present in config folder. Get Snappyflow trace config using const Snappyflow = require('sf-apm-lib'); var sfObj = new Snappyflow(); // Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml. // Add below part to manually configure the initialization let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; sfObj.init(profileKey, projectName, appName); // Manual override let sfTraceConfig = sfObj.getTraceConfig(); Copy Initialize apm object using var apm; try { apm = require('elastic-apm-node').start({ serviceName: '<SERVICE_NAME>', // Specify your service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'] }) } catch (e) { console.log(e); } Copy Attach apm object to globals – This is required so we can use apm variable in other files as part of global sails object module.exports.globals = { _: require('@sailshq/lodash'), async: false, models: true, sails: true, apm : apm, logger: logger }; Copy Also add middleware in http.js file present in config folder. Which allows to instrument our code module.exports.http = { middleware: { order: [ 'elasticAPM' ], elasticAPM: (function () { return function (err, req, res, next) { apm.middleware.connect(); if (typeof err !== 'undefined') apm.captureError(err); return next(); }; })() }}; Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in Kubernetes deployment file. https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/ If deploying with helm provide above variables in values.yaml and use them in deployment file of charts. https://phoenixnap.com/kb/helm-environment-variables Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/RefappNodeSail "},{"title":"Docker","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#docker","content":""},{"title":"Node.JS Express","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#nodejs-express-2","content":"Install nodejs dependencies and save it in package.json using RUN npm install --save elastic-apm-node@^3.20.0 RUN npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries: \"elastic-apm-node\": \"^3.20.0\" \"sf-apm-lib\": \"^1.0.2\" Copy And run ‘npm install’ to install dependencies Add initilization code at start of the file in app.js Get Snappyflow trace config using const Snappyflow = require('sf-apm-lib'); var sfObj = new Snappyflow(); // Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml. // Add below part to manually configure the initialization let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; sfObj.init(profileKey, projectName, appName); // Manual override let sfTraceConfig = sfObj.getTraceConfig(); Copy Initialize apm object using var apm; try { apm = require('elastic-apm-node').start({ serviceName: '<SERVICE_NAME>', // Specify your service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'] }) } catch (e) { console.log(e); } Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in docker-compose.yml or docker stack deployment file or at command line when using docker run command for deployment. Eg: Docker-compose and stack: https://docs.docker.com/compose/environment-variables/ Docker run cli command: docker run -d -t -i -e SF_PROJECT_NAME='<Project name>' \\ -e SF_APP_NAME='<SF_APP_NAME>' \\ -e SF_PROFILE_KEY='<snappyflow profile key>' \\ --name <container_name> <dockerhub_id/image_name> Copy Once your server is up and running you can check trace in Snappyflow Server. // Project related info For viewing trace in Snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-express "},{"title":"Node.JS Sails","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#nodejs-sails-2","content":"Install nodejs dependencies and save it in package.json using RUN npm install --save elastic-apm-node@^3.20.0 RUN npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries: \"elastic-apm-node\": \"^3.20.0\" \"sf-apm-lib\": \"^1.0.2\" Copy And run ‘npm install’ to install dependencies Add initilization code at start of the file in globals.js present in config folder. Get Snappyflow trace config using const Snappyflow = require('sf-apm-lib'); var sfObj = new Snappyflow(); // Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml. // Add below part to manually configure the initialization let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; sfObj.init(profileKey, projectName, appName); // Manual override let sfTraceConfig = sfObj.getTraceConfig(); Copy Initialize apm object using var apm; try { apm = require('elastic-apm-node').start({ serviceName: '<SERVICE_NAME>', // Specify your service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'] }) } catch (e) { console.log(e); } Copy Attach apm object to globals – This is required so we can use apm variable in other files as part of global sails object. module.exports.globals = { _: require('@sailshq/lodash'), async: false, models: true, sails: true, apm : apm, logger: logger }; Copy Also add middleware in http.js file present in config folder. Which allows to instrument our code. module.exports.http = { middleware: { order: [ 'elasticAPM' ], elasticAPM: (function () { return function (err, req, res, next) { apm.middleware.connect(); if (typeof err !== 'undefined') apm.captureError(err); return next(); }; })() } }; Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in docker-compose.yml or docker stack deployment file or at command line when using docker run command for deployment. Eg: Docker-compose and stack: https://docs.docker.com/compose/environment-variables/ Docker run cli command: docker run -d -t -i -e SF_PROJECT_NAME='<SF_PROJECT_NAME>' \\ -e SF_APP_NAME='<SF_APP_NAME>' \\ -e SF_PROFILE_KEY='<snappyflow profile key>' \\ --name <container_name> <dockerhub_id/image_name> Copy Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/RefappNodeSail "},{"title":"ECS","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#ecs","content":""},{"title":"Node.JS Express","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#nodejs-express-3","content":"Install nodejs dependencies and save it in package.json using npm install --save elastic-apm-node@^3.20.0 npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries \"elastic-apm-node\": \"^3.20.0\" \"sf-apm-lib\": \"^1.0.2\" Copy And run ‘npm install’ to install dependencies Add initilization code at start of the file in app.js Get Snappyflow trace config using: const Snappyflow = require('sf-apm-lib'); var sfObj = new Snappyflow(); // Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml. // Add below part to manually configure the initialization let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; sfObj.init(profileKey, projectName, appName); // Manual override let sfTraceConfig = sfObj.getTraceConfig(); Copy Initialize apm object using var apm; try { apm = require('elastic-apm-node').start({ serviceName: '<SERVICE_NAME>', // Specify your service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'] }) } catch (e) { console.log(e); } Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in add container section of task definitions. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in Snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to: View dashboard -> Click on Tracing on left side bar -> Click on view transaction -> Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-express "},{"title":"Node.JS Sails","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#nodejs-sails-3","content":"Install nodejs dependencies and save it in package.json using npm install --save elastic-apm-node@^3.20.0 npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries \"elastic-apm-node\": \"^3.20.0\" \"sf-apm-lib\": \"^1.0.2\" Copy And run ‘npm install’ to install dependencies Add initilization code at start of the file in globals.js present in config folder. Get Snappyflow trace config using const Snappyflow = require('sf-apm-lib'); var sfObj = new Snappyflow(); // Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml. // Add below part to manually configure the initialization let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; sfObj.init(profileKey, projectName, appName); // Manual override let sfTraceConfig = sfObj.getTraceConfig(); Copy Initialize apm object using var apm; try { apm = require('elastic-apm-node').start({ serviceName: '<SERVICE_NAME>', // Specify your service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'] }) } catch (e) { console.log(e); } Copy Attach apm object to globals – This is required so we can use apm variable in other files as part of global sails object module.exports.globals = { _: require('@sailshq/lodash'), async: false, models: true, sails: true, apm : apm, logger: logger }; Copy Also add middleware in http.js file present in config folder. Which allows to instrument our code module.exports.http = { middleware: { order: [ 'elasticAPM' ], elasticAPM: (function () { return function (err, req, res, next) { apm.middleware.connect(); if (typeof err !== 'undefined') apm.captureError(err); return next(); }; })() } }; Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in add container section of task definitions. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/RefappNodeSail "},{"title":"AWS Lambda","type":1,"pageTitle":"Tracing node.js applications","url":"docs/Tracing/nodejs#aws-lambda","content":"Install dependency libraries in the node_modules directory using the npm install command npm install sf-apm-lib@^1.0.2 npm install elastic-apm-node@^3.20.0 Copy Ref: https://docs.aws.amazon.com/lambda/latest/dg/nodejs-package.html Instrument lambda function to enable tracing Add code outside lambda handler method to get tracing config and create trace client // SnappyFlow Tracing config const Snappyflow = require('sf-apm-lib'); let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; var sfObj = new Snappyflow(); sfObj.init(profileKey, projectName, appName); var apm; try { var sfTraceConfig = sfObj.getTraceConfig(); apm = require('elastic-apm-node').start({ serviceName: '<SERVICE_NAME_CHANGEME>', serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'], captureBody: 'all' }) } catch (e) { console.log(e) } Copy Add custom instrumentation inside lambda handler method Ref: https://www.elastic.co/guide/en/apm/agent/nodejs/current/custom-transactions.html https://www.elastic.co/guide/en/apm/agent/nodejs/current/custom-spans.html // Create custom transaction var trans = apm.startTransaction('lambda handler', 'lambda'); //Create custom span is needed var span = apm.startSpan('parse json'); // your CODE here // End of span if (span) span.end() //Some more code part of the transaction or add more spans here. Don’t RETURN/EXIT //end custom transaction trans.result = 'success'; trans.end(); // RETURN code Copy Deploy the lambda app. Follow README to test sample app Reference app: https://github.com/upendrasahu/aws-lambda-nodejs-tracing-sample Configure Lambda function before trigger/invoke. Add the environment variable SF_PROFILE_KEY and set the value to your profile key copied from SnappyFlow. Add environment variables SF_APP_NAME and SF_PROJECT_NAME with appropriate values. Create this Project and Application in SnappyFlow if not already present. At this point you can trigger lambda function and get tracing data in SnappyFlow. "},{"title":"Tracing Python Applications","type":0,"sectionRef":"#","url":"docs/Tracing/python","content":"","keywords":""},{"title":"Choose your platform","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#choose-your-platform","content":""},{"title":"Instance","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#instance","content":""},{"title":"Django","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#django","content":"Add sf-elastic-apm==6.3.4sf-apm-lib==0.1.1 Copy entries in requirements.txt file and install these in your project environment or Install through CLI using pip install sf-elastic-apm==6.3.4 pip install sf-apm-lib==0.1.1 Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variable. Add following entries in settings.py Add import statement from sf_apm_lib.snappyflow import Snappyflow Copy Add following entry in INSTALLED_APPS 'elasticapm.contrib.django' Copy Add following entry in MIDDLEWARE 'elasticapm.contrib.django.middleware.TracingMiddleware' Copy Add this entry for instrumenting Django app try: sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = os.getenv('SF_PROJECT_NAME') SF_APP_NAME = os.getenv('SF_APP_NAME') profile_key = os.getenv('SF_PROFILE_KEY') sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() ELASTIC_APM={ 'SERVICE_NAME': \"<Service name>\" , # Specify your service name for tracing 'SERVER_URL': SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), 'GLOBAL_LABELS': SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), 'VERIFY_SERVER_CERT': SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT'), 'SPAN_FRAMES_MIN_DURATION': SFTRACE_CONFIG.get('SFTRACE_SPAN_FRAMES_MIN_DURATION'), 'STACK_TRACE_LIMIT': SFTRACE_CONFIG.get('SFTRACE_STACK_TRACE_LIMIT'), 'CAPTURE_SPAN_STACK_TRACES': SFTRACE_CONFIG.get('SFTRACE_CAPTURE_SPAN_STACK_TRACES'), 'DJANGO_TRANSACTION_NAME_FROM_ROUTE': True, 'CENTRAL_CONFIG': False, 'DEBUG': True } except Exception as error: print(\"Error while fetching snappyflow tracing configurations\", error) Copy Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-django "},{"title":"Flask","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#flask","content":"Add sf-elastic-apm[flask]==6.3.4sf-apm-lib==0.1.1 Copy entries in requirements.txt file and install these in your project environment or Install through CLI using pip install sf-elastic-apm[flask]==6.3.4 pip install sf-apm-lib==0.1.1 Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variable. Add following entries in app.py Add imports statement from elasticapm.contrib.flask import ElasticAPM from sf_apm_lib.snappyflow import Snappyflow Copy Get trace config sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = os.getenv('SF_PROJECT_NAME') SF_APP_NAME = os.getenv('SF_APP_NAME') profile_key = os.getenv('SF_PROFILE_KEY') sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() Copy Initialize elastic apm and instrument it to flask app app.config['ELASTIC_APM'] = { 'SERVICE_NAME': '<SERVICE_NAME>', # Specify your service name for tracing 'SERVER_URL': SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), 'GLOBAL_LABELS': SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), 'VERIFY_SERVER_CERT': SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT'), 'SPAN_FRAMES_MIN_DURATION': SFTRACE_CONFIG.get('SFTRACE_SPAN_FRAMES_MIN_DURATION'), 'STACK_TRACE_LIMIT': SFTRACE_CONFIG.get('SFTRACE_STACK_TRACE_LIMIT'), 'CAPTURE_SPAN_STACK_TRACES': SFTRACE_CONFIG.get('SFTRACE_CAPTURE_SPAN_STACK_TRACES'), 'DEBUG': True } apm = ElasticAPM(app) Copy Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab "},{"title":"Script","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#script","content":"Install following requirements pip install sf-elastic-apm==6.3.4 pip install sf-apm-lib==0.1.1 Copy Add following code at start of script file to setup elastic apm client import elasticapm from sf_apm_lib.snappyflow import Snappyflow sf = Snappyflow() # Initialize Snappyflow. By default intialization will pick profileKey, projectName and appName from sfagent config.yaml. # Add below part to manually configure the initialization SF_PROJECT_NAME = '<Snappyflow Project Name>' SF_APP_NAME = '<Snappyflow App Name>' profile_key = '<Snappyflow Profile Key>' sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration trace_config = sf.get_trace_config() # Returns trace config client = elasticapm.Client( service_name=\"<Service name> \",# Specify service name for tracing server_url=trace_config['SFTRACE_SERVER_URL'], verify_cert=trace_config['SFTRACE_VERIFY_SERVER_CERT'], global_labels=trace_config['SFTRACE_GLOBAL_LABELS'] ) elasticapm.instrument() # Only call this once, as early as possible. Copy Once instrumentation is completed we can create custom transaction and span Example def main(): sess = requests.Session() for url in [ 'https://www.elastic.co', 'https://benchmarks.elastic.co' ]: resp = sess.get(url) time.sleep(1) client.begin_transaction(transaction_type=\"script\") main() # Record an exception try: 1/0 except ZeroDivisionError: ident = client.capture_exception() print (\"Exception caught; reference is %s\" % ident) client.end_transaction(name=__name__, result=\"success\") Copy Refer link to know more: https://www.elastic.co/guide/en/apm/agent/python/master/instrumenting-custom-code.html Now run you script and test your trace in snappyflow server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -> Click on Tracing on left side bar -> Click on view transaction -> Go to real time tab Refer complete script: https://github.com/snappyflow/tracing-reference-apps/blob/master/refapp-django/python_script_trace.py "},{"title":"Celery","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#celery","content":"Install following requirements (Following example is based on redis broker) pip install sf-elastic-apm==6.3.4 pip install redis pip install sf-apm-lib==0.1.1 Copy Add following code at start of the file where celery app is initialized to setup elastic apm client from sf_apm_lib.snappyflow import Snappyflow from elasticapm import Client, instrument from elasticapm.contrib.celery import register_exception_tracking, register_instrumentation instrument() try: sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = '<SF_PROJECT_NAME>' # Replace with appropriate Snappyflow project name SF_APP_NAME = '<SF_APP_NAME>' # Replace with appropriate Snappyflow app name profile_key = '<SF_PROFILE_KEY>' # Replace Snappyflow Profile key sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() apm_client = Client( service_name= '<Service_Name>', # Specify service name for tracing server_url= SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), global_labels= SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), verify_server_cert= SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT') ) register_exception_tracking(apm_client) register_instrumentation(apm_client) except Exception as error: print(\"Error while fetching snappyflow tracing configurations\", error) Copy Once instrumentation is done and celery worker is running we can see trace for each celery task in Snappyflow server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -> Click on Tracing on left side bar -> Click on view transaction -> Go to real time tab Refer complete code: https://github.com/snappyflow/tracing-reference-apps/blob/master/ref-celery/tasks.py "},{"title":"Kubernetes","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#kubernetes","content":""},{"title":"Django","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#django-1","content":"Add sf-elastic-apm==6.3.4sf-apm-lib==0.1.1 Copy entries in requirements.txt file and install these in your project environment or Install through CLI using pip install sf-elastic-apm==6.3.4 pip install sf-apm-lib==0.1.1 Copy Add following entries in settings.py Add import statement from sf_apm_lib.snappyflow import Snappyflow Copy Add following entry in INSTALLED_APPS 'elasticapm.contrib.django' Copy Add following entry in MIDDLEWARE 'elasticapm.contrib.django.middleware.TracingMiddleware' Copy Add this entry for instrumenting Django app try: sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = os.getenv('SF_PROJECT_NAME') SF_APP_NAME = os.getenv('SF_APP_NAME') profile_key = os.getenv('SF_PROFILE_KEY') sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() ELASTIC_APM={ 'SERVICE_NAME': \"<Service name>\" , # Specify your service name for tracing 'SERVER_URL': SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), 'GLOBAL_LABELS': SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), 'VERIFY_SERVER_CERT': SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT'), 'SPAN_FRAMES_MIN_DURATION': SFTRACE_CONFIG.get('SFTRACE_SPAN_FRAMES_MIN_DURATION'), 'STACK_TRACE_LIMIT': SFTRACE_CONFIG.get('SFTRACE_STACK_TRACE_LIMIT'), 'CAPTURE_SPAN_STACK_TRACES': SFTRACE_CONFIG.get('SFTRACE_CAPTURE_SPAN_STACK_TRACES'), 'DJANGO_TRANSACTION_NAME_FROM_ROUTE': True, 'CENTRAL_CONFIG': False, 'DEBUG': True } except Exception as error: print(\"Error while fetching snappyflow tracing configurations\", error) Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in Kubernetes deployment file. https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/ If deploying with helm provide above variables in values.yaml and use them in deployment file of charts. https://phoenixnap.com/kb/helm-environment-variables Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-django "},{"title":"Flask","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#flask-1","content":"Add sf-elastic-apm[flask]==6.3.4sf-apm-lib==0.1.1 Copy entries in requirements.txt file and install these in your project environment or Install through CLI using RUN pip install sf-elastic-apm[flask]==6.3.4 RUN pip install sf-apm-lib==0.1.1 Copy Add following entries in app.py Add imports statement from elasticapm.contrib.flask import ElasticAPM from sf_apm_lib.snappyflow import Snappyflow Copy Get trace config sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = os.getenv('SF_PROJECT_NAME') SF_APP_NAME = os.getenv('SF_APP_NAME') profile_key = os.getenv('SF_PROFILE_KEY') sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() Copy Initialize elastic apm and instrument it to flask app app.config['ELASTIC_APM'] = { 'SERVICE_NAME': '<SERVICE_NAME>', # Specify your service name for tracing 'SERVER_URL': SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), 'GLOBAL_LABELS': SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), 'VERIFY_SERVER_CERT': SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT'), 'SPAN_FRAMES_MIN_DURATION': SFTRACE_CONFIG.get('SFTRACE_SPAN_FRAMES_MIN_DURATION'), 'STACK_TRACE_LIMIT': SFTRACE_CONFIG.get('SFTRACE_STACK_TRACE_LIMIT'), 'CAPTURE_SPAN_STACK_TRACES': SFTRACE_CONFIG.get('SFTRACE_CAPTURE_SPAN_STACK_TRACES'), 'DEBUG': True } apm = ElasticAPM(app) Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in Kubernetes deployment file. https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/ If deploying with helm provide above variables in values.yaml and use them in deployment file of charts. https://phoenixnap.com/kb/helm-environment-variables Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab "},{"title":"Celery","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#celery-1","content":"Install following requirements (Following example is based on redis broker) pip install sf-elastic-apm==6.3.4 pip install redis pip install sf-apm-lib==0.1.1 Copy Add following code at start of the file where celery app is initialized to setup elastic apm client from sf_apm_lib.snappyflow import Snappyflow from elasticapm import Client, instrument from elasticapm.contrib.celery import register_exception_tracking, register_instrumentation instrument() try: sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = '<SF_PROJECT_NAME>' # Replace with appropriate Snappyflow project name SF_APP_NAME = '<SF_APP_NAME>' # Replace with appropriate Snappyflow app name profile_key = '<SF_PROFILE_KEY>' # Replace Snappyflow Profile key sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() apm_client = Client(service_name= '<Service_Name>', # Specify service name for tracing server_url= SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), global_labels= SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), verify_server_cert= SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT') ) register_exception_tracking(apm_client) register_instrumentation(apm_client) except Exception as error: print(\"Error while fetching snappyflow tracing configurations\", error) Copy Once instrumentation is done and celery worker is running we can see trace for each celery task in Snappyflow server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -> Click on Tracing on left side bar -> Click on view transaction -> Go to real time tab Refer complete code: https://github.com/snappyflow/tracing-reference-apps/blob/master/ref-celery/tasks.py "},{"title":"Docker","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#docker","content":""},{"title":"Django","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#django-2","content":"Add sf-elastic-apm==6.3.4sf-apm-lib==0.1.1 Copy entries in requirements.txt file and install these in your project environment or Install through CLI using RUN pip install sf-elastic-apm==6.3.4 RUN pip install sf-apm-lib==0.1.1 Copy Add following entries in settings.py Add import statement from sf_apm_lib.snappyflow import Snappyflow Copy Add following entry in INSTALLED_APPS 'elasticapm.contrib.django' Copy Add following entry in MIDDLEWARE 'elasticapm.contrib.django.middleware.TracingMiddleware' Copy Add this entry for instrumenting Django app try: sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = os.getenv('SF_PROJECT_NAME') SF_APP_NAME = os.getenv('SF_APP_NAME') profile_key = os.getenv('SF_PROFILE_KEY') sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() ELASTIC_APM={ 'SERVICE_NAME': \"<Service name>\" , # Specify your service name for tracing 'SERVER_URL': SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), 'GLOBAL_LABELS': SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), 'VERIFY_SERVER_CERT': SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT'), 'SPAN_FRAMES_MIN_DURATION': SFTRACE_CONFIG.get('SFTRACE_SPAN_FRAMES_MIN_DURATION'), 'STACK_TRACE_LIMIT': SFTRACE_CONFIG.get('SFTRACE_STACK_TRACE_LIMIT'), 'CAPTURE_SPAN_STACK_TRACES': SFTRACE_CONFIG.get('SFTRACE_CAPTURE_SPAN_STACK_TRACES'), 'DJANGO_TRANSACTION_NAME_FROM_ROUTE': True, 'CENTRAL_CONFIG': False, 'DEBUG': True } except Exception as error: print(\"Error while fetching snappyflow tracing configurations\", error) Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in docker-compose.yml or docker stack deployment file or at command line when using docker run command for deployment. Eg: Docker-compose and stack: https://docs.docker.com/compose/environment-variables/ Docker RUN: docker run -d -t -i -e SF_PROJECT_NAME='' \\ -e SF_APP_NAME='' \\ -e SF_PROFILE_KEY='' \\ -p 80:80 \\ --link redis:redis \\ --name <container_name> <dockerhub_id/image_name> Copy Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-django "},{"title":"Flask","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#flask-2","content":"Add sf-elastic-apm[flask]==6.3.4sf-apm-lib==0.1.1 Copy entries in requirements.txt file and install these in your project environment or Install through CLI using RUN pip install sf-elastic-apm[flask]==6.3.4 RUN pip install sf-apm-lib==0.1.1 Copy Add following entries in app.py Add imports statement from elasticapm.contrib.flask import ElasticAPM from sf_apm_lib.snappyflow import Snappyflow Copy Get trace config sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = os.getenv('SF_PROJECT_NAME') SF_APP_NAME = os.getenv('SF_APP_NAME') profile_key = os.getenv('SF_PROFILE_KEY') sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() Copy Initialize elastic apm and instrument it to flask app app.config['ELASTIC_APM'] = { 'SERVICE_NAME': '<SERVICE_NAME>', # Specify your service name for tracing 'SERVER_URL': SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), 'GLOBAL_LABELS': SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), 'VERIFY_SERVER_CERT': SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT'), 'SPAN_FRAMES_MIN_DURATION': SFTRACE_CONFIG.get('SFTRACE_SPAN_FRAMES_MIN_DURATION'), 'STACK_TRACE_LIMIT': SFTRACE_CONFIG.get('SFTRACE_STACK_TRACE_LIMIT'), 'CAPTURE_SPAN_STACK_TRACES': SFTRACE_CONFIG.get('SFTRACE_CAPTURE_SPAN_STACK_TRACES'), 'DEBUG': True } apm = ElasticAPM(app) Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in docker-compose.yml or docker stack deployment file or at command line when using docker run command for deployment. Eg: Docker-compose and stack: https://docs.docker.com/compose/environment-variables/ Docker run cli command: docker run -d -t -i -e SF_PROJECT_NAME='<SF_PROJECT_NAME>' \\ -e SF_APP_NAME='<SF_APP_NAME>' \\ -e SF_PROFILE_KEY='<snappyflow profile key>' \\ --name <container_name> <dockerhub_id/image_name> Copy Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab "},{"title":"Celery","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#celery-2","content":"Install following requirements (Following example is based on redis broker) pip install sf-elastic-apm==6.3.4 pip install redis pip install sf-apm-lib==0.1.1 Copy Add following code at start of the file where celery app is initialized to setup elastic apm client from sf_apm_lib.snappyflow import Snappyflow from elasticapm import Client, instrument from elasticapm.contrib.celery import register_exception_tracking, register_instrumentation instrument() try: sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = '<SF_PROJECT_NAME>' # Replace with appropriate Snappyflow project name SF_APP_NAME = '<SF_APP_NAME>' # Replace with appropriate Snappyflow app name profile_key = '<SF_PROFILE_KEY>' # Replace Snappyflow Profile key sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() apm_client = Client( service_name= '<Service_Name>', # Specify service name for tracing server_url= SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), global_labels= SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), verify_server_cert= SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT') ) register_exception_tracking(apm_client) register_instrumentation(apm_client) except Exception as error: print(\"Error while fetching snappyflow tracing configurations\", error) Copy Once instrumentation is done and celery worker is running we can see trace for each celery task in Snappyflow server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -> Click on Tracing on left side bar -> Click on view transaction -> Go to real time tab Refer complete code: https://github.com/snappyflow/tracing-reference-apps/blob/master/ref-celery/tasks.py "},{"title":"ECS","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#ecs","content":""},{"title":"Django","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#django-3","content":"Add sf-elastic-apm==6.3.4sf-apm-lib==0.1.1 Copy entries in requirements.txt file and install these in your project environment or Install through CLI using pip install sf-elastic-apm==6.3.4 pip install sf-apm-lib==0.1.1 Copy Add following entries in settings.py Add import statement from sf_apm_lib.snappyflow import Snappyflow Copy Add following entry in INSTALLED_APPS 'elasticapm.contrib.django' Copy Add following entry in MIDDLEWARE: 'elasticapm.contrib.django.middleware.TracingMiddleware' Copy Add this entry for instrumenting Django app try: sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = os.getenv('SF_PROJECT_NAME') SF_APP_NAME = os.getenv('SF_APP_NAME') profile_key = os.getenv('SF_PROFILE_KEY') sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() ELASTIC_APM={ 'SERVICE_NAME': \"<Service name>\" , # Specify your service name for tracing 'SERVER_URL': SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), 'GLOBAL_LABELS': SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), 'VERIFY_SERVER_CERT': SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT'), 'SPAN_FRAMES_MIN_DURATION': SFTRACE_CONFIG.get('SFTRACE_SPAN_FRAMES_MIN_DURATION'), 'STACK_TRACE_LIMIT': SFTRACE_CONFIG.get('SFTRACE_STACK_TRACE_LIMIT'), 'CAPTURE_SPAN_STACK_TRACES': SFTRACE_CONFIG.get('SFTRACE_CAPTURE_SPAN_STACK_TRACES'), 'DJANGO_TRANSACTION_NAME_FROM_ROUTE': True, 'CENTRAL_CONFIG': False, 'DEBUG': True } except Exception as error: print(\"Error while fetching snappyflow tracing configurations\", error) Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in add container section of task definitions. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-django "},{"title":"Flask","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#flask-3","content":"Add sf-elastic-apm[flask]==6.3.4sf-apm-lib==0.1.1 Copy entries in requirements.txt file and install these in your project environment or Install through CLI using RUN pip install sf-elastic-apm[flask]==6.3.4 RUN pip install sf-apm-lib==0.1.1 Copy Add following entries in app.py Add imports statement from elasticapm.contrib.flask import ElasticAPM from sf_apm_lib.snappyflow import Snappyflow Copy Get trace config sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = os.getenv('SF_PROJECT_NAME') SF_APP_NAME = os.getenv('SF_APP_NAME') profile_key = os.getenv('SF_PROFILE_KEY') sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() Copy Initialize elastic apm and instrument it to flask app app.config['ELASTIC_APM'] = { 'SERVICE_NAME': '<SERVICE_NAME>', # Specify your service name for tracing 'SERVER_URL': SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), 'GLOBAL_LABELS': SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), 'VERIFY_SERVER_CERT': SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT'), 'SPAN_FRAMES_MIN_DURATION': SFTRACE_CONFIG.get('SFTRACE_SPAN_FRAMES_MIN_DURATION'), 'STACK_TRACE_LIMIT': SFTRACE_CONFIG.get('SFTRACE_STACK_TRACE_LIMIT'), 'CAPTURE_SPAN_STACK_TRACES': SFTRACE_CONFIG.get('SFTRACE_CAPTURE_SPAN_STACK_TRACES'), 'DEBUG': True } apm = ElasticAPM(app) Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in add container section of task definitions. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -> Click on Tracing on lef side bar -> Click on view transaction -> Go to real time tab "},{"title":"Celery","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#celery-3","content":"Install following requirements (Following example is based on redis broker) pip install sf-elastic-apm==6.3.4 pip install redis pip install sf-apm-lib==0.1.1 Copy Add following code at start of the file where celery app is initialized to setup elastic apm client from sf_apm_lib.snappyflow import Snappyflow from elasticapm import Client, instrument from elasticapm.contrib.celery import register_exception_tracking, register_instrumentation instrument() try: sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = '<SF_PROJECT_NAME>' # Replace with appropriate Snappyflow project name SF_APP_NAME = '<SF_APP_NAME>' # Replace with appropriate Snappyflow app name profile_key = '<SF_PROFILE_KEY>' # Replace Snappyflow Profile key sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() apm_client = Client( service_name= '<Service_Name>', # Specify service name for tracing server_url= SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), global_labels= SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), verify_server_cert= SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT') ) register_exception_tracking(apm_client) register_instrumentation(apm_client) except Exception as error: print(\"Error while fetching snappyflow tracing configurations\", error) Copy Once instrumentation is done and celery worker is running we can see trace for each celery task in Snappyflow server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -> Click on Tracing on left side bar -> Click on view transaction -> Go to real time tab Refer complete code: https://github.com/snappyflow/tracing-reference-apps/blob/master/ref-celery/tasks.py "},{"title":"AWS Lambda","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#aws-lambda","content":""},{"title":"Script","type":1,"pageTitle":"Tracing Python Applications","url":"docs/Tracing/python#script-1","content":"Add these python libraries in requirements.txt file. Follow the AWS lambda doc on adding runtime dependency to lambda function. sf-apm-lib==0.1.1 sf-elastic-apm==6.3.4 Copy Ref: https://docs.aws.amazon.com/lambda/latest/dg/python-package-create.html#python-package-create-with-dependency Instrument lambda function to enable tracing. Import Libraries import elasticapm from sf_apm_lib.snappyflow import Snappyflow Copy Add code to get SnappyFlow Trace config, outside lambda handler method. sf = Snappyflow()SF_PROJECT_NAME = os.environ['SF_PROJECT_NAME'] SF_APP_NAME = os.environ['SF_APP_NAME'] profile_key = os.environ['SF_PROFILE_KEY'] sf.init(profile_key, SF_PROJECT_NAME, SF_APP_NAME) trace_config = snappyflow.get_trace_config() Copy Add custom instrumentation in lambda handler function def lambda_handler(event, context): client = elasticapm.Client(service_name=\"<SERVICE_NAME_CHANGEME>\", server_url=trace_config['SFTRACE_SERVER_URL'], verify_cert=trace_config['SFTRACE_VERIFY_SERVER_CERT'], global_labels=trace_config['SFTRACE_GLOBAL_LABELS'] ) elasticapm.instrument() client.begin_transaction(transaction_type=\"script\") # DO SOME WORK. No return statements. client.end_transaction(name=__name__, result=\"success\") # RETURN STATEMENT e.g. return response Copy Deploy the Lambda function. Follow README to test sample app Sample code for reference: https://github.com/upendrasahu/aws-lambda-python-tracing-sample Configure Lambda function before trigger/invoke. Add the environment variable SF_PROFILE_KEY and set the value to your profile key copied from SnappyFlow. Add environment variables SF_APP_NAME and SF_PROJECT_NAME with appropriate values. "}]